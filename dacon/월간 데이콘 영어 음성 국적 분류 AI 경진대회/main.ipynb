{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "from glob import glob\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch_poly_lr_decay import PolynomialLRDecay\n",
    "import random\n",
    "from torchvision import models\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import math\n",
    "import librosa\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2'\n",
    "torch.set_num_threads(8)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"C:\\\\Users\\\\Home\\\\Desktop\\\\영어\\\\sample_submission.csv\")\n",
    "africa_train_paths = sorted(glob(\"C:\\\\Users\\\\Home\\\\Desktop\\\\영어\\\\train\\\\africa\\\\*.wav\"))\n",
    "australia_train_paths = sorted(glob(\"C:\\\\Users\\\\Home\\\\Desktop\\\\영어\\\\train\\\\australia\\\\*.wav\"))\n",
    "canada_train_paths = sorted(glob(\"C:\\\\Users\\\\Home\\\\Desktop\\\\영어\\\\train\\\\canada\\\\*.wav\"))\n",
    "england_train_paths = sorted(glob(\"C:\\\\Users\\\\Home\\\\Desktop\\\\영어\\\\train\\\\england\\\\*.wav\"))\n",
    "hongkong_train_paths = sorted(glob(\"C:\\\\Users\\\\Home\\\\Desktop\\\\영어\\\\train\\\\hongkong\\\\*.wav\"))\n",
    "us_train_paths = sorted(glob(\"C:\\\\Users\\\\Home\\\\Desktop\\\\영어\\\\train\\\\us\\\\*.wav\"))\n",
    "test_paths = [f'C:\\\\Users\\\\Home\\\\Desktop\\\\영어\\\\test\\\\{k+1}.wav' for k in range(6100)]\n",
    "\n",
    "def load_data(paths):\n",
    "    result = []\n",
    "    for path in tqdm(paths):\n",
    "        data, sr = librosa.load(path, sr = 16000)\n",
    "        result.append(data)\n",
    "    result = np.array(result)\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_feature(data, sr = 16000, n_fft = 2048, win_length = 200, hop_length = 160, n_mels = 64):\n",
    "    mel = []\n",
    "    for i in tqdm(data):\n",
    "        mel_ = librosa.feature.melspectrogram(i, sr = sr, n_fft = n_fft, win_length = win_length, hop_length = hop_length, n_mels = n_mels)\n",
    "        mel.append(mel_)\n",
    "    mel = np.array(mel)\n",
    "    mel = librosa.power_to_db(mel, ref = np.max)\n",
    "\n",
    "\n",
    "    return mel\n",
    "\n",
    "\n",
    "def set_length(data, d_mini):\n",
    "\n",
    "    result = []\n",
    "    for value in tqdm(data):\n",
    "        value = value[:d_mini]\n",
    "        if len(value)<d_mini:\n",
    "            value = np.append(value, [0]*(d_mini-len(value)))\n",
    "        result.append(value)\n",
    "    result = np.array(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "africa_train_data = load_data(africa_train_paths)\n",
    "np.save(\"./dataset/africa-sorted.npy\", africa_train_data)\n",
    "\n",
    "australia_train_data = load_data(australia_train_paths)\n",
    "np.save(\"./dataset/australia-sorted.npy\", australia_train_data)\n",
    "\n",
    "canada_train_data = load_data(canada_train_paths)\n",
    "np.save(\"./dataset/canada-sorted.npy\", canada_train_data)\n",
    "\n",
    "england_train_data = load_data(england_train_paths)\n",
    "np.save(\"./dataset/england-sorted.npy\", england_train_data)\n",
    "\n",
    "hongkong_train_data = load_data(hongkong_train_paths)\n",
    "np.save(\"./dataset/hongkong-sorted.npy\", hongkong_train_data)\n",
    "\n",
    "us_train_data = load_data(us_train_paths)\n",
    "np.save(\"./dataset/us-sorted.npy\", us_train_data)\n",
    "train_data_list = [africa_train_data, australia_train_data, canada_train_data, england_train_data, hongkong_train_data, us_train_data]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_data = load_data(test_paths)\n",
    "np.save(\"./dataset/test_npy.npy\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.concatenate(train_data_list, axis=0)\n",
    "train_x = set_length(train_x, 100000)\n",
    "\n",
    "train_x_200 = get_feature(data = train_x, win_length=200)\n",
    "train_x_400 = get_feature(data = train_x, win_length=400)\n",
    "train_x_800 = get_feature(data = train_x, win_length=800)\n",
    "train_x_1000 = get_feature(data = train_x, win_length=1000)\n",
    "\n",
    "\n",
    "train_x_200 = train_x_200.reshape(train_x_200.shape[0], train_x_200.shape[1], train_x_200.shape[2], 1)\n",
    "train_x_400 = train_x_400.reshape(train_x_400.shape[0], train_x_400.shape[1], train_x_400.shape[2], 1)\n",
    "train_x_800 = train_x_800.reshape(train_x_800.shape[0], train_x_800.shape[1], train_x_800.shape[2], 1)\n",
    "train_x_1000 = train_x_1000.reshape(train_x_1000.shape[0], train_x_1000.shape[1], train_x_1000.shape[2], 1)\n",
    "\n",
    "train_x_multi = np.concatenate([train_x_200,\n",
    "                                train_x_400,\n",
    "                                train_x_800,\n",
    "                                train_x_1000], -1)\n",
    "np.save('C:\\\\Users\\\\Home\\\\Desktop\\\\영어\\\\dataset\\\\train_x_multi.npy', train_x_multi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.load('C:\\\\Users\\\\Home\\\\Desktop\\\\영어\\\\test_npy.npy', allow_pickle=True)\n",
    "test_x = set_length(test_x, 100000)\n",
    "test_x_200 = get_feature(data = test_x, win_length=200)\n",
    "test_x_400 = get_feature(data = test_x, win_length=400)\n",
    "test_x_800 = get_feature(data = test_x, win_length=800)\n",
    "test_x_1000 = get_feature(data = test_x, win_length=1000)\n",
    "\n",
    "test_x_200 = test_x_200.reshape(test_x_200.shape[0], test_x_200.shape[1], test_x_200.shape[2], 1)\n",
    "test_x_400 = test_x_400.reshape(test_x_400.shape[0], test_x_400.shape[1], test_x_400.shape[2], 1)\n",
    "test_x_800 = test_x_800.reshape(test_x_800.shape[0], test_x_800.shape[1], test_x_800.shape[2], 1)\n",
    "test_x_1000 = test_x_1000.reshape(test_x_1000.shape[0], test_x_1000.shape[1], test_x_1000.shape[2], 1)\n",
    "test_x_multi = np.concatenate([test_x_200,\n",
    "                                test_x_400,\n",
    "                                test_x_800,\n",
    "                                test_x_1000], -1)\n",
    "np.save('C:\\\\Users\\\\Home\\\\Desktop\\\\영어\\\\dataset\\\\test_x_multi.npy', test_x_mulati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.concatenate((np.zeros(len(africa_train_data), dtype = int),\n",
    "                        np.ones(len(australia_train_data), dtype = int),\n",
    "                         np.ones(len(canada_train_data), dtype = int) * 2,\n",
    "                         np.ones(len(england_train_data), dtype = int) * 3,\n",
    "                         np.ones(len(hongkong_train_data), dtype = int) * 4,\n",
    "                         np.ones(len(us_train_data), dtype = int) * 5), axis = 0)\n",
    "np.save('C:\\\\Users\\\\Home\\\\Desktop\\\\영어\\\\dataset\\\\train_y_sort.npy', train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_bn_relu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(conv_bn_relu, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.BN = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.BN(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, N):\n",
    "        super(Network, self).__init__()\n",
    "        self.N = N\n",
    "        self.AveragePooling = nn.AvgPool2d(2)\n",
    "        self.MaxPooling = nn.MaxPool2d(2)\n",
    "        \n",
    "        \n",
    "        self.input_conv = conv_bn_relu(in_channels=4, out_channels=self.N, kernel_size=3)\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            conv_bn_relu(in_channels=self.N*1, out_channels=self.N*2, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*4, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*4, out_channels=self.N*2, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*1, kernel_size=3),\n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            conv_bn_relu(in_channels=self.N*1, out_channels=self.N*2, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*4, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*4, out_channels=self.N*2, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*1, kernel_size=3),\n",
    "        )\n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            conv_bn_relu(in_channels=self.N*1, out_channels=self.N*2, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*4, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*4, out_channels=self.N*2, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*1, kernel_size=3),\n",
    "        )\n",
    "        \n",
    "        self.block4 = nn.Sequential(\n",
    "            conv_bn_relu(in_channels=self.N*1, out_channels=self.N*2, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*4, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*4, out_channels=self.N*2, kernel_size=3),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*1, kernel_size=3),\n",
    "        )\n",
    "        \n",
    "        self.pool_block = nn.Sequential(\n",
    "            conv_bn_relu(in_channels=self.N*1, out_channels=self.N*2, kernel_size=3),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.AvgPool2d(2),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*2, kernel_size=3),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.AvgPool2d(2),\n",
    "            conv_bn_relu(in_channels=self.N*2, out_channels=self.N*2, kernel_size=3),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=self.N*2, out_features=6),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, out=''):\n",
    "        x = self.input_conv(x)\n",
    "        x = self.MaxPooling(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool_block(x)\n",
    "        x=self.output(x)\n",
    "        \n",
    "        if out=='sigmoid':\n",
    "            x = F.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class VoiceDatasetSimple(Dataset):\n",
    "        def __init__(self, X, y, transform, inference=False, roll=False):\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "            self.transform = transform\n",
    "            self.inference = inference\n",
    "            self.roll = roll\n",
    "        def __len__(self):\n",
    "            return len(self.X)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            X = self.X[idx]\n",
    "            X = (X-train_x_min)/(train_x_max-train_x_min)\n",
    "            \n",
    "            if self.inference:\n",
    "                X = self.transform(X)\n",
    "                return X\n",
    "            else:\n",
    "                if (self.roll==True) and (random.randint(0, 1)==1):\n",
    "                    X = np.roll(X,random.randint(-200, 200), axis=1)\n",
    "                    \n",
    "                X = self.transform(X)    \n",
    "                y = self.y[idx]\n",
    "                \n",
    "                onehot = np.zeros(6)\n",
    "                onehot[y] = 1.\n",
    "                y = onehot\n",
    "                return X, y\n",
    "\n",
    "def model_save(model, path):\n",
    "    torch.save({\n",
    "        'model': model.state_dict(),\n",
    "    }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for person in range(3):\n",
    "    train_x = np.load('C:\\\\Users\\\\Home\\\\Desktop\\\\영어\\\\dataset\\\\train_x_multi.npy')\n",
    "    train_y = np.load('C:\\\\Users\\\\Home\\\\Desktop\\\\영어\\\\dataset\\\\train_y_sort.npy')\n",
    "    train_x_min = train_x.min()\n",
    "    train_x_max = train_x.max()\n",
    "\n",
    "\n",
    "    idx = [k+person for k in range(0, len(train_x), 3)][:-1]\n",
    "    train_x=train_x[idx]\n",
    "    train_y=train_y[idx]\n",
    "\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    folds=[]\n",
    "    for train_idx, valid_idx in skf.split(train_x, train_y):\n",
    "        folds.append((train_idx, valid_idx))\n",
    "\n",
    "        \n",
    "    for fold in range(5):\n",
    "        epochs=35\n",
    "        batch_size=128\n",
    "        model_name = f'network-epoch{epochs}-person({person})-fold({fold}).pth'\n",
    "        train_idx, valid_idx = folds[fold]\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        train_dataset = VoiceDatasetSimple(X=train_x[train_idx], y=train_y[train_idx], transform=transform, roll=False)\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        valid_dataset = VoiceDatasetSimple(X=train_x[valid_idx], y=train_y[valid_idx], transform=transform)\n",
    "        valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "        # model compile\n",
    "        model = Network(16).to(device)\n",
    "        model = nn.DataParallel(model, device_ids=[0])\n",
    "        \n",
    "        \n",
    "        # optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr =1e-3)\n",
    "        Q = math.floor(len(train_dataset)/batch_size+1)*epochs/7\n",
    "        lrs = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = Q)\n",
    "        \n",
    "        # loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "        best = 9999\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            start = time.time()\n",
    "            model.train()\n",
    "            train_loss=0\n",
    "            train_pred_list=[]\n",
    "            train_true_list=[]\n",
    "            train_log_loss=0\n",
    "            for X, y in (train_loader):\n",
    "                X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "                y = torch.tensor(y, dtype=torch.float32 , device=device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                pred = model(X)\n",
    "                loss = criterion(pred, y.argmax(1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lrs.step()\n",
    "\n",
    "                train_pred_list += F.softmax(pred).argmax(1).detach().cpu().numpy().tolist()\n",
    "                train_true_list += y.argmax(1).detach().cpu().numpy().tolist()\n",
    "                train_loss+=loss.item()\n",
    "            train_accuracy=accuracy_score(train_true_list, train_pred_list)\n",
    "\n",
    "\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                valid_loss=0\n",
    "                valid_log_loss=0\n",
    "                valid_pred_list=[]\n",
    "                valid_true_list=[]\n",
    "                for X, y in (valid_loader):\n",
    "                    X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "                    y = torch.tensor(y, dtype=torch.float32 , device=device)\n",
    "\n",
    "                    pred = model(X)\n",
    "                    loss = criterion(pred, y.argmax(1))\n",
    "\n",
    "                    valid_pred_list += F.softmax(pred).argmax(1).detach().cpu().numpy().tolist()\n",
    "                    valid_true_list += y.argmax(1).detach().cpu().numpy().tolist()\n",
    "                    valid_loss+=loss.item()\n",
    "\n",
    "            valid_accuracy=accuracy_score(valid_true_list, valid_pred_list)\n",
    "\n",
    "            if valid_loss/len(valid_loader) < best:\n",
    "                model_save(model, f'model/{model_name}')\n",
    "                best = valid_loss/len(valid_loader)\n",
    "\n",
    "            print(f'===================== Epoch : {epoch+1}/{epochs}    time : {time.time()-start:.0f}s =====================')\n",
    "            print(f'TRAIN -> loss : {train_loss/len(train_loader):.5f}     accuracy : {train_accuracy:.5f}')\n",
    "            print(f'VALID -> loss : {valid_loss/len(valid_loader):.5f}     accuracy : {valid_accuracy:.5f}    best : {best:.5f}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:15<00:00, 13.31it/s]\n",
      "100%|██████████| 204/204 [00:08<00:00, 24.97it/s]\n",
      "100%|██████████| 204/204 [00:08<00:00, 24.81it/s]\n",
      "100%|██████████| 204/204 [00:08<00:00, 24.70it/s]\n",
      "100%|██████████| 204/204 [00:08<00:00, 23.09it/s]\n",
      "100%|██████████| 204/204 [00:08<00:00, 24.91it/s]\n",
      "100%|██████████| 204/204 [00:08<00:00, 24.93it/s]\n",
      "100%|██████████| 204/204 [00:08<00:00, 25.14it/s]\n",
      "100%|██████████| 204/204 [00:08<00:00, 24.86it/s]\n",
      "100%|██████████| 204/204 [00:09<00:00, 22.16it/s]\n",
      "100%|██████████| 204/204 [00:08<00:00, 24.58it/s]\n",
      "100%|██████████| 204/204 [00:08<00:00, 24.77it/s]\n",
      "100%|██████████| 204/204 [00:08<00:00, 24.84it/s]\n",
      "100%|██████████| 204/204 [00:08<00:00, 24.65it/s]\n",
      "100%|██████████| 204/204 [00:08<00:00, 25.12it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.load('C:\\\\Users\\\\Home\\\\Desktop\\\\영어\\\\dataset\\\\test_x_multi.npy')\n",
    "\n",
    "test_dataset = VoiceDatasetSimple(X=X_test, y=None, transform=transform, inference=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=30, shuffle=False)\n",
    "\n",
    "result=0\n",
    "for person in range(3):\n",
    "    for fold in range(5):\n",
    "        with torch.no_grad():\n",
    "            weights = torch.load(f'model/network-epoch{epochs}-person({person})-fold({fold}).pth')\n",
    "            model.load_state_dict(weights['model'])\n",
    "            model.eval()\n",
    "            preds = []\n",
    "            for X in tqdm(test_loader):\n",
    "                X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "                pred = F.softmax(model(X)).detach().cpu().numpy().tolist()\n",
    "                preds+=pred\n",
    "        preds = np.array(preds)\n",
    "        result+=preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('C:\\\\Users\\\\Home\\\\Desktop\\\\영어\\\\sample_submission.csv')\n",
    "submission.iloc[:,1:] = result/15\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
