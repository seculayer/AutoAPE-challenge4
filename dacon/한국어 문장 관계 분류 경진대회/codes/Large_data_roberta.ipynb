{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 10 13:17:31 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:05.0 Off |                  Off |\r\n",
      "| N/A   47C    P0    93W / 300W |   1178MiB / 32510MiB |     83%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:00:06.0 Off |                  Off |\r\n",
      "| N/A   45C    P0    72W / 300W |    844MiB / 32510MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     65182      C   .../envs/test_env/bin/python     1175MiB |\r\n",
      "|    1   N/A  N/A      7312      C   .../envs/test_env/bin/python      841MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/centos/psw/KSRC'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_bVw3BzS66Gp"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast, BartModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "K7aUlj6L7eAH",
    "outputId": "575cdb6d-f613-45e8-82d4-146d2a0c2761",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159470</th>\n",
       "      <td>159470</td>\n",
       "      <td>같은 방향으로 보이는 숲이 우거진 지역에 의상을 입은 사람들이 모여 있다.</td>\n",
       "      <td>사람들은 의상을 입는다</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159471</th>\n",
       "      <td>159471</td>\n",
       "      <td>검은 원피스에 커다란 흰 활을 든 소녀가 카메라를 등지고 서 있다.</td>\n",
       "      <td>하이힐을 신은 남자</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159472</th>\n",
       "      <td>159472</td>\n",
       "      <td>검은 원피스에 커다란 흰 활을 든 소녀가 카메라를 등지고 서 있다.</td>\n",
       "      <td>서 있는 소녀</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159473</th>\n",
       "      <td>159473</td>\n",
       "      <td>검은 원피스에 커다란 흰 활을 든 소녀가 카메라를 등지고 서 있다.</td>\n",
       "      <td>사진 촬영 준비를 하고 있는 소녀</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159474</th>\n",
       "      <td>159474</td>\n",
       "      <td>한 남자가 유람선을 배경으로 부두에서 포즈를 취하고 있다.</td>\n",
       "      <td>포즈를 취하는 인간.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                                    premise          hypothesis  \\\n",
       "159470  159470  같은 방향으로 보이는 숲이 우거진 지역에 의상을 입은 사람들이 모여 있다.        사람들은 의상을 입는다   \n",
       "159471  159471      검은 원피스에 커다란 흰 활을 든 소녀가 카메라를 등지고 서 있다.          하이힐을 신은 남자   \n",
       "159472  159472      검은 원피스에 커다란 흰 활을 든 소녀가 카메라를 등지고 서 있다.             서 있는 소녀   \n",
       "159473  159473      검은 원피스에 커다란 흰 활을 든 소녀가 카메라를 등지고 서 있다.  사진 촬영 준비를 하고 있는 소녀   \n",
       "159474  159474           한 남자가 유람선을 배경으로 부두에서 포즈를 취하고 있다.         포즈를 취하는 인간.   \n",
       "\n",
       "                label  \n",
       "159470     entailment  \n",
       "159471  contradiction  \n",
       "159472     entailment  \n",
       "159473        neutral  \n",
       "159474     entailment  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dpath = '/content/drive/My Drive/Seculayer/KSRC/'\n",
    "train = pd.read_csv('data/with_kakao_train.csv',encoding='utf-8')\n",
    "train = train.rename(columns={'Unnamed: 0':\"index\"})\n",
    "test = pd.read_csv('data/test_data.csv',encoding='utf-8')\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "yv3YDRoOHPoX",
    "outputId": "0883bafc-6b61-4dd4-c270-51516ed832f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서 소년이나 장정들이 ...</td>\n",
       "      <td>씨름의 여자들의 놀이이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나 ...</td>\n",
       "      <td>자작극을 벌인 이는 3명이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다</td>\n",
       "      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n",
       "      <td>원주민들은 종합대책에 만족했다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면 이런 상황에서는 ...</td>\n",
       "      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            premise  \\\n",
       "0      0  씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서 소년이나 장정들이 ...   \n",
       "1      1  삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나 ...   \n",
       "2      2                     이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다   \n",
       "3      3  광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...   \n",
       "4      4  진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면 이런 상황에서는 ...   \n",
       "\n",
       "                                hypothesis          label  \n",
       "0                           씨름의 여자들의 놀이이다.  contradiction  \n",
       "1                         자작극을 벌인 이는 3명이다.  contradiction  \n",
       "2  예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.     entailment  \n",
       "3                        원주민들은 종합대책에 만족했다.        neutral  \n",
       "4       이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.        neutral  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불 용어 처리 \n",
    "\n",
    "train['premise'] = train['premise'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
    "test['premise'] = test['premise'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "hCfxgISsHvZl",
    "outputId": "43f42d44-7632-48bd-d175-3c42d7029276"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서 소년이나 장정들이 ...</td>\n",
       "      <td>씨름의 여자들의 놀이이다</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나 ...</td>\n",
       "      <td>자작극을 벌인 이는 3명이다</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다</td>\n",
       "      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n",
       "      <td>원주민들은 종합대책에 만족했다</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면 이런 상황에서는 ...</td>\n",
       "      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            premise  \\\n",
       "0      0  씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서 소년이나 장정들이 ...   \n",
       "1      1  삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나 ...   \n",
       "2      2                     이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다   \n",
       "3      3  광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...   \n",
       "4      4  진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면 이런 상황에서는 ...   \n",
       "\n",
       "                               hypothesis          label  \n",
       "0                           씨름의 여자들의 놀이이다  contradiction  \n",
       "1                         자작극을 벌인 이는 3명이다  contradiction  \n",
       "2  예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다     entailment  \n",
       "3                        원주민들은 종합대책에 만족했다        neutral  \n",
       "4       이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다        neutral  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['hypothesis'] = train['hypothesis'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
    "test['hypothesis'] = test['hypothesis'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159325 entries, 0 to 159474\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   index       159325 non-null  int64 \n",
      " 1   premise     159325 non-null  object\n",
      " 2   hypothesis  159325 non-null  object\n",
      " 3   label       159325 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train['length'] = train['premise'].apply(lambda x: len(x))\n",
    "train = train.loc[train['length']<105]\n",
    "train.drop('length',axis=1,inplace=True)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6FpW8SM7lwW",
    "outputId": "e779fb75-c660-4224-8296-fb17f2facfb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159475 entries, 0 to 159474\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   index       159475 non-null  int64 \n",
      " 1   premise     159475 non-null  object\n",
      " 2   hypothesis  159475 non-null  object\n",
      " 3   label       159475 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 4.9+ MB\n",
      "None\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1666 entries, 0 to 1665\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   index       1666 non-null   int64 \n",
      " 1   premise     1666 non-null   object\n",
      " 2   hypothesis  1666 non-null   object\n",
      " 3   label       1666 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 52.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 결측치는 없음\n",
    "\n",
    "print(train.info(), end='\\n\\n')\n",
    "print(test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4L1jeCl72DF",
    "outputId": "b5c8ce74-4bce-4603-a0f6-e8cbcd527b6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Label: \n",
      "entailment       53618\n",
      "contradiction    53468\n",
      "neutral          52389\n",
      "Name: label, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train Label: ', train['label'].value_counts(), sep='\\n', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "9SAoqqNmAU3x",
    "outputId": "cbe32eff-2df0-4a4f-9d17-50537c24cd33"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEGCAYAAACkQqisAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVqklEQVR4nO3df7RlZX3f8fdHBpQUEJDJlDDgUJ1lRFqpjIDaZEVJYKBVqAsVq5lBqdNUNGlXtcU2DQalatOUBn+QYBn5UVpErTK60HE6qIkm6AzKD4EQbhDDsFBGBwWiQga//eM8Fw/jvZc7z8w5lzvzfq211937u5+993POued+7v5x9klVIUlSj6fMdQckSfOXISJJ6maISJK6GSKSpG6GiCSp24K57sC4HXTQQbVkyZK57oYkzRvXX3/996pq4VTzdrsQWbJkCRs3bpzrbkjSvJHk29PN83CWJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqdtu94n17XH02y+b6y7s8q7/gxUjWe/fnPsPR7JePd5hv3fzXHdBc8w9EUlSN0NEktTNEJEkdfOciKQnnZe8/yVz3YVd3lfe+pWdsh73RCRJ3QwRSVI3Q0SS1M0QkSR1M0QkSd0MEUlSt5GGSJK7ktyc5IYkG1vtwCTrktzRfh7Q6klyQZKJJDclecHQela29nckWTlUP7qtf6Itm1E+HknS441jT+SlVXVUVS1r02cD66tqKbC+TQOcBCxtwyrgQhiEDnAOcCxwDHDOZPC0Nm8aWm756B+OJGnSXBzOOgW4tI1fCpw6VL+sBq4D9k9yMHAisK6qtlTV/cA6YHmbt19VXVdVBVw2tC5J0hiMOkQK+HyS65OsarVFVXVvG/8OsKiNHwLcPbTsplabqb5pivrPSbIqycYkGzdv3rwjj0eSNGTUtz35J1V1T5JfBNYl+cvhmVVVSWrEfaCqLgIuAli2bNnItydJu4uR7olU1T3t533AJxmc0/huOxRF+3lfa34PcOjQ4otbbab64inqkqQxGVmIJPl7SfadHAdOAL4JrAEmr7BaCVzdxtcAK9pVWscBP2yHvdYCJyQ5oJ1QPwFY2+Y9kOS4dlXWiqF1SZLGYJSHsxYBn2xX3S4A/ndVfS7JBuCqJGcC3wZe3dpfA5wMTAA/At4AUFVbkrwL2NDanVtVW9r4m4FLgL2Bz7ZBkjQmIwuRqroTeP4U9e8Dx09RL+Csada1Glg9RX0jcOQOd1aS1MVPrEuSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkrqNPESS7JHkG0k+06YPT/LVJBNJPppkr1Z/apueaPOXDK3jHa1+e5ITh+rLW20iydmjfiySpMcbx57I7wC3DU2/Dzi/qp4N3A+c2epnAve3+vmtHUmOAE4HngcsBz7UgmkP4IPAScARwGtbW0nSmIw0RJIsBv4p8D/bdICXAR9vTS4FTm3jp7Rp2vzjW/tTgCur6uGq+hYwARzThomqurOqHgGubG0lSWMy6j2R/wH8e+CnbfoZwA+qamub3gQc0sYPAe4GaPN/2No/Vt9mmenqPyfJqiQbk2zcvHnzDj4kSdKkkYVIkn8G3FdV149qG7NVVRdV1bKqWrZw4cK57o4k7TIWjHDdLwFekeRk4GnAfsAfAfsnWdD2NhYD97T29wCHApuSLACeDnx/qD5peJnp6pKkMRjZnkhVvaOqFlfVEgYnxq+tqtcBXwBOa81WAle38TVtmjb/2qqqVj+9Xb11OLAU+BqwAVjarvbaq21jzagejyTp541yT2Q6/wG4Msm7gW8AF7f6xcDlSSaALQxCgaq6JclVwK3AVuCsqnoUIMlbgLXAHsDqqrplrI9EknZzYwmRqvoi8MU2fieDK6u2bfMT4FXTLH8ecN4U9WuAa3ZiVyVJ28FPrEuSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkrrNKkSSrJ9NTZK0e5kxRJI8LcmBwEFJDkhyYBuWAIfMYtmvJbkxyS1Jfr/VD0/y1SQTST6aZK9Wf2qbnmjzlwyt6x2tfnuSE4fqy1ttIsnZ/U+DJKnHE+2J/CvgeuCX28/J4WrgA0+w7MPAy6rq+cBRwPIkxwHvA86vqmcD9wNntvZnAve3+vmtHUmOAE4HngcsBz6UZI8kewAfBE4CjgBe29pKksZkxhCpqj+qqsOBt1XVP6iqw9vw/KqaMURq4KE2uWcbCngZ8PFWvxQ4tY2f0qZp849Pkla/sqoerqpvARPAMW2YqKo7q+oR4MrWVpI0Jgtm06iq3p/kxcCS4WWq6rKZlmt7C9cDz2aw1/DXwA+qamtrsomfHRY7BLi7rXdrkh8Cz2j164ZWO7zM3dvUj52mH6uAVQCHHXbYTF2WJG2HWYVIksuBZwE3AI+2cgEzhkhVPQoclWR/4JMMDouNXVVdBFwEsGzZspqLPkjSrmhWIQIsA46oqq4/wFX1gyRfAF4E7J9kQdsbWQzc05rdAxwKbEqyAHg68P2h+qThZaarS5LGYLafE/km8Pe3Z8VJFrY9EJLsDfwGcBvwBeC01mwlg5P0AGvaNG3+tS201gCnt6u3DgeWAl8DNgBL29VeezE4+b5me/ooSdoxs90TOQi4NcnXGFx1BUBVvWKGZQ4GLm3nRZ4CXFVVn0lyK3BlkncD3wAubu0vBi5PMgFsYRAKVNUtSa4CbgW2Ame1w2QkeQuwFtgDWF1Vt8zy8UiSdoLZhsg7t3fFVXUT8I+nqN/J4Mqqbes/AV41zbrOA86bon4NcM329k2StHPM9uqsL426I5Kk+We2V2c9yOBqLIC9GHzm42+rar9RdUyS9OQ32z2RfSfHhz4AeNyoOiVJmh+2+y6+7ZPonwJOfKK2kqRd22wPZ71yaPIpDD438pOR9EiSNG/M9uqslw+NbwXuwvtUSdJub7bnRN4w6o5Ikuaf2X4p1eIkn0xyXxs+kWTxqDsnSXpym+2J9Y8wuKXIL7Xh060mSdqNzTZEFlbVR6pqaxsuARaOsF+SpHlgtiHy/SSvn/xGwSSvZ3CHXUnSbmy2IfJG4NXAd4B7Gdxl94wR9UmSNE/M9hLfc4GVVXU/QJIDgf/GIFwkSbup2e6J/KPJAAGoqi1McYdeSdLuZbYh8pQkB0xOtD2R2e7FSJJ2UbMNgj8E/iLJx9r0q5ji+z0kSbuX2X5i/bIkG4GXtdIrq+rW0XVLkjQfzPqQVAsNg0OS9JjtvhW8JEmTDBFJUjdDRJLUzRCRJHUzRCRJ3QwRSVI3Q0SS1M0QkSR1M0QkSd0MEUlSN0NEktRtZCGS5NAkX0hya5JbkvxOqx+YZF2SO9rPA1o9SS5IMpHkpiQvGFrXytb+jiQrh+pHJ7m5LXNBkozq8UiSft4o90S2Av+uqo4AjgPOSnIEcDawvqqWAuvbNMBJwNI2rAIuhMe+u+Qc4FjgGOCcoe82uRB409Byy0f4eCRJ2xhZiFTVvVX19Tb+IHAbcAhwCnBpa3YpcGobPwW4rAauA/ZPcjBwIrCuqra0b1dcByxv8/arquuqqoDLhtYlSRqDsZwTSbKEwdfpfhVYVFX3tlnfARa18UOAu4cW29RqM9U3TVGXJI3JyEMkyT7AJ4B/U1UPDM9rexA1hj6sSrIxycbNmzePenOStNsYaYgk2ZNBgFxRVf+3lb/bDkXRft7X6vcAhw4tvrjVZqovnqL+c6rqoqpaVlXLFi5cuGMPSpL0mFFenRXgYuC2qvrvQ7PWAJNXWK0Erh6qr2hXaR0H/LAd9loLnJDkgHZC/QRgbZv3QJLj2rZWDK1LkjQGs/563A4vAX4TuDnJDa32H4H3AlclORP4NvDqNu8a4GRgAvgR8AaAqtqS5F3Ahtbu3Kra0sbfDFwC7A18tg2SpDEZWYhU1ZeB6T63cfwU7Qs4a5p1rQZWT1HfCBy5A92UJO0AP7EuSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkboaIJKmbISJJ6maISJK6GSKSpG6GiCSpmyEiSepmiEiSuhkikqRuhogkqZshIknqZohIkrqNLESSrE5yX5JvDtUOTLIuyR3t5wGtniQXJJlIclOSFwwts7K1vyPJyqH60UlubstckCSjeiySpKmNck/kEmD5NrWzgfVVtRRY36YBTgKWtmEVcCEMQgc4BzgWOAY4ZzJ4Wps3DS237bYkSSM2shCpqj8FtmxTPgW4tI1fCpw6VL+sBq4D9k9yMHAisK6qtlTV/cA6YHmbt19VXVdVBVw2tC5J0piM+5zIoqq6t41/B1jUxg8B7h5qt6nVZqpvmqI+pSSrkmxMsnHz5s079ggkSY+ZsxPrbQ+ixrSti6pqWVUtW7hw4Tg2KUm7hXGHyHfboSjaz/ta/R7g0KF2i1ttpvriKeqSpDEad4isASavsFoJXD1UX9Gu0joO+GE77LUWOCHJAe2E+gnA2jbvgSTHtauyVgytS5I0JgtGteIk/wf4NeCgJJsYXGX1XuCqJGcC3wZe3ZpfA5wMTAA/At4AUFVbkrwL2NDanVtVkyfr38zgCrC9gc+2QZI0RiMLkap67TSzjp+ibQFnTbOe1cDqKeobgSN3pI+SpB3jJ9YlSd0MEUlSN0NEktTNEJEkdTNEJEndDBFJUjdDRJLUzRCRJHUzRCRJ3QwRSVI3Q0SS1M0QkSR1M0QkSd0MEUlSN0NEktTNEJEkdTNEJEndDBFJUjdDRJLUzRCRJHUzRCRJ3QwRSVI3Q0SS1M0QkSR1M0QkSd0MEUlSN0NEktTNEJEkdTNEJEndDBFJUrd5HyJJlie5PclEkrPnuj+StDuZ1yGSZA/gg8BJwBHAa5McMbe9kqTdx7wOEeAYYKKq7qyqR4ArgVPmuE+StNtIVc11H7olOQ1YXlX/sk3/JnBsVb1lm3argFVt8jnA7WPt6PgcBHxvrjuhbr5+89uu/Po9s6oWTjVjwbh7Mheq6iLgornux6gl2VhVy+a6H+rj6ze/7a6v33w/nHUPcOjQ9OJWkySNwXwPkQ3A0iSHJ9kLOB1YM8d9kqTdxrw+nFVVW5O8BVgL7AGsrqpb5rhbc2mXP2S3i/P1m992y9dvXp9YlyTNrfl+OEuSNIcMEUlSN0PkSSbJkiT/Yies551J3tbGz03y6zO0PSrJyUPTr/AWMn2SnDqbuyYk+a0kK9r4Je0zT6Ps1xlJfmmU29COvX+TPLSz+zMOhsiTzxJgyl/CJF0XQlTV71XV/5uhyVHAYyFSVWuq6r092xKnMrgFz4yq6o+r6rLRd+cxZwCGyOgtYSe/f5/sDJGdLMmKJDcluTHJ5e0/k2tbbX2Sw1q7S5JckOTPk9w59J/oe4FfSXJDkn/b/oNck+RaYH2Sfdp6vp7k5iSnDG37PyX5qyRfZvDJfIa2dVobf2Hb5o1Jvpbk6cC5wGvaNl/TtvmB1n57+7/LSfL69lzdkORPkuyR5KEk57Xn8boki5K8GHgF8Aet7bOSvCnJhtbuE0l+oa3zsT3FbbZ1V5L3tOU3JnlBkrVJ/jrJbw21e3tb701Jfr/VliS5LcmHk9yS5PNJ9m6vzTLgirbevcfzzM0fMzx3z0ryuSTXJ/mzJL/c2j9u73FoL6L7/TtvVZXDThqA5wF/BRzUpg8EPg2sbNNvBD7Vxi8BPsYgyI9gcA8wgF8DPjO0zjOATcCBbXoBsF8bPwiYAAIcDdwM/AKwX6u/bWhbpwF7AXcCL2z1/dr6zgA+sM02P9DGt6v/u9oAPLc9B3u26Q8BK4ACXt5q/xX43eHnemj5ZwyNvxt4axt/57avTxu/C/jXbfx84CZgX2Ah8N1WP4HB5aRpz/9ngF9l8F/wVuCo1u4q4PVt/IvAsrl+Pp+sw3TPHbAeWNpqxwLXTvM6P9R+dr1/h9cx34ZdcvdqDr0M+FhVfQ+gqrYkeRHwyjb/cgZ/cCZ9qqp+CtyaZNEM611XVVvaeID/kuRXgZ8ChwCLgF8BPllVPwJIMtWHLp8D3FtVG1r/HmhtZ3pMO6P/89nxDAJ6Q3ue9gbuAx5h8Mcb4HrgN6ZZ/sgk7wb2B/Zh8JmmJzL52t0M7FNVDwIPJnk4yf4MQuQE4But3T7AUuBvgG9V1Q1D/Voyi+1pYKrn7sXAx4beI0/tWO9s3r/f6ezznDNE5tbDQ+Mz/SX/26Hx1zH4r/Toqvq7JHcBTxtB32Zjtv2fzwJcWlXveFwxeVu1fx+BR5n+vXQJcGpV3ZjkDAb/qT6Ryef1pzz+Of5p206A91TVn2zTpyXbtH+UQehpdrZ97hYBP6iqo6Zou5V2OiDJUxjs5U/nyfr+3Sk8J7JzXQu8KskzAJIcCPw5g9uxwOAX6M+eYB0PMjh8MZ2nA/e1X8CXAs9s9T8FTm3HcfcFXj7FsrcDByd5Yevfvhmc7Jtpm9vb/13NeuC0JL8Ig9c0yTNnaL/tc7kvcG+SPRk8fzvDWuCNSfZpfTpksn/b0S89sQeAbyV5FUAGnt/m3cVgDxUG58H2bOO97995yz2RnaiqbklyHvClJI8yONzwVuAjSd4ObAbe8ASruQl4NMmNDP6LvX+b+VcAn05yM7AR+Mu27a8n+ShwI4PDLRum6N8jSV4DvL+dXP0x8OvAF4Czk9wAvGebxba3/7uUqro1ye8Cn2//cf4dcNYMi1wJfDjJbzM4D/Wfga8yeO6+yk74Q15Vn0/yXOAv2mGWhxgcv390hsUuAf44yY+BF1XVj3e0H7uJ1wEXtt+BPRm8vjcCHwaubu/Tz/GzvY2u9+985m1PJEndPJwlSepmiEiSuhkikqRuhogkqZshIknqZohII5QnuDNru2fTN7dznSO/6680W4aIJKmbISKNwRPcvXVBkivaXWQ/PnSn36OTfKndQXZtkoPnqPvStAwRaTx+AvzzqnoB8FLgD/Ozu/o9B/hQVT2Xwa023txuk/J+BneKPRpYDZw3B/2WZuRtT6TxmO7urQB3V9VX2vj/An6bwa00jgTWtazZA7h3rD2WZsEQkcZjpru3bnvvoWIQOrdU1YvG10Vp+3k4SxqPme7eelj73hkYfLXqlxnccXnhZD3JnkmeN9YeS7NgiEjjcQWwrN29dQWPv3vr7cBZSW4DDgAurKpHGNwF+H3tjrA3MPiCJOlJxbv4SpK6uSciSepmiEiSuhkikqRuhogkqZshIknqZohIkroZIpKkbv8fzu1vSF32cgAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data imbalance는 사실상 존재하지 않음 \n",
    "\n",
    "sns.countplot(data=train,x='label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "T6hQolMqAkji"
   },
   "outputs": [],
   "source": [
    "max_premise = np.max(train['premise'].str.len())\n",
    "max_hypothesis = np.max(train['hypothesis'].str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3AApuxsJBcYi",
    "outputId": "744f3bd4-e74b-40cf-defb-abfd64225d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max premise = 104 \n",
      "max hypothesis = 102\n"
     ]
    }
   ],
   "source": [
    "print('max premise =',max_premise,\"\\nmax hypothesis =\",max_hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEKCAYAAADTgGjXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeuElEQVR4nO3dfZRc9X3f8fdHT8tasJbWLFr0FMlFtY3dGJMNyMEnJ4FYCOJatMYExw3CVarkgF0/xhZ2T6nt+hROc4KhDaSqUSRSAsYYF8WlBhmD0+QYjISxeDLRGptoJS1SvEICg4Tl/faP+5tlNJrdnb2auzOz83mds2fu/d17Z797pdnv3t+jIgIzM7M8pjU6ADMza11OImZmlpuTiJmZ5eYkYmZmuTmJmJlZbk4iZmaWW2FJRNKbJD1W9nVQ0sckdUvaImlHep2bzpekGyT1S9ou6cyy91qdzt8haXVRMZuZ2cRoMsaJSJoO7ALOBq4EhiLiGknrgLkR8RlJFwIfAS5M510fEWdL6ga2An1AANuAX4uI/YUHbmZmY5qs6qzzgB9HxHPAKmBTKt8EXJS2VwG3ROYhYI6kU4HzgS0RMZQSxxZg5STFbWZmY5gxSd/nUuC2tD0vIvak7UFgXtpeAOwsu2YglY1WPqqTTz45lixZcpwhm5m1l23btv1TRPRM5JrCk4ikWcB7gasqj0VESKpLfZqktcBagMWLF7N169Z6vK2ZWduQ9NxEr5mM6qwLgEcj4vm0/3yqpiK97k3lu4BFZdctTGWjlR8lItZHRF9E9PX0TCiRmplZTpORRD7Aa1VZAJuBUg+r1cDdZeWXpV5ay4EDqdrrXmCFpLmpJ9eKVGZmZg1WaHWWpNnAu4E/Kiu+BrhD0hrgOeCSVH4PWc+sfuBl4EMAETEk6YvAI+m8L0TEUJFxm5lZbSali+9k6+vrC7eJmJlNjKRtEdE3kWs8Yt3MzHJzEjEzs9ycRMzMLDcnETMzy22yRqxbHQwPDzM4OAhAb28v06b5bwAzayz/Fmohg4ODXH7jfVx+430jycTMrJH8JNJiOru6Gx2CmdkIP4mYmVluTiJmZpabk4iZmeXmJGJmZrk5iZiZWW7unTUFePyImTWKf9tMAR4/YmaN4ieRKcLjR8ysEfwkYmZmuflJpAVFRRuImVmjOIm0oMMv7edTt+9jxqyZbLxiRaPDMbM25iTSojq6upk1a2ajwzCzNuc2ETMzy81JxMzMcnMSMTOz3ApNIpLmSLpT0o8kPS3pnZK6JW2RtCO9zk3nStINkvolbZd0Ztn7rE7n75C0usiYW0mpl9bg4CBEo6Mxs3ZUdMP69cC3IuJiSbOA1wGfBe6PiGskrQPWAZ8BLgCWpa+zgZuAsyV1A1cDfWS/KrdJ2hwR+wuOvemVemkdOfwSs3sWu6HdzCZdYU8ikl4P/CZwM0BEvBoRLwCrgE3ptE3ARWl7FXBLZB4C5kg6FTgf2BIRQylxbAFWFhV3q+no6qbzpDmNDsPM2lSR1VlLgX3AX0r6gaSvSJoNzIuIPemcQWBe2l4A7Cy7fiCVjVZuZmYNVmQSmQGcCdwUEe8Afk5WdTUiIoI61eZLWitpq6St+/btq8dbmpnZOIpMIgPAQEQ8nPbvJEsqz6dqKtLr3nR8F7Co7PqFqWy08qNExPqI6IuIvp6enrr+IGZmVl1hSSQiBoGdkt6Uis4DngI2A6UeVquBu9P2ZuCy1EtrOXAgVXvdC6yQNDf15FqRyqxCqbfW7t27GR4ebnQ4ZtYGiu6d9RHg1tQz61ngQ2SJ6w5Ja4DngEvSufcAFwL9wMvpXCJiSNIXgUfSeV+IiKGC425JlXNqzZ8/v9EhmdkUV2gSiYjHyLrmVjqvyrkBXDnK+2wANtQ1uCnKc2qZ2WTyiHUzM8vNScTMzHJzEjEzs9ycRMzMLDcnETMzy81JxMzMcnMSMTOz3JxEzMwsNycRMzPLzUnEzMxycxIxM7Pcip6A0epg2Gupm1mTchJpAYODg1x+430cenE/s3sWNzocM7MRTiItorOrG/kxxMyajNtEzMwsNycRMzPLzUnEzMxycxIxM7PcnETMzCw3JxEzM8vNScTMzHJzEjEzs9wKTSKSfirpcUmPSdqayrolbZG0I73OTeWSdIOkfknbJZ1Z9j6r0/k7JK0uMmYzM6vdZDyJ/HZEnBERfWl/HXB/RCwD7k/7ABcAy9LXWuAmyJIOcDVwNnAWcHUp8ZiZWWM1ojprFbApbW8CLiorvyUyDwFzJJ0KnA9siYihiNgPbAFWTnLMZmZWRdFJJID7JG2TtDaVzYuIPWl7EJiXthcAO8uuHUhlo5WbmVmDFT0B47siYpekU4Atkn5UfjAiQlJdZhVMSWotwOLFnunWzGwyFPokEhG70ute4BtkbRrPp2oq0uvedPouYFHZ5QtT2Wjlld9rfUT0RURfT09PvX8UMzOrorAkImm2pJNK28AK4AlgM1DqYbUauDttbwYuS720lgMHUrXXvcAKSXNTg/qKVGZmZg1WZHXWPOAbkkrf568j4luSHgHukLQGeA64JJ1/D3Ah0A+8DHwIICKGJH0ReCSd94WIGCowbjMzq1FhSSQingXeXqX8Z8B5VcoDuHKU99oAbKh3jGZmdnw8Yt3MzHJzEjEzs9ycRMzMLLeix4lYA8TwMIODgwD09vYybZr/VjCzYjiJTEGHX9rPp27fx4xZM9l4xQrmz5/f6JDMbIpyEpmiOrq6mTVrZqPDMLMpzvUcZmaWm5OImZnl5uqsKcwN7GZWNCeRKcwN7GZWNCeRKc4N7GZWJNdvmJlZbk4iZmaWm5OImZnl5iRiZma5uWG9DQ2766+Z1Yl/e7ShwcFBLr/xPi6/8b6RZGJmloefRNpI6QlkcHCQzpO6ifATiZkdHyeRNlJ6Ajn04n5m9yxm+NBBD0Y0s+PiJNJmOru6ETGy78GIZnY8XH9hZma5OYmYmVluhScRSdMl/UDSN9P+UkkPS+qX9FVJs1J5R9rvT8eXlL3HVan8GUnnFx2zmZnVZjKeRD4KPF22fy1wXUScBuwH1qTyNcD+VH5dOg9JpwOXAm8FVgI3Spo+CXGbmdk4Ck0ikhYCvwt8Je0LOBe4M52yCbgoba9K+6Tj56XzVwG3R8ThiPgJ0A+cVWTcZmZWm6KfRL4MfBoYTvtvAF6IiCNpfwBYkLYXADsB0vED6fyR8irXmJlZAxWWRCS9B9gbEduK+h4V32+tpK2Stu7bt28yvqWZWdsr8knkHOC9kn4K3E5WjXU9MEdSaXzKQmBX2t4FLAJIx18P/Ky8vMo1IyJifUT0RURfT09P/X8aMzM7Rk1JRNI5tZSVi4irImJhRCwhaxj/TkR8EHgAuDidthq4O21vTvuk49+JiEjll6beW0uBZcD3a4nbalNai3337t0MDw+Pf4GZWVLrk8h/q7GsFp8BPiGpn6zN4+ZUfjPwhlT+CWAdQEQ8CdwBPAV8C7gyIn6Z83tbFdla7Ns8IaOZTdiY055IeifwG0CPpE+UHeoCau5mGxEPAg+m7Wep0rsqIg4B7x/l+i8BX6r1+9nEefoTM8tjvLmzZgEnpvNOKis/yGtVUmZm1qbGTCIR8V3gu5I2RsRzkxSTmZm1iFpn8e2QtB5YUn5NRJxbRFDtrHzVQcjW+TAza1a1JpGvAX9BNvLcjdoFKq350dnVzSsHh9h4xYpGh2RmNqpak8iRiLip0EhsRGdXN51zPNbFzJpfrV18/0bSFZJOldRd+io0MjMza3q1PomUBgH+SVlZAG+sbzhmZtZKakoiEbG06EDMzKz11JREJF1WrTwibqlvONZIUdYzrLe3l2nTvPClmY2t1uqsXy/bPgE4D3gUcBKZQrLpT/YxfeZ0rn3fGfT29jqZmNmYaq3O+kj5vqQ5ZDPz2hTT0dXN8KGDfOr2bcyYNZONV6xg/vz5jQ7LzJpUrU8ilX4OuJ1kCvNcWmZWi1rbRP6GrDcWZBMvvoVsZl0zM2tjtT6J/GnZ9hHguYgYKCAeMzNrITW1mKaJGH9ENpPvXODVIoOy5uDFqsxsPLWubHgJ2WqC7wcuAR6W5KngpzgvVmVm46m1OutzwK9HxF4AST3At4E7iwrMmoMb2M1sLLUOAJhWSiDJzyZwrZmZTVG1Pol8S9K9wG1p//eAe4oJyczMWsV4a6yfBsyLiD+R9K+Bd6VD3wNuLTo4MzNrbuM9iXwZuAogIu4C7gKQ9C/SsX9ZYGxmZtbkxksi8yLi8crCiHhc0pJiQrJmNuxJGs2szHi/AeaMcaxzrAslnSDp+5J+KOlJSZ9P5UslPSypX9JXJc1K5R1pvz8dX1L2Xlel8mcknV/bj2ZFKC3f626/ZgbjJ5Gtkv5dZaGkPwS2jXPtYeDciHg7cAawUtJy4Frguog4DdgPrEnnrwH2p/Lr0nlIOh24FHgrsBK4UdL0Gn42K0hnVzedXV7Y0szGr876GPANSR/ktaTRB8wC/tVYF0ZEAC+l3ZnpK4Bzgd9P5ZuA/wTcBKxK25CNP/nvkpTKb4+Iw8BPJPUDZ5E17puZWQONmUQi4nngNyT9NvC2VPx/IuI7tbx5emLYBpwG/DnwY+CFiDiSThkAFqTtBcDO9H2PSDoAvCGVP1T2tuXXmJlZA9W6nsgDwAMTffOI+CVwRlp/5BvAmyf6HrWStBZYC7B48eKivo2ZmZWZlK41EfECWRJ6JzBHUil5LQR2pe1dwCKAdPz1ZCPjR8qrXFP+PdZHRF9E9PX09BTxY5iZWYXCkoiknvQEgqRO4N3A02TJpDR542rg7rS9Oe2Tjn8ntatsBi5NvbeWAsvIJoO0BvIMv2YG+Vc2rMWpwKbULjINuCMivinpKeB2Sf8Z+AFwczr/ZuCvUsP5EFmPLCLiSUl3AE+RrWVyZaomswYqrcfuJXTN2lthSSQitgPvqFL+LFnvqsryQ2RTzVd7ry8BX6p3jHZ8Orq6mTljugcfmrWxIp9ErA34icSsvTmJ2HHzmiNm7ct1D2ZmlpufRJpEaWLDwcHBbFy/mVkLcBJpEqWJDQ+9uJ/ZPYvHnt3SzKxJOIk0kc6ubuTHEDNrIU4iVldeb8SsvfgTbnXl9UbM2oufRKzuvNaIWftwErG6iLJqLAJQQ8Mxs0niJGJ1URq5fuTwS8zuWezBh2ZtwknE6qajq5uZh/xfyqyduGHdzMxy85+NDeaR6mbWypxEGqxypLqZWStxEmkCU3mkugcfmk1t/kRboTz40Gxq85NIg0z1tpDycSOdJ3V73IjZFOUk0iBTvS3E40bM2oOTSANN5bYQ8LgRs3bgT7g1JTfIm7UGfzJtUpTaSHbv3s3w8PC457tB3qw1FJZEJC2S9ICkpyQ9Kemjqbxb0hZJO9Lr3FQuSTdI6pe0XdKZZe+1Op2/Q9LqomK24mRtJNsmlBQ6u7o9I7BZkyvySeQI8MmIOB1YDlwp6XRgHXB/RCwD7k/7ABcAy9LXWuAmyJIOcDVwNnAWcHUp8Vhr6XBSMJtyCksiEbEnIh5N2y8CTwMLgFXApnTaJuCitL0KuCUyDwFzJJ0KnA9siYihiNgPbAFWFhW3Ncbw8DC7d++uubrLzJrDpDSsS1oCvAN4GJgXEXvSoUFgXtpeAOwsu2wglY1WblNIqQ0kYphr33dGVuh1ScyaXuFJRNKJwNeBj0XEQem13woREZLq0sdV0lqyajAWL5564y6mmsreV5C1gRw6+DM+dfs2jy8xaxGF9s6SNJMsgdwaEXel4udTNRXpdW8q3wUsKrt8YSobrfwoEbE+Ivoioq+np6e+P4jV3Vi9rzq6uuk8aU5jAjOzCSmyd5aAm4GnI+LPyg5tBko9rFYDd5eVX5Z6aS0HDqRqr3uBFZLmpgb1FanMWlwtva9KXYMHBgYYGBhwm4lZkymyOusc4A+AxyU9lso+C1wD3CFpDfAccEk6dg9wIdAPvAx8CCAihiR9EXgknfeFiBgqMG6bRMeszV6hfPqUGR0nMmPWTDZesYL58+dPbqBmVlVhSSQi/o7Rm0XPq3J+AFeO8l4bgA31i86aReUcW9WUpk+ZdkKX20jMmoynPbGG8xxbZq3Ln1xrKeE5tcyaipOItZRS9ZfbRsyag5OItZyOru6j2kY8469Z4/jTZi3PM/6aNY6fRGxK8MSOZo3hJxEzM8vNScTMzHJzEjEzs9ycRMzMLDc3rE+SalOfW32VBiKWJmicNm2au/yaFcxJZJKMuuiS1U3lZI3TZ07n2vedwSmnnAI4qZgVwUlkElVbdMnqq3yyxuFDB0fudXlS6e3tdTIxqxMnkQbwhIOTp1pS8ZQpZvXj32TWVjq6upk5Y7qnSTGrEycRazuVkzj29vY6qZjl5CRibal8EsdSpwfA1VxmE+QkYobn3jLLy8/tZmaWm59EzBKvmmg2cU4iZolXTTSbOCeRgpWmOxkcHPQI9RZQuWqimY2tsOd1SRsk7ZX0RFlZt6Qtknak17mpXJJukNQvabukM8uuWZ3O3yFpdVHxFqXU8+djm77Lq7/4RaPDsRqUqrV27949Mg+XmVVXZKXvRmBlRdk64P6IWAbcn/YBLgCWpa+1wE2QJR3gauBs4Czg6lLiaSWdXd10njSn0WFYjbJqrW0jy+0ODw+ze/fukaRSuW/WzgqrzoqIv5W0pKJ4FfBbaXsT8CDwmVR+S0QE8JCkOZJOTeduiYghAElbyBLTbUXFbQZjjyMBPK7ELJnsNpF5EbEnbQ8C89L2AmBn2XkDqWy0crNJVTmOpLOr2725zGhgw3pEhKS6NTVLWktWFcbixZ4d145feZIgAB19fKzpU8CJxdrDZCeR5yWdGhF7UnXV3lS+C1hUdt7CVLaL16q/SuUPVnvjiFgPrAfo6+tzPyg7buXrk8zuWXzUxI2lnnbVqr06u7p55eCQq7qsLUz2n0mbgVIPq9XA3WXll6VeWsuBA6na615ghaS5qUF9RSozmxQdZZ0iSg3uY/W06+zqpnNOj6dRsbZR2JOIpNvIniJOljRA1svqGuAOSWuA54BL0un3ABcC/cDLwIcAImJI0heBR9J5Xyg1sps1wkTXgqlcFtnVWzbVFNk76wOjHDqvyrkBXDnK+2wANtQxNLNJU9mzy9PO21TjEetmBShvlO88qXukUd7TzttU4yRiVoCxGuU7T+omYnhkICPAtGnT/GRiLclJxKwg5e0nlUklW+8925/RcaInfbSW5SRiNkkqG+VL+9NO6PKkj9aynEQK4tl7zawdOIkUpNSAeujF/czu8Qh6y6f0x4jbTqxZOYkUqLOrG/kxxGpQOQ8XMPIku+7r2zn00n63nVhTchIxawKV83ABRz3JdiqYdkLXUb28yp9IPKjRGsVJxKxJVK6qWO1JdrQlfCcy/sQJx+rJScSsxZQnm/IOHOWDGsfiAY9WT04iZi0oypJHqc2kclBj6Smj8skDjl0fxSwvJ5E6qfZBNStK5eDFzrQ0T7XqrmorM5rVi5NInfiDapNttBmFK9tW4NiVGMsX2XIbiR0PJ5E6chWBNbPKp5fR1pB3G4lNhJOIWRsZ7enFfwBZXk4ix+mY6U1q6B1j1gwqq7dKMwuDq7Wsdk4ix6lyehNPpGetYrSZhafPnM617zuD3t5eJxMbl5NIHXh6E2tV1WYWzpLJNk+xYjVxEjGzY1Tr4WVWjZ9TzcwsNz+J5OT1QszMnEQmbHiU6SbMpioPRrSxtEwSkbQSuB6YDnwlIq5pRByVvbFK002YTVWVgxF7e3udVGxESyQRSdOBPwfeDQwAj0jaHBFPTVYMlbOlujeWtZPywYjjJZXSOaX9akmm2lxz5Ss4lnglx+bXEkkEOAvoj4hnASTdDqwCCksilcuS7t2719VX1tbKByd2ntQ9MjixVLUbDHPt+84AOGq/NN4EXksuo11TWsHxyOGXmNFx4siYlVNOOQUYPalUfl5Lxjsfjk10oyW4auc2i0ZWObZKElkA7CzbHwDOLuqb7d69m8HBQT6+8UEOv3SAaR2zGT78c1538kIADh8cyv6Tv3rkmNdfzpo58o/5ysEhDr34QtXzxnt9ZdZMXjk4VJf3Ot6Yyq8fHBxsaCz1jKmIWF45OJS9Tsn7s58P/89/GPksHDn8UpX9bx+zP3PmDK67/LcA+PjGB+k4cQ4HB5+rek2lwy8dGDk+rWP2yHtVzpRd7fNay/nAMccrj5XirnZusyiP+bZ1l07q2B5FNH+1jKSLgZUR8Ydp/w+AsyPiw2XnrAXWpt03Ac9UeauTgX8qONyitGrsrRo3tG7srRo3tG7srRo3HB37r0REz0QubpUnkV3AorL9halsRESsB9aP9SaStkZEX/3DK16rxt6qcUPrxt6qcUPrxt6qccPxx958lXvVPQIsk7RU0izgUmBzg2MyM2t7LfEkEhFHJH0YuJesi++GiHiywWGZmbW9lkgiABFxD3DPcb7NmNVdTa5VY2/VuKF1Y2/VuKF1Y2/VuOE4Y2+JhnUzM2tOrdImYmZmTahtkoiklZKekdQvaV2j4xmNpEWSHpD0lKQnJX00lXdL2iJpR3qd2+hYq5E0XdIPJH0z7S+V9HC6719NHSOajqQ5ku6U9CNJT0t6Zyvcc0kfT/9PnpB0m6QTmvWeS9ogaa+kJ8rKqt5jZW5IP8N2SWc2LvJRY/+v6f/LdknfkDSn7NhVKfZnJJ3fkKCpHnfZsU9KCkknp/1c97wtkkjZtCkXAKcDH5B0emOjGtUR4JMRcTqwHLgyxboOuD8ilgH3p/1m9FHg6bL9a4HrIuI0YD+wpiFRje964FsR8Wbg7WQ/Q1Pfc0kLgH8P9EXE28g6nVxK897zjcDKirLR7vEFwLL0tRa4aZJiHM1Gjo19C/C2iPhV4B+AqwDS5/VS4K3pmhvT76BG2MixcSNpEbAC+Mey4lz3vC2SCGXTpkTEq0Bp2pSmExF7IuLRtP0i2S+zBWTxbkqnbQIuakiAY5C0EPhd4CtpX8C5wJ3plGaN+/XAbwI3A0TEqxHxAi1wz8k6x3RKmgG8DthDk97ziPhbYKiieLR7vAq4JTIPAXMknTopgVZRLfaIuC8ijqTdh8jGr0EW++0RcTgifgL0k/0OmnSj3HOA64BPc/RCFrnuebskkWrTpixoUCw1k7QEeAfwMDAvIvakQ4PAvEbFNYYvk/3HLE1g9AbghbIPWrPe96XAPuAvU1XcVyTNpsnveUTsAv6U7K/JPcABYButcc9LRrvHrfaZ/bfA/03bTR27pFXAroj4YcWhXHG3SxJpOZJOBL4OfCwiDpYfi6xLXVN1q5P0HmBvRGxrdCw5zADOBG6KiHcAP6ei6qpJ7/lcsr8elwLzgdlUqbpoFc14j2sh6XNk1dC3NjqW8Uh6HfBZ4D/W6z3bJYmMO21KM5E0kyyB3BoRd6Xi50uPlul1b6PiG8U5wHsl/ZSsuvBcsnaGOamqBZr3vg8AAxHxcNq/kyypNPs9/x3gJxGxLyJ+AdxF9u/QCve8ZLR73BKfWUmXA+8BPhivjZdo5tj/GdkfHT9Mn9WFwKOSeskZd7skkZaZNiW1I9wMPB0Rf1Z2aDOwOm2vBu6e7NjGEhFXRcTCiFhCdn+/ExEfBB4ALk6nNV3cABExCOyU9KZUdB7ZMgNNfc/JqrGWS3pd+n9Tirvp73mZ0e7xZuCy1GNoOXCgrNqrKShbKO/TwHsj4uWyQ5uBSyV1SFpK1lD9/UbEWCkiHo+IUyJiSfqsDgBnps9AvnseEW3xBVxI1oPix8DnGh3PGHG+i+yRfjvwWPq6kKx94X5gB/BtoLvRsY7xM/wW8M20/UayD1A/8DWgo9HxjRLzGcDWdN//NzC3Fe458HngR8ATwF8BHc16z4HbyNpufpF+ea0Z7R4DIutR+WPgcbIeaM0Wez9ZG0Lpc/oXZed/LsX+DHBBM8VdcfynwMnHc889Yt3MzHJrl+osMzMrgJOImZnl5iRiZma5OYmYmVluTiJmZpabk4hZk5A0X9Kd459p1jzcxdesTiRNj4hfNjoOs8nkJxGzGkhaktaOuDWtN3JnGin+U0nXSnoUeL+kFZK+J+lRSV9Lc6CRzvsvkh6TtFXSmZLulfRjSX9c9j2eSNtvlfT9dP52SctS+b8pK/8fDZxi3AxwEjGbiDcBN0bEW4CDwBWp/GcRcSbZiOv/APxO2t8KfKLs+n+MiDOA/0e2zsPFZGvGfL7K9/pj4Pp0fh8wIOktwO8B56TyXwIfrOPPZzZhM8Y/xcySnRHx92n7f5EtCAXw1fS6nGzRs7/PprJiFvC9sutL87U9DpwY2XoxL0o6XL4qXvI94HNpjZa7ImKHpPOAXwMeSe/fSfNNCmltxknErHaVDYil/Z+nVwFbIuIDo1x/OL0Ol22X9o/6LEbEX0t6mGyRr3sk/VF6/00RcVXO+M3qztVZZrVbLOmdafv3gb+rOP4QcI6k0wAkzZb0z/N8I0lvBJ6NiBvIZrb9VbKJCi+WdEo6p1vSr+R5f7N6cRIxq90zZGveP002y+9Ra1BHxD7gcuA2SdvJqqTenPN7XQI8Iekx4G1ky5Y+Rdbmcl96/y1Aw5aMNQN38TWrSVqq+JsR8bZGx2LWTPwkYmZmuflJxMzMcvOTiJmZ5eYkYmZmuTmJmJlZbk4iZmaWm5OImZnl5iRiZma5/X/keskK8JAiiAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(x=train['premise'].str.len());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "5xFFliUaCiwR",
    "outputId": "b7132a6d-24cb-4773-edaa-13d9d7e75af2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(x=train['hypothesis'].str.len());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed:int = 2023):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (12): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (13): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (14): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (15): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (16): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (17): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (18): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (19): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (20): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (21): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (22): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (23): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"learning_rate\": 2.5e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000,\n",
      "  \"warmup_ratio\": 0.09,\n",
      "  \"weight_decay\": 0.007\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'klue/roberta-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.num_labels = 3\n",
    "config.learning_rate = 2.5e-05\n",
    "config.warmup_ratio = 0.09\n",
    "config.weight_decay = 0.007\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "print(model)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,  6120,  2031,  2170,  2318,  9325,  2250,  2270,  2052,  2241,\n",
      "         4819,  1325,  3677,  2470, 12447,  2170,  3979,  2069,  5072,  2259,\n",
      "         1767,  2170,  3641,  2069, 10822,  2259, 27567,  1415,  2259,  4175,\n",
      "        15351, 10031,  2069,  3681,  2200,  3855,  2052,  5837,  2205,  2062,\n",
      "            2,  6120,  2031,  2073,  9325,  2250,  2270,  2069,  1889,  2015,\n",
      "         2090,  2097,  5873,  3855,  2069,  3605,     2,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1])\n",
      "[CLS] 대학생들에게 수강신청이란 취업 시 필요한 학점에 영향을 미치는 탓에 시간을 다투는 총성 없는 전쟁이나 다름없을 정도로 경쟁이 치열하다 [SEP] 대학생들은 수강신청을 하기위해 엄청난 경쟁을 한다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# train test split 및 tokenizing \n",
    "# token에 들어가는 문장은 premise와 hypothesis를 concat 한 문장\n",
    "\n",
    "train_dataset, eval_dataset = train_test_split(train, test_size=0.2, shuffle=True, stratify=train['label'])\n",
    "\n",
    "tokenized_train = tokenizer(\n",
    "    list(train_dataset['premise']),\n",
    "    list(train_dataset['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=210, # Max_Length = 190\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "tokenized_eval = tokenizer(\n",
    "    list(eval_dataset['premise']),\n",
    "    list(eval_dataset['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=210,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "print(tokenized_train['input_ids'][0])\n",
    "print(tokenizer.decode(tokenized_train['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pair_dataset, label):\n",
    "        self.pair_dataset = pair_dataset\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "        item['label'] = torch.tensor(self.label[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "def label_to_num(label):\n",
    "    label_dict = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2, \"answer\": 3}\n",
    "    num_label = []\n",
    "\n",
    "    for v in label:\n",
    "        num_label.append(label_dict[v])\n",
    "\n",
    "    return num_label\n",
    "\n",
    "\n",
    "train_label = label_to_num(train_dataset['label'].values)\n",
    "eval_label = label_to_num(eval_dataset['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BERTDataset(tokenized_train, train_label)\n",
    "eval_dataset = BERTDataset(tokenized_eval, eval_label)\n",
    "\n",
    "# print(train_dataset.__len__())\n",
    "# print(train_dataset.__getitem__(19997))\n",
    "# print(tokenizer.decode(train_dataset.__getitem__(19997)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "  \"\"\" validation을 위한 metrics function \"\"\"\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  probs = pred.predictions\n",
    "\n",
    "  # calculate accuracy using sklearn's function\n",
    "  acc = accuracy_score(labels, preds) # 리더보드 평가에는 포함되지 않습니다.\n",
    "\n",
    "  return {\n",
    "      'accuracy': acc,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_ars = TrainingArguments(\n",
    "    output_dir='result/Large_data_roberta/',\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_total_limit=5,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 500,\n",
    "    load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_ars,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 127460\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 13944\n",
      "  Number of trainable parameters = 336659459\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msangmi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/centos/psw/KSRC/wandb/run-20230110_132651-y2l1ua0g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sangmi/huggingface/runs/y2l1ua0g\" target=\"_blank\">result/Large_data_roberta/</a></strong> to <a href=\"https://wandb.ai/sangmi/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12501' max='13944' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12501/13944 4:21:46 < 30:13, 0.80 it/s, Epoch 6.28/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.690100</td>\n",
       "      <td>0.559950</td>\n",
       "      <td>0.790460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.550900</td>\n",
       "      <td>0.482720</td>\n",
       "      <td>0.820022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.512300</td>\n",
       "      <td>0.471710</td>\n",
       "      <td>0.831822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>0.458030</td>\n",
       "      <td>0.844532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.378500</td>\n",
       "      <td>0.455192</td>\n",
       "      <td>0.847074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.371900</td>\n",
       "      <td>0.459506</td>\n",
       "      <td>0.842398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.398900</td>\n",
       "      <td>0.449689</td>\n",
       "      <td>0.846823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.387600</td>\n",
       "      <td>0.462132</td>\n",
       "      <td>0.849239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.280500</td>\n",
       "      <td>0.468862</td>\n",
       "      <td>0.855139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>0.414123</td>\n",
       "      <td>0.859313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.288800</td>\n",
       "      <td>0.421173</td>\n",
       "      <td>0.863675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.282600</td>\n",
       "      <td>0.574655</td>\n",
       "      <td>0.857744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.210600</td>\n",
       "      <td>0.439579</td>\n",
       "      <td>0.864585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.473406</td>\n",
       "      <td>0.864271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.220300</td>\n",
       "      <td>0.455115</td>\n",
       "      <td>0.859124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.215200</td>\n",
       "      <td>0.480610</td>\n",
       "      <td>0.865495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.163000</td>\n",
       "      <td>0.448538</td>\n",
       "      <td>0.866531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.153500</td>\n",
       "      <td>0.519533</td>\n",
       "      <td>0.870453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.151500</td>\n",
       "      <td>0.542756</td>\n",
       "      <td>0.865087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.147000</td>\n",
       "      <td>0.548962</td>\n",
       "      <td>0.869292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.107600</td>\n",
       "      <td>0.553490</td>\n",
       "      <td>0.869198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.106300</td>\n",
       "      <td>0.542255</td>\n",
       "      <td>0.871332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.514826</td>\n",
       "      <td>0.873027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.573896</td>\n",
       "      <td>0.873435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='624' max='1992' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 624/1992 01:18 < 02:52, 7.92 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-500\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-500/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-500/special_tokens_map.json\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-1000\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-1000/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-1000/special_tokens_map.json\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-1500\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-1500/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-1500/special_tokens_map.json\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-2000\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-2000/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-2000/special_tokens_map.json\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-2500\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-2500/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-2500/special_tokens_map.json\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-3000\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-3000/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-500] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-3500\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-3500/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-1000] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-4000\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-4000/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-1500] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-4500\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-4500/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-2000] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-5000\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-5000/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-2500] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-5500\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-5500/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-3000] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-6000\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-6000/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-3500] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-6500\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-6500/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-4000] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-7000\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-7000/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-4500] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-7500\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-7500/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-5500] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-8000\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-8000/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-6000] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-8500\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-8500/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-6500] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-9000\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-9000/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-7000] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-9500\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-9500/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-7500] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-10000\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-10000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in result/Large_data_roberta/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-8000] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-10500\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-10500/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-8500] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-11000\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-11000/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-9000] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-11500\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-11500/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-9500] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/Large_data_roberta/checkpoint-12000\n",
      "Configuration saved in result/Large_data_roberta/checkpoint-12000/config.json\n",
      "Model weights saved in result/Large_data_roberta/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in result/Large_data_roberta/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in result/Large_data_roberta/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [result/Large_data_roberta/checkpoint-10000] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 31865\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2060/215002418.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result/Large_data_roberta/best_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         )\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1824\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1826\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1827\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2087\u001b[0m                     )\n\u001b[1;32m   2088\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2089\u001b[0;31m                 \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2090\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2801\u001b[0m             \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m             \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2803\u001b[0;31m             \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2804\u001b[0m         )\n\u001b[1;32m   2805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2995\u001b[0m                 )\n\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2997\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_across_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2998\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2999\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_logits_for_metrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_pad_across_processes\u001b[0;34m(self, tensor, pad_index)\u001b[0m\n\u001b[1;32m   3131\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3132\u001b[0m         \u001b[0;31m# Gather all sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3133\u001b[0;31m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3134\u001b[0m         \u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('result/Large_data_roberta/best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/centos/.cache/huggingface/hub/models--klue--roberta-large/snapshots/5193b95701189160c45d02a1033a4ea55bdbe259/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/centos/.cache/huggingface/hub/models--klue--roberta-large/snapshots/5193b95701189160c45d02a1033a4ea55bdbe259/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/centos/.cache/huggingface/hub/models--klue--roberta-large/snapshots/5193b95701189160c45d02a1033a4ea55bdbe259/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/centos/.cache/huggingface/hub/models--klue--roberta-large/snapshots/5193b95701189160c45d02a1033a4ea55bdbe259/tokenizer_config.json\n",
      "loading configuration file result/Large_data_roberta/checkpoint-5000/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"result/Large_data_roberta/checkpoint-5000\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"learning_rate\": 2.5e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000,\n",
      "  \"warmup_ratio\": 0.09,\n",
      "  \"weight_decay\": 0.007\n",
      "}\n",
      "\n",
      "loading weights file result/Large_data_roberta/checkpoint-5000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at result/Large_data_roberta/checkpoint-5000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='klue/roberta-large', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "Tokenizer_NAME = \"klue/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(Tokenizer_NAME)\n",
    "\n",
    "MODEL_NAME = 'result/Large_data_roberta/checkpoint-5000'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "model.to(device)\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1666\n",
      "{'input_ids': tensor([    0,   720,  3994,  2052, 10428,  2775,   647,  3657,  2119,  1085,\n",
      "            3,     2,   720,  3994,  2052,   911,  2075,  3669,  2119,  3926,\n",
      "         2088,  1513,  2359, 13964,     2,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(3)}\n",
      "[CLS] 18일 귀국이라 발인도 지켜드리지 못해 더욱 죄송할 따름입니다 [SEP] 18일 배를 타고 여행을 떠났습니다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "test_label = label_to_num(test['label'].values)\n",
    "\n",
    "tokenized_test = tokenizer(\n",
    "    list(test['premise']),\n",
    "    list(test['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "test_dataset = BERTDataset(tokenized_test, test_label)\n",
    "\n",
    "print(test_dataset.__len__())\n",
    "print(test_dataset.__getitem__(1665))\n",
    "print(tokenizer.decode(test_dataset.__getitem__(6)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:07<00:00, 14.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 0, 1, 0, 2, 2, 2, 0, 2, 1, 0, 1, 0, 2, 0, 1, 2, 1, 2, 2, 2, 2, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 2, 2, 1, 0, 1, 0, 1, 2, 0, 2, 2, 2, 1, 2, 0, 1, 0, 2, 1, 1, 2, 2, 2, 1, 2, 2, 0, 0, 2, 2, 2, 2, 1, 2, 2, 2, 1, 0, 2, 0, 1, 2, 2, 0, 2, 2, 2, 2, 1, 1, 0, 0, 1, 2, 1, 0, 2, 1, 2, 0, 1, 0, 2, 1, 1, 0, 2, 2, 0, 2, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 0, 2, 1, 0, 1, 1, 1, 1, 1, 1, 2, 2, 0, 0, 1, 1, 2, 1, 2, 1, 0, 2, 0, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0, 0, 0, 2, 1, 1, 0, 0, 1, 0, 2, 2, 2, 0, 2, 0, 2, 2, 0, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 2, 1, 2, 2, 1, 2, 0, 1, 2, 2, 0, 1, 2, 0, 2, 2, 2, 0, 1, 1, 0, 2, 2, 0, 2, 1, 1, 1, 2, 2, 1, 1, 0, 1, 1, 2, 2, 0, 1, 2, 1, 1, 2, 1, 1, 0, 2, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 1, 2, 0, 2, 2, 1, 1, 0, 1, 2, 0, 1, 2, 1, 1, 0, 0, 1, 0, 2, 2, 2, 2, 1, 2, 1, 0, 1, 2, 0, 0, 2, 1, 1, 0, 2, 1, 2, 0, 0, 0, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1, 0, 1, 1, 1, 2, 0, 2, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 0, 1, 0, 1, 2, 1, 1, 2, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 2, 1, 1, 2, 0, 1, 1, 1, 2, 1, 2, 0, 1, 2, 1, 2, 0, 1, 2, 1, 1, 1, 2, 1, 2, 1, 0, 1, 2, 1, 1, 0, 2, 2, 0, 0, 2, 0, 0, 0, 2, 1, 2, 0, 2, 0, 0, 2, 0, 2, 0, 1, 2, 1, 2, 2, 2, 1, 0, 1, 0, 1, 1, 0, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 0, 2, 1, 2, 2, 0, 2, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 2, 0, 1, 1, 1, 0, 0, 1, 1, 0, 2, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 2, 0, 2, 1, 2, 2, 0, 0, 2, 0, 1, 1, 1, 2, 0, 0, 1, 2, 2, 0, 2, 1, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 1, 1, 0, 2, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 2, 2, 1, 0, 0, 1, 0, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 0, 1, 2, 1, 0, 2, 0, 2, 1, 0, 0, 0, 2, 0, 2, 2, 1, 0, 1, 2, 2, 2, 2, 0, 0, 2, 1, 2, 0, 0, 1, 0, 1, 0, 2, 1, 2, 1, 0, 0, 2, 0, 0, 2, 0, 1, 0, 0, 2, 1, 0, 1, 1, 1, 2, 0, 0, 1, 2, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 2, 0, 1, 1, 2, 2, 1, 2, 1, 2, 0, 1, 0, 1, 1, 1, 2, 0, 0, 0, 2, 0, 2, 1, 1, 0, 0, 1, 1, 0, 2, 0, 2, 0, 2, 1, 1, 2, 0, 1, 1, 0, 2, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 0, 2, 2, 1, 1, 2, 1, 0, 1, 1, 1, 0, 1, 0, 0, 2, 0, 0, 2, 0, 1, 0, 1, 0, 0, 2, 0, 0, 2, 0, 0, 1, 1, 2, 1, 2, 0, 2, 0, 0, 0, 2, 2, 2, 0, 0, 2, 1, 1, 0, 1, 2, 1, 1, 0, 0, 2, 0, 2, 0, 2, 1, 2, 2, 0, 2, 0, 0, 0, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 0, 2, 2, 1, 2, 2, 0, 2, 0, 1, 2, 0, 2, 0, 1, 2, 1, 2, 0, 0, 2, 2, 1, 0, 2, 2, 0, 2, 1, 2, 0, 0, 1, 1, 0, 1, 0, 1, 0, 2, 2, 0, 1, 0, 0, 2, 0, 1, 0, 2, 1, 1, 0, 0, 1, 1, 0, 1, 1, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 0, 1, 1, 0, 2, 1, 0, 1, 2, 1, 1, 0, 1, 0, 2, 2, 1, 1, 0, 2, 2, 1, 0, 2, 1, 0, 0, 2, 1, 0, 1, 2, 2, 2, 2, 0, 0, 2, 1, 2, 2, 1, 0, 2, 2, 1, 0, 1, 2, 2, 2, 1, 1, 0, 0, 1, 2, 2, 0, 2, 0, 0, 0, 0, 1, 2, 0, 2, 0, 2, 1, 1, 0, 0, 0, 2, 1, 1, 2, 0, 2, 2, 2, 0, 2, 2, 1, 1, 2, 0, 2, 0, 0, 1, 2, 1, 0, 0, 0, 1, 2, 0, 2, 2, 1, 2, 0, 2, 0, 1, 0, 0, 1, 2, 2, 0, 0, 2, 2, 0, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 2, 1, 1, 1, 0, 2, 1, 0, 0, 1, 0, 0, 2, 1, 1, 0, 0, 2, 0, 0, 0, 2, 0, 1, 2, 2, 1, 1, 2, 1, 0, 0, 2, 1, 2, 0, 2, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 1, 2, 0, 0, 0, 1, 1, 0, 2, 2, 2, 0, 2, 2, 1, 1, 1, 0, 2, 0, 0, 2, 1, 2, 1, 1, 1, 1, 2, 1, 0, 1, 2, 1, 0, 1, 1, 0, 1, 0, 0, 2, 2, 2, 1, 0, 2, 2, 1, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 2, 1, 0, 2, 0, 1, 1, 1, 1, 1, 0, 1, 2, 0, 2, 0, 1, 2, 2, 0, 2, 2, 1, 0, 1, 2, 0, 1, 0, 1, 1, 0, 0, 0, 2, 1, 1, 2, 2, 2, 2, 0, 1, 1, 0, 2, 1, 2, 1, 0, 1, 2, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 1, 1, 0, 2, 0, 0, 0, 1, 0, 1, 2, 1, 0, 2, 2, 0, 2, 2, 0, 1, 1, 1, 2, 2, 0, 0, 2, 2, 1, 2, 1, 2, 2, 0, 2, 0, 2, 1, 2, 0, 1, 2, 1, 2, 2, 2, 0, 1, 0, 0, 0, 2, 0, 0, 1, 2, 1, 1, 0, 1, 0, 2, 2, 2, 1, 1, 1, 1, 2, 0, 0, 1, 2, 1, 2, 0, 2, 2, 2, 1, 2, 2, 0, 1, 1, 0, 0, 2, 1, 1, 0, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 1, 0, 1, 1, 1, 2, 0, 2, 0, 2, 1, 2, 1, 0, 0, 0, 0, 2, 0, 0, 2, 2, 2, 0, 1, 1, 1, 1, 0, 1, 0, 1, 2, 2, 0, 1, 0, 1, 0, 1, 0, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 0, 1, 1, 2, 1, 2, 1, 0, 2, 2, 2, 2, 0, 2, 0, 1, 2, 1, 2, 2, 0, 2, 2, 0, 2, 1, 1, 0, 2, 1, 0, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 0, 0, 2, 0, 1, 0, 2, 0, 1, 1, 2, 1, 2, 2, 2, 1, 0, 0, 1, 1, 1, 0, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 0, 2, 1, 2, 1, 2, 2, 2, 1, 0, 1, 1, 1, 2, 0, 1, 0, 0, 1, 2, 1, 1, 2, 2, 0, 0, 2, 0, 0, 0, 1, 2, 2, 2, 0, 0, 1, 0, 0, 1, 1, 1, 2, 2, 0, 1, 2, 2, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 2, 2, 0, 2, 2, 0, 2, 2, 1, 0, 2, 1, 1, 0, 1, 0, 2, 1, 1, 2, 1, 0, 0, 2, 0, 2, 0, 1, 1, 2, 1, 2, 1, 2, 1, 0, 0, 1, 1, 2, 2, 1, 0, 1, 2, 1, 1, 2, 2, 0, 0, 1, 1, 1, 1, 2, 0, 2, 2, 1, 1, 0, 1, 0, 2, 0, 2, 0, 1, 1, 0, 1, 1, 0, 1, 0, 2, 1, 0, 0, 2, 0, 2, 1, 1, 2, 1, 1, 0, 2, 1, 1, 2, 1, 2, 0, 1, 2, 1, 2, 2, 1, 2, 0, 0, 1, 2, 1, 2, 0, 0, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 0, 2, 0, 2, 2, 0, 2, 0, 0, 2, 0, 2, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 1, 1, 1, 2, 1, 2, 2, 2, 1, 0, 0, 2, 2, 2, 2, 0, 0, 1, 1, 1, 0, 2, 2, 0, 0, 0, 2, 1, 1, 1, 2, 0, 2, 0, 0, 1, 0, 2, 2, 0, 2, 0, 1, 2, 2, 2, 1, 2, 0, 1, 2, 1, 2, 2, 1, 1, 2, 1, 0, 0, 0, 2, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 0, 1, 1, 0, 2, 1, 1, 0, 1, 1, 2, 2, 0, 2, 1, 2, 2, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "output_pred = []\n",
    "output_prob = []\n",
    "\n",
    "for i, data in enumerate(tqdm(dataloader)):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=data['input_ids'].to(device),\n",
    "            attention_mask=data['attention_mask'].to(device),\n",
    "            token_type_ids=data['token_type_ids'].to(device)\n",
    "        )\n",
    "    logits = outputs[0]\n",
    "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    result = np.argmax(logits, axis=-1)\n",
    "\n",
    "    output_pred.append(result)\n",
    "    output_prob.append(prob)\n",
    "  \n",
    "pred_answer, output_prob = np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()\n",
    "print(pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.37633875012397766, 0.33025845885276794, 0.293402761220932]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_prob[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.000503677292726934, 0.9964536428451538, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.053734514862298965, 0.002680903533473611, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0.011614812538027763, 0.003720239968970418, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0.0011501049157232046, 0.9467388391494751, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0.012297039851546288, 0.9785420894622803, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               prob\n",
       "0      0  [0.000503677292726934, 0.9964536428451538, 0.0...\n",
       "1      1  [0.053734514862298965, 0.002680903533473611, 0...\n",
       "2      2  [0.011614812538027763, 0.003720239968970418, 0...\n",
       "3      3  [0.0011501049157232046, 0.9467388391494751, 0....\n",
       "4      4  [0.012297039851546288, 0.9785420894622803, 0.0..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(prob_of_output, columns=['index', 'prob'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_of_output = []\n",
    "for i, v in enumerate(output_prob):\n",
    "    prob_of_output.append([i,v])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, [0.000503677292726934, 0.9964536428451538, 0.0030427409801632166]],\n",
       " [1, [0.053734514862298965, 0.002680903533473611, 0.9435845017433167]],\n",
       " [2, [0.011614812538027763, 0.003720239968970418, 0.9846648573875427]],\n",
       " [3, [0.0011501049157232046, 0.9467388391494751, 0.052111007273197174]],\n",
       " [4, [0.012297039851546288, 0.9785420894622803, 0.009160801768302917]],\n",
       " [5, [0.002121122321113944, 0.7874798774719238, 0.21039903163909912]],\n",
       " [6, [0.007405277341604233, 0.007436033338308334, 0.9851586222648621]],\n",
       " [7, [0.3682658076286316, 0.4189690351486206, 0.212765172123909]],\n",
       " [8, [0.9745813012123108, 0.002231544116511941, 0.023187166079878807]],\n",
       " [9, [0.07236459851264954, 0.7306638360023499, 0.19697163999080658]],\n",
       " [10, [0.00907385628670454, 0.9000028371810913, 0.0909232422709465]],\n",
       " [11, [0.9796241521835327, 0.0037880379240959883, 0.01658778265118599]],\n",
       " [12, [0.00038295050035230815, 0.9957523345947266, 0.0038646692410111427]],\n",
       " [13, [0.9867089986801147, 0.00841760728508234, 0.004873491823673248]],\n",
       " [14, [0.0021429387852549553, 0.003980581648647785, 0.9938765168190002]],\n",
       " [15, [0.0037626619450747967, 0.000912079936824739, 0.9953252077102661]],\n",
       " [16, [0.9490405321121216, 0.002949586370959878, 0.04800979420542717]],\n",
       " [17, [0.0011518232058733702, 0.001701605855487287, 0.9971465468406677]],\n",
       " [18, [0.0008374006138183177, 0.9907324314117432, 0.008430112153291702]],\n",
       " [19, [0.0013309952337294817, 0.0015570656396448612, 0.9971119165420532]],\n",
       " [20, [0.0007060300558805466, 0.9826953411102295, 0.016598623245954514]],\n",
       " [21, [0.010259237140417099, 0.010854349471628666, 0.9788863658905029]],\n",
       " [22, [0.723028302192688, 0.18745432794094086, 0.08951740711927414]],\n",
       " [23, [0.9941421151161194, 0.001854465575888753, 0.004003394395112991]],\n",
       " [24, [0.0003094435087405145, 0.9946264624595642, 0.005064067896455526]],\n",
       " [25, [0.0023276028223335743, 0.0189397893846035, 0.9787325263023376]],\n",
       " [26, [0.13948412239551544, 0.003085492178797722, 0.8574303388595581]],\n",
       " [27, [0.8455102443695068, 0.002356736920773983, 0.152133047580719]],\n",
       " [28, [0.9739716053009033, 0.0023103237617760897, 0.0237180944532156]],\n",
       " [29, [0.5892155170440674, 0.009167459793388844, 0.40161705017089844]],\n",
       " [30, [0.0007841497426852584, 0.9836137294769287, 0.015602142550051212]],\n",
       " [31, [0.9305335283279419, 0.0528266504406929, 0.016639739274978638]],\n",
       " [32, [0.00045006044092588127, 0.9899131059646606, 0.00963680911809206]],\n",
       " [33, [0.0010090639116242528, 0.01182253286242485, 0.9871683120727539]],\n",
       " [34, [0.017384514212608337, 0.0018127132207155228, 0.9808027744293213]],\n",
       " [35, [0.040562525391578674, 0.27977147698402405, 0.6796659827232361]],\n",
       " [36, [0.2276102900505066, 0.019539879634976387, 0.7528497576713562]],\n",
       " [37, [0.0005972830695100129, 0.998082160949707, 0.0013206250732764602]],\n",
       " [38, [0.9710388779640198, 0.0019695572555065155, 0.02699146792292595]],\n",
       " [39, [0.0004367437504697591, 0.9677574634552002, 0.031805723905563354]],\n",
       " [40, [0.0010890676639974117, 0.026255179196596146, 0.972655713558197]],\n",
       " [41, [0.21894605457782745, 0.039632461965084076, 0.7414215207099915]],\n",
       " [42, [0.002389011438935995, 0.004523868672549725, 0.993087112903595]],\n",
       " [43, [0.009314727038145065, 0.00206478638574481, 0.9886205196380615]],\n",
       " [44, [0.002398495562374592, 0.011917522177100182, 0.9856840372085571]],\n",
       " [45, [0.0005014208145439625, 0.9940151572227478, 0.005483497399836779]],\n",
       " [46, [0.0014074029168114066, 0.0012170918053016067, 0.99737548828125]],\n",
       " [47, [0.9743812680244446, 0.0008049691095948219, 0.02481376752257347]],\n",
       " [48, [0.0029948255978524685, 0.9864107966423035, 0.010594366118311882]],\n",
       " [49, [0.9882453680038452, 0.006478500086814165, 0.0052761356346309185]],\n",
       " [50, [0.0034726005978882313, 0.001349249156191945, 0.99517822265625]],\n",
       " [51, [0.0006149269174784422, 0.9961512684822083, 0.0032337179873138666]],\n",
       " [52, [0.324916273355484, 0.5222955942153931, 0.15278814733028412]],\n",
       " [53, [0.0009800288826227188, 0.0024192878045141697, 0.9966006875038147]],\n",
       " [54, [0.011847189627587795, 0.005571753717958927, 0.9825810790061951]],\n",
       " [55, [0.9282179474830627, 0.026438431814312935, 0.04534362256526947]],\n",
       " [56, [0.0010690104681998491, 0.0036521267611533403, 0.9952788352966309]],\n",
       " [57, [0.0010281368158757687, 0.005727077834308147, 0.9932448267936707]],\n",
       " [58, [0.0517323762178421, 0.06352666765451431, 0.8847408890724182]],\n",
       " [59, [0.010034413076937199, 0.0011474767234176397, 0.9888179898262024]],\n",
       " [60, [0.9829346537590027, 0.004903058055788279, 0.012162232771515846]],\n",
       " [61, [0.0013631954789161682, 0.008356642909348011, 0.9902800917625427]],\n",
       " [62, [0.0012715670745819807, 0.0014244706835597754, 0.99730384349823]],\n",
       " [63, [0.006324705667793751, 0.0031026769429445267, 0.9905726313591003]],\n",
       " [64, [0.008199968375265598, 0.0031443031039088964, 0.988655686378479]],\n",
       " [65, [0.00037944011273793876, 0.9981973767280579, 0.0014232267858460546]],\n",
       " [66, [0.0026999737601727247, 0.0008239050512202084, 0.9964761137962341]],\n",
       " [67, [0.001573039568029344, 0.0016880517359822989, 0.996738851070404]],\n",
       " [68, [0.01139394473284483, 0.0015039605787023902, 0.9871020913124084]],\n",
       " [69, [0.0017572324723005295, 0.9890605211257935, 0.00918219517916441]],\n",
       " [70, [0.9562107920646667, 0.0012960591120645404, 0.04249317944049835]],\n",
       " [71, [0.0011217164574190974, 0.0035231350921094418, 0.9953551292419434]],\n",
       " [72, [0.994309663772583, 0.0024408467579632998, 0.0032495190389454365]],\n",
       " [73, [0.0020183343440294266, 0.9895500540733337, 0.008431565947830677]],\n",
       " [74, [0.001161132357083261, 0.002026352798566222, 0.9968125224113464]],\n",
       " [75, [0.004076729994267225, 0.0014759424375370145, 0.9944473505020142]],\n",
       " [76, [0.9462699294090271, 0.003803228959441185, 0.049926891922950745]],\n",
       " [77, [0.0033210190013051033, 0.0012279835063964128, 0.995451033115387]],\n",
       " [78, [0.001113679027184844, 0.0016931422287598252, 0.9971931576728821]],\n",
       " [79, [0.0015751478495076299, 0.9801666140556335, 0.01825825683772564]],\n",
       " [80, [0.0018450821517035365, 0.0017340036574751139, 0.9964209794998169]],\n",
       " [81, [0.007246129214763641, 0.005042867735028267, 0.9877110123634338]],\n",
       " [82, [0.0004464861995074898, 0.9968913197517395, 0.002662188373506069]],\n",
       " [83, [0.12343067675828934, 0.7003665566444397, 0.17620278894901276]],\n",
       " [84, [0.9121029376983643, 0.003523294348269701, 0.08437377214431763]],\n",
       " [85, [0.0027222291100770235, 0.9894060492515564, 0.007871740497648716]],\n",
       " [86, [0.0021167497616261244, 0.003896911395713687, 0.9939863681793213]],\n",
       " [87, [0.0006443304009735584, 0.9965866804122925, 0.0027690210845321417]],\n",
       " [88, [0.9846531748771667, 0.0015339809469878674, 0.013812810182571411]],\n",
       " [89, [0.000849657051730901, 0.00977459829300642, 0.9893757104873657]],\n",
       " [90, [0.0011075532529503107, 0.9968417882919312, 0.0020506519358605146]],\n",
       " [91, [0.00754521694034338, 0.0010583943221718073, 0.9913963675498962]],\n",
       " [92, [0.991166889667511, 0.0037525834050029516, 0.0050806039944291115]],\n",
       " [93, [0.00030750175938010216, 0.9825630187988281, 0.017129534855484962]],\n",
       " [94, [0.9767627120018005, 0.002000828506425023, 0.02123655192553997]],\n",
       " [95, [0.6201614737510681, 0.0044378372840583324, 0.37540072202682495]],\n",
       " [96, [0.00045235242578200996, 0.9966391324996948, 0.002908466150984168]],\n",
       " [97, [0.004045412410050631, 0.2708950936794281, 0.725059449672699]],\n",
       " [98, [0.9757964611053467, 0.0023211392108350992, 0.02188246138393879]],\n",
       " [99, [0.002507786499336362, 0.001390955876559019, 0.9961012601852417]],\n",
       " [100, [0.0015433451626449823, 0.0012017687549814582, 0.9972549080848694]],\n",
       " [101, [0.9739100933074951, 0.0017906720750033855, 0.02429921180009842]],\n",
       " [102, [0.0034045111387968063, 0.004524776246398687, 0.992070734500885]],\n",
       " [103, [0.994838535785675, 0.0009447882766835392, 0.004216611385345459]],\n",
       " [104, [0.0017552644712850451, 0.001877402770332992, 0.996367335319519]],\n",
       " [105, [0.9615573287010193, 0.022676894441246986, 0.0157657191157341]],\n",
       " [106, [0.002840193919837475, 0.002790021011605859, 0.9943698048591614]],\n",
       " [107, [0.00547404307872057, 0.0015756089705973864, 0.9929503202438354]],\n",
       " [108, [0.024332880973815918, 0.5565023422241211, 0.4191648066043854]],\n",
       " [109, [0.8541656732559204, 0.11247549951076508, 0.033358775079250336]],\n",
       " [110, [0.0007159892120398581, 0.9958873391151428, 0.0033966894261538982]],\n",
       " [111, [0.0013534682802855968, 0.001769605209119618, 0.9968769550323486]],\n",
       " [112, [0.0006795802037231624, 0.9957078695297241, 0.003612472442910075]],\n",
       " [113, [0.9906660914421082, 0.0017623776802793145, 0.007571511436253786]],\n",
       " [114, [0.00173332286067307, 0.0027164160273969173, 0.995550274848938]],\n",
       " [115, [0.5702488422393799, 0.040715523064136505, 0.3890356421470642]],\n",
       " [116, [0.7803575396537781, 0.12801721692085266, 0.09162525832653046]],\n",
       " [117, [0.0005661696777679026, 0.9857150912284851, 0.013718763366341591]],\n",
       " [118, [0.0020316042937338352, 0.949343204498291, 0.04862517863512039]],\n",
       " [119, [0.00047576724318787456, 0.9849551320075989, 0.014569094404578209]],\n",
       " [120, [0.0006838170229457319, 0.9963961243629456, 0.002920116065070033]],\n",
       " [121, [0.006160303950309753, 0.05293243005871773, 0.9409072995185852]],\n",
       " [122, [0.0009254731703549623, 0.9966513514518738, 0.002423156052827835]],\n",
       " [123, [0.0012304327683523297, 0.0016363809118047357, 0.9971332550048828]],\n",
       " [124, [0.045565176755189896, 0.008289842866361141, 0.9461449384689331]],\n",
       " [125, [0.40001434087753296, 0.07403957843780518, 0.5259460806846619]],\n",
       " [126, [0.9901711344718933, 0.0027505524922162294, 0.007078404538333416]],\n",
       " [127, [0.018853407353162766, 0.9734105467796326, 0.0077360570430755615]],\n",
       " [128, [0.0013849695678800344, 0.993034839630127, 0.005580219905823469]],\n",
       " [129, [0.0034937108866870403, 0.0014479276724159718, 0.9950584173202515]],\n",
       " [130, [0.015417276881635189, 0.9451854825019836, 0.03939728066325188]],\n",
       " [131, [0.009430951438844204, 0.0009470919612795115, 0.9896219968795776]],\n",
       " [132, [0.006598257459700108, 0.9672789573669434, 0.026122773066163063]],\n",
       " [133, [0.5560767650604248, 0.01980115845799446, 0.4241220951080322]],\n",
       " [134, [0.005765797104686499, 0.002057556062936783, 0.9921767115592957]],\n",
       " [135, [0.9828169345855713, 0.00446558790281415, 0.012717418372631073]],\n",
       " [136, [0.7586397528648376, 0.12937325239181519, 0.11198700964450836]],\n",
       " [137, [0.0012666808906942606, 0.0012573215644806623, 0.9974759221076965]],\n",
       " [138, [0.0006150664994493127, 0.9644511342048645, 0.034933801740407944]],\n",
       " [139, [0.00040312434430234134, 0.9951647520065308, 0.004432086832821369]],\n",
       " [140, [0.0032449173741042614, 0.0030070138163864613, 0.9937480688095093]],\n",
       " [141, [0.0015656910836696625, 0.0077548255212605, 0.9906794428825378]],\n",
       " [142, [0.0014293320709839463, 0.005193738732486963, 0.9933769106864929]],\n",
       " [143, [0.9112680554389954, 0.03309432044625282, 0.05563763901591301]],\n",
       " [144, [0.01279221847653389, 0.006986179389059544, 0.9802215695381165]],\n",
       " [145, [0.011229750700294971, 0.0033885177690535784, 0.9853817224502563]],\n",
       " [146, [0.987733006477356, 0.00402795197442174, 0.008239012211561203]],\n",
       " [147, [0.0006174417212605476, 0.9943845272064209, 0.00499807670712471]],\n",
       " [148, [0.0013003540225327015, 0.0020005826372653246, 0.9966990351676941]],\n",
       " [149, [0.0014248723164200783, 0.0015227129915729165, 0.9970524311065674]],\n",
       " [150, [0.0007271302165463567, 0.9953789710998535, 0.003893939545378089]],\n",
       " [151, [0.9782112836837769, 0.005411518272012472, 0.01637726090848446]],\n",
       " [152, [0.0038875392638146877, 0.7835559844970703, 0.21255651116371155]],\n",
       " [153, [0.008979898877441883, 0.2815956771373749, 0.7094243764877319]],\n",
       " [154, [0.9860538840293884, 0.008491123095154762, 0.005454989615827799]],\n",
       " [155, [0.9944128394126892, 0.0020945039577782154, 0.0034926552325487137]],\n",
       " [156, [0.9948256015777588, 0.0012220731005072594, 0.003952337428927422]],\n",
       " [157, [0.061181940138339996, 0.5918264985084534, 0.3469915986061096]],\n",
       " [158, [0.022239554673433304, 0.008155683986842632, 0.9696047306060791]],\n",
       " [159, [0.0017252331599593163, 0.8640493154525757, 0.1342255026102066]],\n",
       " [160, [0.04489021748304367, 0.004515967797487974, 0.9505937695503235]],\n",
       " [161, [0.9858860969543457, 0.0025726640596985817, 0.011541304178535938]],\n",
       " [162, [0.0009133580606430769, 0.002133284928277135, 0.9969533681869507]],\n",
       " [163, [0.9510936737060547, 0.017140505835413933, 0.03176578879356384]],\n",
       " [164, [0.003750365460291505, 0.011482500471174717, 0.9847671389579773]],\n",
       " [165, [0.004729514475911856, 0.32041096687316895, 0.6748594641685486]],\n",
       " [166, [0.012173990719020367, 0.0011611004592850804, 0.986664891242981]],\n",
       " [167, [0.9928677082061768, 0.001649370533414185, 0.005482978653162718]],\n",
       " [168, [0.001548544387333095, 0.0014813435263931751, 0.9969700574874878]],\n",
       " [169, [0.9953113198280334, 0.0017402236117050052, 0.002948485082015395]],\n",
       " [170, [0.009595395997166634, 0.09471427649259567, 0.8956902623176575]],\n",
       " [171, [0.0015628847759217024, 0.0021155301947146654, 0.9963215589523315]],\n",
       " [172, [0.2830813527107239, 0.23643015325069427, 0.4804885685443878]],\n",
       " [173, [0.0018612504936754704, 0.002510883379727602, 0.9956278800964355]],\n",
       " [174, [0.003113378770649433, 0.3987427055835724, 0.598143994808197]],\n",
       " [175, [0.008650921285152435, 0.4686129689216614, 0.5227360725402832]],\n",
       " [176, [0.006263766437768936, 0.008596373721957207, 0.9851399064064026]],\n",
       " [177, [0.0015830716583877802, 0.0028156396001577377, 0.9956012964248657]],\n",
       " [178, [0.0004219428519718349, 0.998002827167511, 0.0015752814942970872]],\n",
       " [179, [0.003147311042994261, 0.04136315733194351, 0.9554895758628845]],\n",
       " [180, [0.0005626166239380836, 0.9957156777381897, 0.0037217375356703997]],\n",
       " [181, [0.0011943662539124489, 0.0013342555612325668, 0.9974714517593384]],\n",
       " [182, [0.004021938890218735, 0.04365244507789612, 0.9523256421089172]],\n",
       " [183, [0.0068952892906963825, 0.008875846862792969, 0.9842288494110107]],\n",
       " [184, [0.00243365322239697, 0.001415325328707695, 0.9961510896682739]],\n",
       " [185, [0.0018420537235215306, 0.0038608675822615623, 0.9942970871925354]],\n",
       " [186, [0.0005023559788241982, 0.991267740726471, 0.008229873143136501]],\n",
       " [187, [0.0019278023391962051, 0.0019529856508597732, 0.9961191415786743]],\n",
       " [188, [0.9819080233573914, 0.0020649482030421495, 0.01602712832391262]],\n",
       " [189, [0.0019338486017659307, 0.9167909622192383, 0.08127520978450775]],\n",
       " [190, [0.3271859288215637, 0.0521584078669548, 0.6206556558609009]],\n",
       " [191, [0.9554645419120789, 0.0014929311582818627, 0.043042611330747604]],\n",
       " [192, [0.0004396311705932021, 0.9935756325721741, 0.005984796676784754]],\n",
       " [193, [0.005179861560463905, 0.0010199898388236761, 0.9938001036643982]],\n",
       " [194, [0.001367264660075307, 0.0019023512722924352, 0.9967304468154907]],\n",
       " [195, [0.9728748798370361, 0.005555292125791311, 0.021569818258285522]],\n",
       " [196, [0.4685260057449341, 0.008108771406114101, 0.5233652591705322]],\n",
       " [197, [0.008022958412766457, 0.017676297575235367, 0.9743006825447083]],\n",
       " [198, [0.9645541906356812, 0.005939725786447525, 0.029506081715226173]],\n",
       " [199, [0.00539913447573781, 0.0022958340123295784, 0.9923050999641418]],\n",
       " [200, [0.004598827566951513, 0.0035736579447984695, 0.9918274879455566]],\n",
       " [201, [0.002723532263189554, 0.0014462950639426708, 0.995830237865448]],\n",
       " [202, [0.9677138328552246, 0.0024176491424441338, 0.029868578538298607]],\n",
       " [203, [0.0003360939444974065, 0.9965921640396118, 0.00307180592790246]],\n",
       " [204, [0.0003240477526560426, 0.997713565826416, 0.0019623618572950363]],\n",
       " [205, [0.9936050772666931, 0.00169095688033849, 0.004704010672867298]],\n",
       " [206, [0.9742386937141418, 0.003290745196864009, 0.022470535710453987]],\n",
       " [207, [0.0013002913910895586, 0.003575806738808751, 0.9951239228248596]],\n",
       " [208, [0.9848633408546448, 0.00782174151390791, 0.00731487013399601]],\n",
       " [209, [0.0009781840490177274, 0.0041957758367061615, 0.9948260188102722]],\n",
       " [210, [0.006503613665699959, 0.9869802594184875, 0.006516091525554657]],\n",
       " [211, [0.0007777411956340075, 0.9958003163337708, 0.0034218821674585342]],\n",
       " [212, [0.00034191671875305474, 0.9956315755844116, 0.004026493523269892]],\n",
       " [213, [0.0025503928773105145, 0.0009822673164308071, 0.9964673519134521]],\n",
       " [214, [0.0005677790031768382, 0.8930966854095459, 0.10633549839258194]],\n",
       " [215, [0.0005438402877189219, 0.9820886254310608, 0.017367573454976082]],\n",
       " [216, [0.001683119684457779, 0.7211160063743591, 0.277200847864151]],\n",
       " [217, [0.9835952520370483, 0.0046470919623970985, 0.0117576252669096]],\n",
       " [218, [0.001040646806359291, 0.992865800857544, 0.006093592848628759]],\n",
       " [219, [0.0030649856198579073, 0.990536630153656, 0.00639843475073576]],\n",
       " [220, [0.004628095775842667, 0.00336633063852787, 0.9920056462287903]],\n",
       " [221, [0.01317836344242096, 0.009142419323325157, 0.9776792526245117]],\n",
       " [222, [0.9847679734230042, 0.0017062954138964415, 0.013525781221687794]],\n",
       " [223, [0.0003342103445902467, 0.9970255494117737, 0.002640258753672242]],\n",
       " [224, [0.0018476250115782022, 0.0022327895276248455, 0.9959194660186768]],\n",
       " [225, [0.006315014790743589, 0.7461691498756409, 0.24751588702201843]],\n",
       " [226, [0.0005553180817514658, 0.9933998584747314, 0.006044883280992508]],\n",
       " [227, [0.0015145068755373359, 0.001234153751283884, 0.9972513318061829]],\n",
       " [228, [0.004161756485700607, 0.8959208130836487, 0.09991737455129623]],\n",
       " [229, [0.006969602312892675, 0.41556912660598755, 0.577461302280426]],\n",
       " [230, [0.9835472702980042, 0.002862080931663513, 0.013590657152235508]],\n",
       " [231, [0.0012330770259723067, 0.0014302852796390653, 0.9973366856575012]],\n",
       " [232, [0.7052870988845825, 0.05725686624646187, 0.23745602369308472]],\n",
       " [233, [0.954289436340332, 0.00690153893083334, 0.038808997720479965]],\n",
       " [234, [0.9855011105537415, 0.0010409853421151638, 0.013457952067255974]],\n",
       " [235, [0.0017306898953393102, 0.0024265970569103956, 0.9958426356315613]],\n",
       " [236, [0.0027921011205762625, 0.0010994801996275783, 0.9961084723472595]],\n",
       " [237, [0.0025706072337925434, 0.060857851058244705, 0.9365715384483337]],\n",
       " [238, [0.9942545294761658, 0.0015249712159857154, 0.004220579285174608]],\n",
       " [239, [0.9896398782730103, 0.0019306022441014647, 0.008429552428424358]],\n",
       " [240, [0.9944539666175842, 0.0013975444016978145, 0.00414845859631896]],\n",
       " [241, [0.0005945232696831226, 0.9964404702186584, 0.0029650088399648666]],\n",
       " [242, [0.0009596996242180467, 0.005755891557782888, 0.9932844638824463]],\n",
       " [243, [0.0018311013700440526, 0.008611448109149933, 0.9895575642585754]],\n",
       " [244, [0.001598954200744629, 0.001207021763548255, 0.9971939325332642]],\n",
       " [245, [0.775421679019928, 0.02557692490518093, 0.1990014612674713]],\n",
       " [246, [0.002198438160121441, 0.0010004538344219327, 0.9968011379241943]],\n",
       " [247, [0.0015027370536699891, 0.0028562312945723534, 0.9956409931182861]],\n",
       " [248, [0.0025739483535289764, 0.29820987582206726, 0.6992161870002747]],\n",
       " [249, [0.008659224957227707, 0.9756286144256592, 0.015712132677435875]],\n",
       " [250, [0.9061163663864136, 0.0011209758231416345, 0.09276257455348969]],\n",
       " [251, [0.0025337946135550737, 0.835837721824646, 0.16162842512130737]],\n",
       " [252, [0.051655370742082596, 0.0007898123003542423, 0.9475548267364502]],\n",
       " [253, [0.964656412601471, 0.0012858527479693294, 0.03405769541859627]],\n",
       " [254, [0.0028971366118639708, 0.37421706318855286, 0.6228857636451721]],\n",
       " [255, [0.0034441929310560226, 0.00238371011801064, 0.9941721558570862]],\n",
       " [256, [0.0038217806722968817, 0.004190470091998577, 0.9919878244400024]],\n",
       " [257, [0.016652008518576622, 0.8137941360473633, 0.16955387592315674]],\n",
       " [258, [0.99134361743927, 0.0055994195863604546, 0.0030570654198527336]],\n",
       " [259, [0.9912126660346985, 0.0008762210491113365, 0.007911130785942078]],\n",
       " [260, [0.49779385328292847, 0.26053327322006226, 0.24167294800281525]],\n",
       " [261, [0.9881008863449097, 0.002432030625641346, 0.009467105381190777]],\n",
       " [262, [0.0011237255530431867, 0.012839267961680889, 0.9860369563102722]],\n",
       " [263, [0.00137371348682791, 0.002550887642428279, 0.9960753321647644]],\n",
       " [264, [0.0025395131669938564, 0.0027217704337090254, 0.9947386384010315]],\n",
       " [265, [0.0021452833898365498, 0.0021950516384094954, 0.9956596493721008]],\n",
       " [266, [0.4653310179710388, 0.45981889963150024, 0.07485005259513855]],\n",
       " [267, [0.9898279905319214, 0.002518421271815896, 0.007653568871319294]],\n",
       " [268, [0.005173958837985992, 0.05591990053653717, 0.9389062523841858]],\n",
       " [269, [0.9952497482299805, 0.0013585188426077366, 0.0033917739056050777]],\n",
       " [270, [0.011767378076910973, 0.29285338521003723, 0.6953792572021484]],\n",
       " [271, [0.0037335497327148914, 0.0022347194608300924, 0.9940317273139954]],\n",
       " [272, [0.9154284000396729, 0.031012743711471558, 0.053558822721242905]],\n",
       " [273, [0.9413726329803467, 0.004502966068685055, 0.05412445589900017]],\n",
       " [274, [0.021402660757303238, 0.00530090881511569, 0.9732965230941772]],\n",
       " [275, [0.0005593777750618756, 0.9938676953315735, 0.005572950001806021]],\n",
       " [276, [0.0070757027715444565, 0.9839586019515991, 0.008965669199824333]],\n",
       " [277, [0.001233976799994707, 0.9888073205947876, 0.009958671405911446]],\n",
       " [278, [0.0028898639138787985, 0.0022488601971417665, 0.9948613047599792]],\n",
       " [279, [0.0010639699175953865, 0.011320555582642555, 0.9876154661178589]],\n",
       " [280, [0.00242670439183712, 0.013963615521788597, 0.983609676361084]],\n",
       " [281, [0.8732977509498596, 0.007412577513605356, 0.11928968876600266]],\n",
       " [282, [0.0426795557141304, 0.07528389990329742, 0.8820365071296692]],\n",
       " [283, [0.9935963153839111, 0.0027485541068017483, 0.0036551039665937424]],\n",
       " [284, [0.0024252922739833593, 0.8923341035842896, 0.10524065792560577]],\n",
       " [285, [0.9910679459571838, 0.0025299862027168274, 0.006402129773050547]],\n",
       " [286, [0.005544173531234264, 0.05677777901291847, 0.9376780986785889]],\n",
       " [287, [0.009652728214859962, 0.942059338092804, 0.04828791320323944]],\n",
       " [288, [0.9931668639183044, 0.0024186265654861927, 0.00441450159996748]],\n",
       " [289, [0.00526037672534585, 0.02856641449034214, 0.9661732316017151]],\n",
       " [290, [0.983460009098053, 0.003244285238906741, 0.013295697048306465]],\n",
       " [291, [0.00041754526318982244, 0.9971182346343994, 0.002464239951223135]],\n",
       " [292, [0.9914274215698242, 0.004502539988607168, 0.004070055205374956]],\n",
       " [293, [0.0033612900879234076, 0.0040323189459741116, 0.9926062822341919]],\n",
       " [294, [0.9778456091880798, 0.003195095108821988, 0.018959226086735725]],\n",
       " [295, [0.000387349515222013, 0.9974982142448425, 0.0021144018974155188]],\n",
       " [296, [0.0005867868894711137, 0.996960461139679, 0.002452721120789647]],\n",
       " [297, [0.018737463280558586, 0.9746994972229004, 0.0065630157478153706]],\n",
       " [298, [0.0010148874716833234, 0.0018229199340566993, 0.9971621632575989]],\n",
       " [299, [0.2365124225616455, 0.009736290201544762, 0.7537512183189392]],\n",
       " [300, [0.1822827011346817, 0.0016255100490525365, 0.816091775894165]],\n",
       " [301, [0.993055522441864, 0.0016964890528470278, 0.005247972439974546]],\n",
       " [302, [0.0007185094873420894, 0.006181090138852596, 0.9931004047393799]],\n",
       " [303, [0.8643292188644409, 0.11696868389844894, 0.018702086061239243]],\n",
       " [304, [0.5076087117195129, 0.4201453924179077, 0.07224585860967636]],\n",
       " [305, [0.9851623177528381, 0.0012540003517642617, 0.013583659194409847]],\n",
       " [306, [0.9112208485603333, 0.06786152720451355, 0.020917721092700958]],\n",
       " [307, [0.0005131775978952646, 0.9966270923614502, 0.0028598180506378412]],\n",
       " [308, [0.8738153576850891, 0.01080718170851469, 0.11537744849920273]],\n",
       " [309, [0.9894599914550781, 0.0016191847389563918, 0.008920860476791859]],\n",
       " [310, [0.015990590676665306, 0.0008212247048504651, 0.9831882119178772]],\n",
       " [311, [0.00023531333135906607, 0.9974576830863953, 0.002307033399119973]],\n",
       " [312, [0.059016454964876175, 0.006801865994930267, 0.9341816902160645]],\n",
       " [313, [0.0022440829779952765, 0.002056882018223405, 0.9956990480422974]],\n",
       " [314, [0.0017834585160017014, 0.0019427217775955796, 0.9962737560272217]],\n",
       " [315, [0.0019242579583078623, 0.00204133871011436, 0.9960344433784485]],\n",
       " [316, [0.09325999021530151, 0.01303365733474493, 0.8937063813209534]],\n",
       " [317, [0.0011794035090133548, 0.002097525168210268, 0.9967231154441833]],\n",
       " [318, [0.9926544427871704, 0.0019339454593136907, 0.005411659367382526]],\n",
       " [319, [0.9439922571182251, 0.010589799843728542, 0.04541797563433647]],\n",
       " [320, [0.9162390232086182, 0.0012920700246468186, 0.08246885985136032]],\n",
       " [321, [0.0013439639005810022, 0.0014841904630884528, 0.9971718192100525]],\n",
       " [322, [0.0017253465484827757, 0.0013747414341196418, 0.9968999624252319]],\n",
       " [323, [0.09170552343130112, 0.007773885037750006, 0.9005206227302551]],\n",
       " [324, [0.021718325093388557, 0.5926397442817688, 0.3856419622898102]],\n",
       " [325, [0.5469985008239746, 0.016176272183656693, 0.4368251860141754]],\n",
       " [326, [0.0006379809346981347, 0.9968444108963013, 0.0025176904164254665]],\n",
       " [327, [0.08137734234333038, 0.011081160977482796, 0.9075414538383484]],\n",
       " [328, [0.0024134921841323376, 0.9667734503746033, 0.0308130644261837]],\n",
       " [329, [0.0003976425505243242, 0.9962181448936462, 0.0033841754775494337]],\n",
       " [330, [0.0015443186275660992, 0.0013166883727535605, 0.9971389770507812]],\n",
       " [331, [0.9830151796340942, 0.008170461282134056, 0.008814376778900623]],\n",
       " [332, [0.9705604314804077, 0.001626855111680925, 0.027812736108899117]],\n",
       " [333, [0.021956635639071465, 0.962125837802887, 0.015917440876364708]],\n",
       " [334, [0.0006182243814691901, 0.9919988512992859, 0.007382916286587715]],\n",
       " [335, [0.0009788498282432556, 0.995735764503479, 0.003285359125584364]],\n",
       " [336, [0.7601510286331177, 0.0019758676644414663, 0.23787307739257812]],\n",
       " [337, [0.00036241530324332416, 0.9972018003463745, 0.0024357496295124292]],\n",
       " [338, [0.002685540821403265, 0.9832627773284912, 0.014051724225282669]],\n",
       " [339, [0.06845388561487198, 0.006122262217104435, 0.9254238605499268]],\n",
       " [340, [0.0011374655878171325, 0.9835345149040222, 0.01532800868153572]],\n",
       " [341, [0.001372958766296506, 0.002387038432061672, 0.9962399005889893]],\n",
       " [342, [0.0024973612744361162, 0.6466533541679382, 0.3508492708206177]],\n",
       " [343, [0.0006561845075339079, 0.9976193308830261, 0.0017244260525330901]],\n",
       " [344, [0.0009675773326307535, 0.006628852803260088, 0.992403507232666]],\n",
       " [345, [0.9923586249351501, 0.0014501899713650346, 0.006191101390868425]],\n",
       " [346, [0.0009807118913158774, 0.997112512588501, 0.001906752004288137]],\n",
       " [347, [0.0007427725940942764, 0.9899915456771851, 0.009265737608075142]],\n",
       " [348, [0.0012636409373953938, 0.9902993440628052, 0.008436952717602253]],\n",
       " [349, [0.0015521395253017545, 0.0017899840604513884, 0.9966578483581543]],\n",
       " [350, [0.013742693699896336, 0.9646811485290527, 0.02157612144947052]],\n",
       " [351, [0.0402214452624321, 0.0013212355552241206, 0.9584572911262512]],\n",
       " [352, [0.0688975602388382, 0.9252980947494507, 0.005804393440485001]],\n",
       " [353, [0.01046267244964838, 0.2737140953540802, 0.715823233127594]],\n",
       " [354, [0.0009782328270375729, 0.007583316881209612, 0.9914383888244629]],\n",
       " [355, [0.15316133201122284, 0.19673508405685425, 0.6501036286354065]],\n",
       " [356, [0.0010688420152291656, 0.012026932090520859, 0.9869042038917542]],\n",
       " [357, [0.9803667664527893, 0.0051357257179915905, 0.01449759490787983]],\n",
       " [358, [0.0004630575713235885, 0.9966328740119934, 0.0029040600638836622]],\n",
       " [359, [0.0011725904187187552, 0.0015835233498364687, 0.9972438812255859]],\n",
       " [360, [0.009778386913239956, 0.9772327542304993, 0.012988844886422157]],\n",
       " [361, [0.0004907458205707371, 0.9975941777229309, 0.0019150965381413698]],\n",
       " [362, [0.014094117097556591, 0.5804106593132019, 0.40549522638320923]],\n",
       " [363, [0.0019735628738999367, 0.002939193742349744, 0.9950873255729675]],\n",
       " [364, [0.000432067085057497, 0.9958814382553101, 0.0036865537986159325]],\n",
       " [365, [0.0065530696883797646, 0.022579548880457878, 0.9708672761917114]],\n",
       " [366, [0.004941853228956461, 0.05178945139050484, 0.9432686567306519]],\n",
       " [367, [0.9898768067359924, 0.001300451229326427, 0.008822725154459476]],\n",
       " [368, [0.0014918734086677432, 0.9964175224304199, 0.0020906489808112383]],\n",
       " [369, [0.005935331340879202, 0.005276574287563562, 0.9887880682945251]],\n",
       " [370, [0.0003014783433172852, 0.9961147308349609, 0.003583792131394148]],\n",
       " [371, [0.004995249677449465, 0.12757660448551178, 0.867428183555603]],\n",
       " [372, [0.9846840500831604, 0.009771300479769707, 0.005544552579522133]],\n",
       " [373, [0.017658866941928864, 0.0006321691325865686, 0.9817090034484863]],\n",
       " [374, [0.0026015457697212696, 0.0027265511453151703, 0.9946720004081726]],\n",
       " [375, [0.9820582270622253, 0.0023023816756904125, 0.015639346092939377]],\n",
       " [376, [0.9937604069709778, 0.002090537454932928, 0.004149057902395725]],\n",
       " [377, [0.0032884671818464994, 0.005652497988194227, 0.9910590052604675]],\n",
       " [378, [0.8454143404960632, 0.017328744754195213, 0.137256920337677]],\n",
       " [379, [0.020052213221788406, 0.010723015293478966, 0.9692248106002808]],\n",
       " [380, [0.9904449582099915, 0.0017097601667046547, 0.007845329120755196]],\n",
       " [381, [0.0018261534860357642, 0.002495145658031106, 0.995678722858429]],\n",
       " [382, [0.0006444086320698261, 0.9972233772277832, 0.0021321941167116165]],\n",
       " [383, [0.0008872854523360729, 0.003150149015709758, 0.9959626793861389]],\n",
       " [384, [0.9861401319503784, 0.0027149594388902187, 0.011144918389618397]],\n",
       " [385, [0.00858811940997839, 0.05490085482597351, 0.9365110993385315]],\n",
       " [386, [0.984311044216156, 0.0013240538537502289, 0.014364845119416714]],\n",
       " [387, [0.6699008345603943, 0.008632315322756767, 0.321466863155365]],\n",
       " [388, [0.0019708946347236633, 0.00395051296800375, 0.994078516960144]],\n",
       " [389, [0.000556988175958395, 0.9919677376747131, 0.007475306745618582]],\n",
       " [390, [0.0020132518839091063, 0.0016753542004153132, 0.996311366558075]],\n",
       " [391, [0.07304054498672485, 0.0584358349442482, 0.8685235977172852]],\n",
       " [392, [0.00048776870244182646, 0.9973382353782654, 0.002174018183723092]],\n",
       " [393, [0.0011992924846708775, 0.20673403143882751, 0.7920666933059692]],\n",
       " [394, [0.0010663459543138742, 0.9954174757003784, 0.003516132477670908]],\n",
       " [395, [0.0007050657877698541, 0.9272075295448303, 0.07208743691444397]],\n",
       " [396, [0.4254326820373535, 0.03101659193634987, 0.5435506701469421]],\n",
       " [397, [0.30851301550865173, 0.006370356306433678, 0.6851165890693665]],\n",
       " [398, [0.0005766734248027205, 0.9859159588813782, 0.013507306575775146]],\n",
       " [399, [0.015297994017601013, 0.002163846278563142, 0.9825381636619568]],\n",
       " [400, [0.001583737088367343, 0.9906224608421326, 0.0077938297763466835]],\n",
       " [401, [0.014595041051506996, 0.0026141875423491, 0.9827908277511597]],\n",
       " [402, [0.0008870337042026222, 0.9906875491142273, 0.008425449952483177]],\n",
       " [403, [0.00038572028279304504, 0.9955549836158752, 0.004059290513396263]],\n",
       " [404, [0.9798363447189331, 0.002007021103054285, 0.01815658062696457]],\n",
       " [405, [0.0015677318442612886, 0.0013018128229305148, 0.9971304535865784]],\n",
       " [406, [0.005555989686399698, 0.011980582028627396, 0.9824634790420532]],\n",
       " [407, [0.001073776395060122, 0.001841146731749177, 0.9970850348472595]],\n",
       " [408, [0.000583132088650018, 0.9844739437103271, 0.014942909590899944]],\n",
       " [409, [0.004293104633688927, 0.001215329160913825, 0.9944915771484375]],\n",
       " [410, [0.0009943555342033505, 0.9865478873252869, 0.0124577721580863]],\n",
       " [411, [0.0005161942099221051, 0.9967336654663086, 0.0027501489967107773]],\n",
       " [412, [0.0008525103912688792, 0.9973898530006409, 0.0017576271202415228]],\n",
       " [413, [0.0003673892642837018, 0.9972234964370728, 0.0024091508239507675]],\n",
       " [414, [0.002333695301786065, 0.030406828969717026, 0.9672594666481018]],\n",
       " [415, [0.0030514595564454794, 0.992169201374054, 0.00477931834757328]],\n",
       " [416, [0.0005645401543006301, 0.9974552989006042, 0.001980177126824856]],\n",
       " [417, [0.9688320159912109, 0.01883460395038128, 0.012333367019891739]],\n",
       " [418, [0.20222769677639008, 0.029976975172758102, 0.7677952647209167]],\n",
       " [419, [0.0006704836268909276, 0.9882733821868896, 0.011056151241064072]],\n",
       " [420, [0.012130452319979668, 0.0037771076895296574, 0.9840924739837646]],\n",
       " [421, [0.048277534544467926, 0.008862926624715328, 0.9428596496582031]],\n",
       " [422, [0.5315120816230774, 0.003961073234677315, 0.46452683210372925]],\n",
       " [423, [0.0030293387826532125, 0.0009017735137604177, 0.9960689544677734]],\n",
       " [424, [0.9926689863204956, 0.0008831190643832088, 0.006447802763432264]],\n",
       " [425, [0.024438081309199333, 0.002462472766637802, 0.9730993509292603]],\n",
       " [426, [0.04231791943311691, 0.013889405876398087, 0.9437925815582275]],\n",
       " [427, [0.0006184010999277234, 0.9971179962158203, 0.0022636649664491415]],\n",
       " [428, [0.0003543421335052699, 0.9951789379119873, 0.004466703161597252]],\n",
       " [429, [0.0010667385067790747, 0.9582833051681519, 0.040649961680173874]],\n",
       " [430, [0.9928606152534485, 0.0010475537274032831, 0.006091809831559658]],\n",
       " [431, [0.0008567650802433491, 0.022695090621709824, 0.9764482378959656]],\n",
       " [432, [0.003071905579417944, 0.7621937990188599, 0.23473432660102844]],\n",
       " [433, [0.002896434161812067, 0.18624593317508698, 0.8108575940132141]],\n",
       " [434, [0.9822649955749512, 0.008278923109173775, 0.009455962106585503]],\n",
       " [435, [0.0010788425570353866, 0.002064540283754468, 0.9968565702438354]],\n",
       " [436, [0.9817019701004028, 0.0021180608309805393, 0.016179926693439484]],\n",
       " [437, [0.9900087118148804, 0.0010289852507412434, 0.008962246589362621]],\n",
       " [438, [0.8984066247940063, 0.018086818978190422, 0.08350662142038345]],\n",
       " [439, [0.9938116073608398, 0.00245081540197134, 0.00373758003115654]],\n",
       " [440, [0.9916732907295227, 0.0024674199521541595, 0.00585925905033946]],\n",
       " [441, [0.087840236723423, 0.7621551156044006, 0.15000462532043457]],\n",
       " [442, [0.8691438436508179, 0.009348709136247635, 0.12150736153125763]],\n",
       " [443, [0.0016452675918117166, 0.00806030910462141, 0.9902944564819336]],\n",
       " [444, [0.0003861432778649032, 0.9977218508720398, 0.0018919343128800392]],\n",
       " [445, [0.001106296549551189, 0.0015345782740041614, 0.9973590970039368]],\n",
       " [446, [0.0013516225153580308, 0.0018009589985013008, 0.9968474507331848]],\n",
       " [447, [0.4590364098548889, 0.017842372879385948, 0.5231212377548218]],\n",
       " [448, [0.005698425695300102, 0.004208707250654697, 0.990092933177948]],\n",
       " [449, [0.0009004608727991581, 0.7801666855812073, 0.2189328670501709]],\n",
       " [450, [0.0014976896345615387, 0.9887354969978333, 0.009766815230250359]],\n",
       " [451, [0.9913431406021118, 0.003087435383349657, 0.005569512955844402]],\n",
       " [452, [0.9046357870101929, 0.0016564549878239632, 0.09370773285627365]],\n",
       " [453, [0.011001938953995705, 0.846472442150116, 0.14252562820911407]],\n",
       " [454, [0.0004862859786953777, 0.9859479665756226, 0.013565734028816223]],\n",
       " [455, [0.9948680400848389, 0.0013836262514814734, 0.0037482476327568293]],\n",
       " [456, [0.004066874738782644, 0.007392205763608217, 0.9885409474372864]],\n",
       " [457, [0.0029883587267249823, 0.9866632223129272, 0.010348333977162838]],\n",
       " [458, [0.013003512285649776, 0.004980564583092928, 0.9820159673690796]],\n",
       " [459, [0.054413359612226486, 0.76226407289505, 0.18332260847091675]],\n",
       " [460, [0.37386369705200195, 0.5835142731666565, 0.04262203350663185]],\n",
       " [461, [0.9939796924591064, 0.0018567051738500595, 0.0041635846719145775]],\n",
       " [462, [0.0006212047301232815, 0.9975047707557678, 0.00187403685413301]],\n",
       " [463, [0.0573643334209919, 0.18657518923282623, 0.7560604810714722]],\n",
       " [464, [0.06284301728010178, 0.7611961364746094, 0.17596085369586945]],\n",
       " [465, [0.000993602559901774, 0.9632893800735474, 0.03571702539920807]],\n",
       " [466, [0.0006889020442031324, 0.9971528053283691, 0.002158310730010271]],\n",
       " [467, [0.0018307651625946164, 0.0017408886924386024, 0.9964283108711243]],\n",
       " [468, [0.9869678616523743, 0.002122280653566122, 0.010909821838140488]],\n",
       " [469, [0.006426930893212557, 0.009191419929265976, 0.9843816161155701]],\n",
       " [470, [0.0015815041260793805, 0.9904453754425049, 0.007973098196089268]],\n",
       " [471, [0.0011366744292899966, 0.21887366473674774, 0.7799896001815796]],\n",
       " [472, [0.9938656687736511, 0.0016197431832551956, 0.004514661617577076]],\n",
       " [473, [0.10530351102352142, 0.02788189798593521, 0.8668146133422852]],\n",
       " [474, [0.99097740650177, 0.0011060618562623858, 0.00791653897613287]],\n",
       " [475, [0.00407060794532299, 0.0013063608203083277, 0.994623064994812]],\n",
       " [476, [0.989177405834198, 0.001439930172637105, 0.009382658638060093]],\n",
       " [477, [0.0019221467664465308, 0.9770058989524841, 0.02107197232544422]],\n",
       " [478, [0.8342128396034241, 0.09637492895126343, 0.0694122463464737]],\n",
       " [479, [0.0005604065372608602, 0.9963738322257996, 0.003065710887312889]],\n",
       " [480, [0.8778592348098755, 0.0022973010782152414, 0.11984347552061081]],\n",
       " [481, [0.9848663210868835, 0.002570530166849494, 0.012563117779791355]],\n",
       " [482, [0.021713852882385254, 0.9291064143180847, 0.049179721623659134]],\n",
       " [483, [0.9611721634864807, 0.0034572044387459755, 0.03537054359912872]],\n",
       " [484, [0.16667667031288147, 0.005734751932322979, 0.8275885581970215]],\n",
       " [485, [0.6214739084243774, 0.19330474734306335, 0.1852213740348816]],\n",
       " [486, [0.9901537299156189, 0.0030233191791921854, 0.006822960451245308]],\n",
       " [487, [0.0020133941434323788, 0.002947469474747777, 0.995039165019989]],\n",
       " [488, [0.0061910818330943584, 0.979832649230957, 0.013976234942674637]],\n",
       " [489, [0.9828991889953613, 0.0012759295059368014, 0.015825005248188972]],\n",
       " [490, [0.8367259502410889, 0.002482765121385455, 0.16079126298427582]],\n",
       " [491, [0.0017700737807899714, 0.0018949767109006643, 0.9963349103927612]],\n",
       " [492, [0.001593402586877346, 0.0018595242872834206, 0.9965470433235168]],\n",
       " [493, [0.11445052176713943, 0.03535442426800728, 0.8501951098442078]],\n",
       " [494, [0.9793174266815186, 0.0020387584809213877, 0.01864369958639145]],\n",
       " [495, [0.938602864742279, 0.01046814490109682, 0.050929080694913864]],\n",
       " [496, [0.9940037131309509, 0.0011959885014221072, 0.004800368100404739]],\n",
       " [497, [0.003342453623190522, 0.001188424532301724, 0.9954690933227539]],\n",
       " [498, [0.0030617762822657824, 0.0020945603027939796, 0.994843602180481]],\n",
       " [499, [0.9590378403663635, 0.012733455747365952, 0.028228698298335075]],\n",
       " [500, [0.991357684135437, 0.001121125533245504, 0.007521114777773619]],\n",
       " [501, [0.7046170234680176, 0.2789146602153778, 0.016468336805701256]],\n",
       " [502, [0.0012766054132953286, 0.0048863510601222515, 0.9938370585441589]],\n",
       " [503, [0.0010420616017654538, 0.9958848357200623, 0.003073102794587612]],\n",
       " [504, [0.002412838162854314, 0.9409062266349792, 0.05668092519044876]],\n",
       " [505, [0.9814419150352478, 0.0020712383557111025, 0.016486914828419685]],\n",
       " [506, [0.061377499252557755, 0.013870512135326862, 0.9247520565986633]],\n",
       " [507, [0.3477371037006378, 0.6310791373252869, 0.021183809265494347]],\n",
       " [508, [0.8825777769088745, 0.0048398361541330814, 0.11258241534233093]],\n",
       " [509, [0.0021065317559987307, 0.9710268974304199, 0.02686663158237934]],\n",
       " [510, [0.00884406641125679, 0.08007640391588211, 0.9110795259475708]],\n",
       " [511, [0.947965681552887, 0.007582646794617176, 0.044451721012592316]],\n",
       " [512, [0.7966738939285278, 0.024915987625718117, 0.1784101277589798]],\n",
       " [513, [0.9838802218437195, 0.0027713594026863575, 0.013348382897675037]],\n",
       " [514, [0.7788792252540588, 0.1771945059299469, 0.043926190584897995]],\n",
       " [515, [0.0004221709677949548, 0.9957166314125061, 0.0038611728232353926]],\n",
       " [516, [0.3717111647129059, 0.004571157973259687, 0.6237176656723022]],\n",
       " [517, [0.9945545196533203, 0.0013100272044539452, 0.004135471303015947]],\n",
       " [518, [0.00046348758041858673, 0.9918869137763977, 0.007649547420442104]],\n",
       " [519, [0.001362282200716436, 0.9730821847915649, 0.025555528700351715]],\n",
       " [520, [0.0019850130192935467, 0.006779650691896677, 0.9912352561950684]],\n",
       " [521, [0.010521382093429565, 0.45329105854034424, 0.5361875891685486]],\n",
       " [522, [0.23537683486938477, 0.74863201379776, 0.015991145744919777]],\n",
       " [523, [0.0025618223007768393, 0.011564500629901886, 0.9858736991882324]],\n",
       " [524, [0.0014710656832903624, 0.0018836106173694134, 0.9966452717781067]],\n",
       " [525, [0.000891270930878818, 0.9839252233505249, 0.015183458104729652]],\n",
       " [526, [0.9690579175949097, 0.022196821868419647, 0.008745294995605946]],\n",
       " [527, [0.9936493039131165, 0.0013064952800050378, 0.005044201388955116]],\n",
       " [528, [0.09875288605690002, 0.8385890126228333, 0.06265808641910553]],\n",
       " [529, [0.9594337940216064, 0.012462408281862736, 0.028103796765208244]],\n",
       " [530, [0.06277571618556976, 0.06263470649719238, 0.8745896220207214]],\n",
       " [531, [0.03059501387178898, 0.003215608187019825, 0.9661893844604492]],\n",
       " [532, [0.003035890869796276, 0.9899225831031799, 0.007041516713798046]],\n",
       " [533, [0.04126434028148651, 0.0015559325693175197, 0.9571797251701355]],\n",
       " [534, [0.0014491728506982327, 0.002343082567676902, 0.9962077140808105]],\n",
       " [535, [0.0050056856125593185, 0.959709107875824, 0.03528526797890663]],\n",
       " [536, [0.001803410123102367, 0.992345929145813, 0.005850676912814379]],\n",
       " [537, [0.0011234208941459656, 0.8879892230033875, 0.11088735610246658]],\n",
       " [538, [0.013709234073758125, 0.0019379083532840014, 0.9843528866767883]],\n",
       " [539, [0.0011509611504152417, 0.0036410775501281023, 0.9952079653739929]],\n",
       " [540, [0.015294475480914116, 0.001712186960503459, 0.9829933643341064]],\n",
       " [541, [0.00047256419202312827, 0.9973715543746948, 0.002155810361728072]],\n",
       " [542, [0.0006342307897284627, 0.989296019077301, 0.010069702751934528]],\n",
       " [543, [0.9898446798324585, 0.0015990028623491526, 0.00855635292828083]],\n",
       " [544, [0.0012109798844903708, 0.9957679510116577, 0.003021101700142026]],\n",
       " [545, [0.0013190064346417785, 0.002355487085878849, 0.9963255524635315]],\n",
       " [546, [0.00041822256753221154, 0.995040237903595, 0.004541558213531971]],\n",
       " [547, [0.9906174540519714, 0.003206967143341899, 0.00617555808275938]],\n",
       " [548, [0.0012587166856974363, 0.0017177877016365528, 0.9970235228538513]],\n",
       " [549, [0.8665771484375, 0.001899892115034163, 0.1315230429172516]],\n",
       " [550, [0.001809664536267519, 0.0010951561853289604, 0.9970951080322266]],\n",
       " [551, [0.0007321060984395444, 0.994716227054596, 0.004551715217530727]],\n",
       " [552, [0.6346980929374695, 0.057611435651779175, 0.30769041180610657]],\n",
       " [553, [0.9904781579971313, 0.0032182789873331785, 0.00630362331867218]],\n",
       " [554, [0.988281786441803, 0.0017203940078616142, 0.009997806511819363]],\n",
       " [555, [0.0025181930977851152, 0.01983434334397316, 0.9776474237442017]],\n",
       " [556, [0.93648362159729, 0.016298914328217506, 0.04721760004758835]],\n",
       " [557, [0.007921574637293816, 0.06971168518066406, 0.922366738319397]],\n",
       " [558, [0.002073469338938594, 0.0012978625018149614, 0.9966287016868591]],\n",
       " [559, [0.0018505752086639404, 0.6133575439453125, 0.38479191064834595]],\n",
       " [560, [0.8818526268005371, 0.08523145318031311, 0.032915856689214706]],\n",
       " [561, [0.000879535567946732, 0.9959281086921692, 0.003192382864654064]],\n",
       " [562, [0.0012554236454889178, 0.004044385626912117, 0.9947001934051514]],\n",
       " [563, [0.0013976660557091236, 0.0027091619558632374, 0.995893120765686]],\n",
       " [564, [0.20176862180233002, 0.021299412474036217, 0.7769319415092468]],\n",
       " [565, [0.0007712138467468321, 0.002710327971726656, 0.9965184926986694]],\n",
       " [566, [0.9574177265167236, 0.02261136658489704, 0.019970884546637535]],\n",
       " [567, [0.9939314126968384, 0.001550352550111711, 0.004518209490925074]],\n",
       " [568, [0.004339552018791437, 0.001220386242493987, 0.9944400787353516]],\n",
       " [569, [0.08657363802194595, 0.8583964705467224, 0.05502994358539581]],\n",
       " [570, [0.8391387462615967, 0.033234331756830215, 0.1276269555091858]],\n",
       " [571, [0.9826898574829102, 0.01219935156404972, 0.005110808182507753]],\n",
       " [572, [0.8715038299560547, 0.10833515971899033, 0.020160986110568047]],\n",
       " [573, [0.002403955440968275, 0.034503210335969925, 0.9630928635597229]],\n",
       " [574, [0.9725151062011719, 0.00986550934612751, 0.017619354650378227]],\n",
       " [575, [0.0013350144727155566, 0.9952889680862427, 0.0033760250080376863]],\n",
       " [576, [0.9744080901145935, 0.00285930628888309, 0.022732600569725037]],\n",
       " [577, [0.003944918513298035, 0.0018933399114757776, 0.9941616654396057]],\n",
       " [578, [0.0005049818428233266, 0.9964078068733215, 0.003087186487391591]],\n",
       " [579, [0.002753131091594696, 0.591140627861023, 0.40610620379447937]],\n",
       " [580, [0.0031302915886044502, 0.9838348627090454, 0.0130348801612854]],\n",
       " [581, [0.9827432632446289, 0.005170566961169243, 0.012086178176105022]],\n",
       " [582, [0.9827525615692139, 0.0010246132733300328, 0.016222849488258362]],\n",
       " [583, [0.0010468229884281754, 0.010232781991362572, 0.9887203574180603]],\n",
       " [584, [0.9936317801475525, 0.0017474580090492964, 0.004620743915438652]],\n",
       " [585, [0.9942527413368225, 0.0034456944558769464, 0.0023014748003333807]],\n",
       " [586, [0.012859776616096497, 0.006581059657037258, 0.9805592894554138]],\n",
       " [587, [0.9932796359062195, 0.0013101915828883648, 0.0054101720452308655]],\n",
       " [588, [0.02782917581498623, 0.8077741265296936, 0.1643967181444168]],\n",
       " [589, [0.9666446447372437, 0.003530561923980713, 0.029824838042259216]],\n",
       " [590, [0.9928057789802551, 0.0018757518846541643, 0.005318506620824337]],\n",
       " [591, [0.00888682808727026, 0.022378971800208092, 0.9687342643737793]],\n",
       " [592, [0.0023534109350293875, 0.9927109479904175, 0.004935596138238907]],\n",
       " [593, [0.990814745426178, 0.0015223555965349078, 0.007662923540920019]],\n",
       " [594, [0.000478259171359241, 0.9976717829704285, 0.0018498965073376894]],\n",
       " [595, [0.00035912200110033154, 0.9979265928268433, 0.0017142411088570952]],\n",
       " [596, [0.03069409169256687, 0.6027249693870544, 0.3665809631347656]],\n",
       " [597, [0.0018388174939900637, 0.0016235982766374946, 0.9965375661849976]],\n",
       " [598, [0.9802086353302002, 0.0027642673812806606, 0.017027031630277634]],\n",
       " [599, [0.9938574433326721, 0.0010305590694770217, 0.005112010985612869]],\n",
       " [600, [0.028287535533308983, 0.9634060859680176, 0.008306385949254036]],\n",
       " [601, [0.0017000819789245725, 0.7454012632369995, 0.25289860367774963]],\n",
       " [602, [0.0005923210410401225, 0.8972105383872986, 0.10219715535640717]],\n",
       " [603, [0.9884195327758789, 0.003041680436581373, 0.00853885617107153]],\n",
       " [604, [0.006708162371069193, 0.13364408910274506, 0.8596476912498474]],\n",
       " [605, [0.005015855189412832, 0.0009606789099052548, 0.9940235018730164]],\n",
       " [606, [0.00031555790337733924, 0.9978625178337097, 0.0018219337798655033]],\n",
       " [607, [0.00045932503417134285, 0.99785977602005, 0.0016809352673590183]],\n",
       " [608, [0.0039410097524523735, 0.9869197607040405, 0.009139235131442547]],\n",
       " [609, [0.00048783651436679065, 0.9967988729476929, 0.002713270951062441]],\n",
       " [610, [0.2559930682182312, 0.6481063961982727, 0.09590054303407669]],\n",
       " [611, [0.5553200840950012, 0.03632102534174919, 0.4083588719367981]],\n",
       " [612, [0.010968268848955631, 0.005300818011164665, 0.9837309122085571]],\n",
       " [613, [0.9890172481536865, 0.0009895091643556952, 0.00999322161078453]],\n",
       " [614, [0.002723034704104066, 0.9454135298728943, 0.05186348035931587]],\n",
       " [615, [0.0008546791505068541, 0.9963992834091187, 0.0027459836564958096]],\n",
       " [616, [0.00508229760453105, 0.0184085201472044, 0.976509153842926]],\n",
       " [617, [0.001805196749046445, 0.0028239923994988203, 0.9953706860542297]],\n",
       " [618, [0.00046701563405804336, 0.9978976249694824, 0.0016353691462427378]],\n",
       " [619, [0.0011738616740331054, 0.0014828270068392158, 0.9973433613777161]],\n",
       " [620, [0.002013983204960823, 0.9943017959594727, 0.003684277879074216]],\n",
       " [621, [0.0025956963654607534, 0.03399905189871788, 0.9634053111076355]],\n",
       " [622, [0.9892367720603943, 0.0016740974970161915, 0.009089132770895958]],\n",
       " [623, [0.00741034559905529, 0.25291669368743896, 0.7396730184555054]],\n",
       " [624, [0.9362444281578064, 0.03358589857816696, 0.030169596895575523]],\n",
       " [625, [0.0005925907753407955, 0.9793381690979004, 0.020069235935807228]],\n",
       " [626, [0.0011652588145807385, 0.9691599011421204, 0.029674870893359184]],\n",
       " [627, [0.00046361368731595576, 0.9973803162574768, 0.002155998721718788]],\n",
       " [628, [0.005243942141532898, 0.004103835206478834, 0.990652322769165]],\n",
       " [629, [0.9836739301681519, 0.002331174211576581, 0.013994858600199223]],\n",
       " [630, [0.003349142149090767, 0.990312933921814, 0.006337965372949839]],\n",
       " [631, [0.992291271686554, 0.0029510874301195145, 0.004757613409310579]],\n",
       " [632, [0.002986575709655881, 0.009690210223197937, 0.9873232245445251]],\n",
       " [633, [0.9778373837471008, 0.01100565493106842, 0.011156932450830936]],\n",
       " [634, [0.004994145128875971, 0.00679375696927309, 0.9882120490074158]],\n",
       " [635, [0.06487160921096802, 0.7691552042961121, 0.1659732311964035]],\n",
       " [636, [0.0024275186005979776, 0.9944562315940857, 0.003116264473646879]],\n",
       " [637, [0.58253413438797, 0.3807407319545746, 0.03672516718506813]],\n",
       " [638, [0.020834019407629967, 0.0012841668212786317, 0.9778818488121033]],\n",
       " [639, [0.036326002329587936, 0.8713868856430054, 0.09228706359863281]],\n",
       " [640, [0.0013876655139029026, 0.649684727191925, 0.34892764687538147]],\n",
       " [641, [0.9775960445404053, 0.007500994484871626, 0.014902904629707336]],\n",
       " [642, [0.003825453808531165, 0.007758447900414467, 0.9884161949157715]],\n",
       " [643, [0.9922864437103271, 0.001389971817843616, 0.006323514971882105]],\n",
       " [644, [0.0012899527791887522, 0.0012113212142139673, 0.9974986910820007]],\n",
       " [645, [0.9857897162437439, 0.0024842768907546997, 0.011725983582437038]],\n",
       " [646, [0.001640156377106905, 0.01541594322770834, 0.9829439520835876]],\n",
       " [647, [0.00076697749318555, 0.9967111349105835, 0.002521864138543606]],\n",
       " [648, [0.000380584824597463, 0.9945766925811768, 0.005042740609496832]],\n",
       " [649, [0.004993921145796776, 0.0016178220976144075, 0.9933881759643555]],\n",
       " [650, [0.9938687682151794, 0.001247270149178803, 0.004884056281298399]],\n",
       " [651, [0.25107458233833313, 0.5492696166038513, 0.19965583086013794]],\n",
       " [652, [0.0005897348746657372, 0.9969944953918457, 0.0024157289881259203]],\n",
       " [653, [0.005361888557672501, 0.0008402716484852135, 0.9937978386878967]],\n",
       " [654, [0.0019989467691630125, 0.00817165058106184, 0.9898293614387512]],\n",
       " [655, [0.002520275767892599, 0.0010533200111240149, 0.9964264035224915]],\n",
       " [656, [0.016348497942090034, 0.01554526761174202, 0.9681062698364258]],\n",
       " [657, [0.00047045029350556433, 0.9929878115653992, 0.006541761104017496]],\n",
       " [658, [0.9843055009841919, 0.0017029904993250966, 0.013991469517350197]],\n",
       " [659, [0.0006990300607867539, 0.9973940849304199, 0.0019069066038355231]],\n",
       " [660, [0.002003635047003627, 0.00280159804970026, 0.9951947331428528]],\n",
       " [661, [0.019807707518339157, 0.938474178314209, 0.041718099266290665]],\n",
       " [662, [0.00213486491702497, 0.004265503026545048, 0.9935996532440186]],\n",
       " [663, [0.00123665074352175, 0.0027942671440541744, 0.9959690570831299]],\n",
       " [664, [0.13056841492652893, 0.0033510769717395306, 0.8660805225372314]],\n",
       " [665, [0.5694870352745056, 0.3782505393028259, 0.052262477576732635]],\n",
       " [666, [0.020119942724704742, 0.4979870617389679, 0.48189306259155273]],\n",
       " [667, [0.04184327274560928, 0.005552520509809256, 0.9526042342185974]],\n",
       " [668, [0.0007743064197711647, 0.0072060502134263515, 0.9920195937156677]],\n",
       " [669, [0.00088907202007249, 0.00598065136000514, 0.9931302666664124]],\n",
       " [670, [0.0007771209347993135, 0.0030916763935238123, 0.996131181716919]],\n",
       " [671, [0.002255583880469203, 0.03609166666865349, 0.9616526961326599]],\n",
       " [672, [0.0022237550001591444, 0.2533053755760193, 0.7444708943367004]],\n",
       " [673, [0.0003063411277253181, 0.9974812865257263, 0.002212404040619731]],\n",
       " [674, [0.001919998088851571, 0.0017030577873811126, 0.9963769316673279]],\n",
       " [675, [0.9751502275466919, 0.0016103602247312665, 0.023239364847540855]],\n",
       " [676, [0.2841463088989258, 0.022498898208141327, 0.6933547854423523]],\n",
       " [677, [0.0018014274537563324, 0.0016299944836646318, 0.9965686798095703]],\n",
       " [678, [0.0005392921739257872, 0.9968409538269043, 0.00261978548951447]],\n",
       " [679, [0.0016792246606200933, 0.9195467829704285, 0.07877391576766968]],\n",
       " [680, [0.001381355687044561, 0.008539598435163498, 0.9900790452957153]],\n",
       " [681, [0.0007707145414315164, 0.9956658482551575, 0.003563521895557642]],\n",
       " [682, [0.9919140338897705, 0.0022847638465464115, 0.005801181308925152]],\n",
       " [683, [0.0004379836318548769, 0.9978525638580322, 0.001709484145976603]],\n",
       " [684, [0.0032289735972881317, 0.9150482416152954, 0.08172276616096497]],\n",
       " [685, [0.0009377628448419273, 0.9943559765815735, 0.004706182982772589]],\n",
       " [686, [0.8559163212776184, 0.00597345270216465, 0.1381102204322815]],\n",
       " [687, [0.0029511109460145235, 0.0016759746940806508, 0.9953729510307312]],\n",
       " [688, [0.8154059648513794, 0.0809517651796341, 0.1036422923207283]],\n",
       " [689, [0.9924691319465637, 0.002370961243286729, 0.005159907042980194]],\n",
       " [690, [0.004540609661489725, 0.003196173347532749, 0.9922633171081543]],\n",
       " [691, [0.991595447063446, 0.0014924592105671763, 0.006912048440426588]],\n",
       " [692, [0.9535179734230042, 0.04183967784047127, 0.004642277956008911]],\n",
       " [693, [0.00622500479221344, 0.04276924952864647, 0.951005756855011]],\n",
       " [694, [0.9914407730102539, 0.002874471480026841, 0.005684784613549709]],\n",
       " [695, [0.11568986624479294, 0.20400741696357727, 0.6803027391433716]],\n",
       " [696, [0.9739017486572266, 0.00188354158308357, 0.024214738979935646]],\n",
       " [697, [0.049370624125003815, 0.8228346705436707, 0.12779471278190613]],\n",
       " [698, [0.813381016254425, 0.0015665129758417606, 0.18505249917507172]],\n",
       " [699, [0.38567742705345154, 0.019990352913737297, 0.594332218170166]],\n",
       " [700, [0.03127297759056091, 0.07397764176130295, 0.8947493433952332]],\n",
       " [701, [0.9773848652839661, 0.004164690151810646, 0.018450485542416573]],\n",
       " [702, [0.9928807020187378, 0.0023185964673757553, 0.004800730850547552]],\n",
       " [703, [0.009016004391014576, 0.004035222809761763, 0.9869487285614014]],\n",
       " [704, [0.5702130198478699, 0.31552377343177795, 0.11426319181919098]],\n",
       " [705, [0.9837862849235535, 0.0030134734697639942, 0.013200236484408379]],\n",
       " [706, [0.0007933276356197894, 0.9905127882957458, 0.008693897165358067]],\n",
       " [707, [0.0011116520036011934, 0.9914106130599976, 0.00747776310890913]],\n",
       " [708, [0.0009275195188820362, 0.0021193698048591614, 0.9969531297683716]],\n",
       " [709, [0.001808040658943355, 0.9323632121086121, 0.06582878530025482]],\n",
       " [710, [0.004272800404578447, 0.0018586094956845045, 0.9938686490058899]],\n",
       " [711, [0.9854490160942078, 0.0027977372519671917, 0.011753194965422153]],\n",
       " [712, [0.0012073421385139227, 0.0022207002621144056, 0.9965718984603882]],\n",
       " [713, [0.989984393119812, 0.0058950441889464855, 0.00412049749866128]],\n",
       " [714, [0.993497371673584, 0.00349439843557775, 0.0030082601588219404]],\n",
       " [715, [0.09444521367549896, 0.8971589207649231, 0.008395901881158352]],\n",
       " [716, [0.40487349033355713, 0.3002385199069977, 0.2948879897594452]],\n",
       " [717, [0.0008855349151417613, 0.0024703498929739, 0.996644139289856]],\n",
       " [718, [0.00897794496268034, 0.0019721081480383873, 0.989050030708313]],\n",
       " [719, [0.9806640148162842, 0.002264177892357111, 0.01707177795469761]],\n",
       " [720, [0.9474464058876038, 0.0030077151022851467, 0.049545854330062866]],\n",
       " [721, [0.0018909816863015294, 0.0016353888204321265, 0.9964736104011536]],\n",
       " [722, [0.017993347719311714, 0.22466188669204712, 0.7573447823524475]],\n",
       " [723, [0.017833657562732697, 0.7974374294281006, 0.18472892045974731]],\n",
       " [724, [0.7429532408714294, 0.0022683527786284685, 0.25477832555770874]],\n",
       " [725, [0.0003452466335147619, 0.996932864189148, 0.0027219026815146208]],\n",
       " [726, [0.003034701570868492, 0.18188144266605377, 0.8150838017463684]],\n",
       " [727, [0.017641954123973846, 0.9411829113960266, 0.04117514193058014]],\n",
       " [728, [0.002360530896112323, 0.9951000809669495, 0.002539368811994791]],\n",
       " [729, [0.9528326392173767, 0.005550865549594164, 0.04161654785275459]],\n",
       " [730, [0.9685400724411011, 0.02521723136305809, 0.006242615170776844]],\n",
       " [731, [0.002035732613876462, 0.008096862584352493, 0.9898673295974731]],\n",
       " [732, [0.006928448099642992, 0.8707535266876221, 0.12231795489788055]],\n",
       " [733, [0.001842772588133812, 0.0012537415605038404, 0.9969034790992737]],\n",
       " [734, [0.9370583295822144, 0.00322655844502151, 0.05971512570977211]],\n",
       " [735, [0.001666286145336926, 0.0025038705207407475, 0.9958298802375793]],\n",
       " [736, [0.010923892259597778, 0.9779002070426941, 0.011175896972417831]],\n",
       " [737, [0.018152626231312752, 0.18621517717838287, 0.7956321835517883]],\n",
       " [738, [0.5276697278022766, 0.0040781861171126366, 0.4682520627975464]],\n",
       " [739, [0.9637589454650879, 0.007466299459338188, 0.02877480909228325]],\n",
       " [740, [0.4633513391017914, 0.00919587817043066, 0.527452826499939]],\n",
       " [741, [0.041833989322185516, 0.8939862251281738, 0.06417982280254364]],\n",
       " [742, [0.18454307317733765, 0.011351107619702816, 0.804105818271637]],\n",
       " [743, [0.9629909992218018, 0.003925141878426075, 0.03308381885290146]],\n",
       " [744, [0.008426773361861706, 0.40955108404159546, 0.5820221304893494]],\n",
       " [745, [0.02414470538496971, 0.002331826835870743, 0.9735234379768372]],\n",
       " [746, [0.0013517072657123208, 0.0016703776782378554, 0.9969779253005981]],\n",
       " [747, [0.5775676965713501, 0.4108507037162781, 0.01158154010772705]],\n",
       " [748, [0.004147888161242008, 0.00194827641826123, 0.9939038157463074]],\n",
       " [749, [0.0025771588552743196, 0.9823997020721436, 0.015023083426058292]],\n",
       " [750, [0.001503216801211238, 0.0021218364126980305, 0.9963749051094055]],\n",
       " [751, [0.005185804329812527, 0.13471628725528717, 0.8600978255271912]],\n",
       " [752, [0.0020203592721372843, 0.0031482998747378588, 0.994831383228302]],\n",
       " [753, [0.015355844050645828, 0.004696141462773085, 0.9799480438232422]],\n",
       " [754, [0.8645497560501099, 0.11941337585449219, 0.016036804765462875]],\n",
       " [755, [0.0010946833062916994, 0.0024000981356948614, 0.9965052604675293]],\n",
       " [756, [0.05396866798400879, 0.0021068688947707415, 0.9439244270324707]],\n",
       " [757, [0.0005507229943759739, 0.9961560368537903, 0.003293223213404417]],\n",
       " [758, [0.0016075836028903723, 0.002726659644395113, 0.9956658482551575]],\n",
       " [759, [0.0011343745281919837, 0.006051274947822094, 0.9928143620491028]],\n",
       " [760, [0.9940432906150818, 0.0014981288695707917, 0.0044585647992789745]],\n",
       " [761, [0.008159973658621311, 0.0019270407501608133, 0.9899129867553711]],\n",
       " [762, [0.9884238839149475, 0.003232290968298912, 0.008343827910721302]],\n",
       " [763, [0.0010392736876383424, 0.9963188171386719, 0.0026418608613312244]],\n",
       " [764, [0.0007453718571923673, 0.0041692317463457584, 0.9950852990150452]],\n",
       " [765, [0.9772174954414368, 0.0073359664529562, 0.015446504577994347]],\n",
       " [766, [0.004028327763080597, 0.0007749604992568493, 0.9951967597007751]],\n",
       " [767, [0.9945517182350159, 0.0016807004576548934, 0.0037676412612199783]],\n",
       " [768, [0.0005767850088886917, 0.9972635507583618, 0.002159625291824341]],\n",
       " [769, [0.012692597694694996, 0.0339350700378418, 0.9533722400665283]],\n",
       " [770, [0.0010903547517955303, 0.9885718822479248, 0.01033773459494114]],\n",
       " [771, [0.0010040070628747344, 0.00292784976772964, 0.9960681200027466]],\n",
       " [772, [0.08693671226501465, 0.00470747472718358, 0.9083558320999146]],\n",
       " [773, [0.5759932398796082, 0.40619927644729614, 0.017807457596063614]],\n",
       " [774, [0.010322562418878078, 0.004508012905716896, 0.985169529914856]],\n",
       " [775, [0.0032896813936531544, 0.002608331385999918, 0.9941019415855408]],\n",
       " [776, [0.0006722678081132472, 0.9902315735816956, 0.009096117690205574]],\n",
       " [777, [0.7866325378417969, 0.00372781022451818, 0.20963968336582184]],\n",
       " [778, [0.001179243205115199, 0.0034351085778325796, 0.9953857064247131]],\n",
       " [779, [0.0009213178418576717, 0.002070995280519128, 0.9970076680183411]],\n",
       " [780, [0.9760987162590027, 0.0020455163903534412, 0.02185579016804695]],\n",
       " [781, [0.5594795346260071, 0.2131609469652176, 0.22735954821109772]],\n",
       " [782, [0.0003855426621157676, 0.9945202469825745, 0.0050942255184054375]],\n",
       " [783, [0.008219394832849503, 0.01508763525635004, 0.9766928553581238]],\n",
       " [784, [0.6313148140907288, 0.27024203538894653, 0.09844321757555008]],\n",
       " [785, [0.8855563402175903, 0.009233972057700157, 0.10520964115858078]],\n",
       " [786, [0.0005368322599679232, 0.9939174056053162, 0.0055458019487559795]],\n",
       " [787, [0.0007845691288821399, 0.815154492855072, 0.18406100571155548]],\n",
       " [788, [0.9937858581542969, 0.0025378309655934572, 0.003676334163174033]],\n",
       " [789, [0.001126697869040072, 0.8274472951889038, 0.17142599821090698]],\n",
       " [790, [0.9847151041030884, 0.0013460710179060698, 0.01393877249211073]],\n",
       " [791, [0.03367386385798454, 0.017897574231028557, 0.9484285712242126]],\n",
       " [792, [0.9570046663284302, 0.027482379227876663, 0.015513007529079914]],\n",
       " [793, [0.0015006560133770108, 0.002369322581216693, 0.9961299896240234]],\n",
       " [794, [0.0046423571184277534, 0.004544124472886324, 0.9908135533332825]],\n",
       " [795, [0.8749207258224487, 0.0021127453073859215, 0.1229664757847786]],\n",
       " [796, [0.3277190327644348, 0.5255703330039978, 0.146710604429245]],\n",
       " [797, [0.994175374507904, 0.0027253692969679832, 0.0030992813408374786]],\n",
       " [798, [0.9941515326499939, 0.0025680908001959324, 0.0032803574576973915]],\n",
       " [799, [0.0018559865420684218, 0.001209493144415319, 0.9969345331192017]],\n",
       " [800, [0.8514016270637512, 0.0751529410481453, 0.07344543188810349]],\n",
       " [801, [0.0012474815594032407, 0.005367286503314972, 0.9933852553367615]],\n",
       " [802, [0.07148697972297668, 0.0029103723354637623, 0.9256027340888977]],\n",
       " [803, [0.0014585652388632298, 0.001907331752590835, 0.9966340661048889]],\n",
       " [804, [0.0005830335430800915, 0.9962122440338135, 0.0032047824934124947]],\n",
       " [805, [0.0025103893131017685, 0.9496541023254395, 0.04783550649881363]],\n",
       " [806, [0.9667474627494812, 0.011927256360650063, 0.02132527157664299]],\n",
       " [807, [0.42896464467048645, 0.0059418827295303345, 0.565093457698822]],\n",
       " [808, [0.0027582477778196335, 0.08157087117433548, 0.9156708121299744]],\n",
       " [809, [0.0978795513510704, 0.7968471646308899, 0.1052732765674591]],\n",
       " [810, [0.12157716602087021, 0.0225558802485466, 0.8558669686317444]],\n",
       " [811, [0.0002504185540601611, 0.9960583448410034, 0.0036912530194967985]],\n",
       " [812, [0.0014709571842104197, 0.9338736534118652, 0.06465543061494827]],\n",
       " [813, [0.006621232721954584, 0.19289575517177582, 0.800482988357544]],\n",
       " [814, [0.0013103734236210585, 0.007310133893042803, 0.9913794994354248]],\n",
       " [815, [0.0009301727986894548, 0.0029512715991586447, 0.9961185455322266]],\n",
       " [816, [0.7057215571403503, 0.008966874331235886, 0.28531160950660706]],\n",
       " [817, [0.01068829745054245, 0.0032978663221001625, 0.9860137701034546]],\n",
       " [818, [0.0014647042844444513, 0.001485269283875823, 0.9970500469207764]],\n",
       " [819, [0.9935978651046753, 0.001952372258529067, 0.004449756816029549]],\n",
       " [820, [0.0006853343220427632, 0.016760047525167465, 0.9825546145439148]],\n",
       " [821, [0.0038019733037799597, 0.02560323104262352, 0.9705947637557983]],\n",
       " [822, [0.020045224577188492, 0.5097367167472839, 0.4702180325984955]],\n",
       " [823, [0.0011771558783948421, 0.0014688973315060139, 0.9973539113998413]],\n",
       " [824, [0.7450082302093506, 0.001426382688805461, 0.25356537103652954]],\n",
       " [825, [0.006054805126041174, 0.04421747848391533, 0.9497277140617371]],\n",
       " [826, [0.00044562460971064866, 0.9972736239433289, 0.002280776621773839]],\n",
       " [827, [0.0016812386456876993, 0.0014855029294267297, 0.9968332648277283]],\n",
       " [828, [0.00567984813824296, 0.9609352350234985, 0.033384982496500015]],\n",
       " [829, [0.9312431216239929, 0.015253032557666302, 0.053503889590501785]],\n",
       " [830, [0.32777875661849976, 0.009561235085129738, 0.6626599431037903]],\n",
       " [831, [0.0005066608428023756, 0.9900869727134705, 0.00940630491822958]],\n",
       " [832, [0.00033559888834133744, 0.9924023151397705, 0.007262075319886208]],\n",
       " [833, [0.6780815720558167, 0.07956647872924805, 0.2423519343137741]],\n",
       " [834, [0.0019853790290653706, 0.0025635012425482273, 0.9954511523246765]],\n",
       " [835, [0.018145350739359856, 0.8701901435852051, 0.11166444420814514]],\n",
       " [836, [0.9912612438201904, 0.002409393899142742, 0.006329271476715803]],\n",
       " [837, [0.0004973645554855466, 0.9976410865783691, 0.0018616003217175603]],\n",
       " [838, [0.000989025691524148, 0.0026144750881940126, 0.9963964819908142]],\n",
       " [839, [0.0007516019977629185, 0.9954067468643188, 0.0038417247124016285]],\n",
       " [840, [0.06704103201627731, 0.5280123949050903, 0.40494659543037415]],\n",
       " [841, [0.2079966813325882, 0.002042867708951235, 0.7899604439735413]],\n",
       " [842, [0.0007716018008068204, 0.9950202703475952, 0.0042080990970134735]],\n",
       " [843, [0.9928083419799805, 0.0032217090483754873, 0.003970000427216291]],\n",
       " [844, [0.0015144519275054336, 0.002215206390246749, 0.9962702989578247]],\n",
       " [845, [0.0015761118847876787, 0.0018072649836540222, 0.9966166615486145]],\n",
       " [846, [0.0217006616294384, 0.9324023723602295, 0.045896902680397034]],\n",
       " [847, [0.00043044902849942446, 0.9750822186470032, 0.024487346410751343]],\n",
       " [848, [0.9907581806182861, 0.0029684160836040974, 0.006273367907851934]],\n",
       " [849, [0.0009245434775948524, 0.02036016806960106, 0.9787153005599976]],\n",
       " [850, [0.0008834325126372278, 0.0029945464339107275, 0.9961219429969788]],\n",
       " [851, [0.0007672657375223935, 0.9910756349563599, 0.008157055824995041]],\n",
       " [852, [0.9773197174072266, 0.0035186896566301584, 0.01916150189936161]],\n",
       " [853, [0.005278291180729866, 0.0016999050276353955, 0.9930217862129211]],\n",
       " [854, [0.0006862560985609889, 0.9958441853523254, 0.003469508606940508]],\n",
       " [855, [0.9913040399551392, 0.0017592861549928784, 0.006936734542250633]],\n",
       " [856, [0.9846667051315308, 0.0024476395919919014, 0.012885681353509426]],\n",
       " [857, [0.0027575320564210415, 0.0013091900618746877, 0.9959333539009094]],\n",
       " [858, [0.0003809609042946249, 0.9972295165061951, 0.0023895646445453167]],\n",
       " [859, [0.3226848542690277, 0.30425143241882324, 0.37306371331214905]],\n",
       " [860, [0.002585562178865075, 0.9895455241203308, 0.007868913933634758]],\n",
       " [861, [0.01535159070044756, 0.0013803228503093123, 0.9832680821418762]],\n",
       " [862, [0.0006849687779322267, 0.9205346703529358, 0.07878038287162781]],\n",
       " [863, [0.007004799321293831, 0.020331261679530144, 0.972663938999176]],\n",
       " [864, [0.016128089278936386, 0.0060175382532179356, 0.9778543710708618]],\n",
       " [865, [0.9814759492874146, 0.008877492509782314, 0.009646601974964142]],\n",
       " [866, [0.9786561727523804, 0.00292736547999084, 0.018416492268443108]],\n",
       " [867, [0.005549915600568056, 0.004297069739550352, 0.9901530146598816]],\n",
       " [868, [0.0006570067489519715, 0.991709291934967, 0.007633713074028492]],\n",
       " [869, [0.005864514969289303, 0.014630313962697983, 0.979505181312561]],\n",
       " [870, [0.03372102975845337, 0.7501921653747559, 0.21608684957027435]],\n",
       " [871, [0.003351468127220869, 0.018462426960468292, 0.9781860709190369]],\n",
       " [872, [0.9411805868148804, 0.009623033925890923, 0.04919632151722908]],\n",
       " [873, [0.09008437395095825, 0.07851247489452362, 0.8314031958580017]],\n",
       " [874, [0.004427276086062193, 0.016280144453048706, 0.9792925119400024]],\n",
       " [875, [0.017589226365089417, 0.4761432111263275, 0.5062675476074219]],\n",
       " [876, [0.9945255517959595, 0.0016943871742114425, 0.0037800525315105915]],\n",
       " [877, [0.0003659306967165321, 0.9962165951728821, 0.003417415078729391]],\n",
       " [878, [0.0016226684674620628, 0.0038440793287009, 0.9945331811904907]],\n",
       " [879, [0.0021408130414783955, 0.0009399194386787713, 0.9969192743301392]],\n",
       " [880, [0.0014253153931349516, 0.002774597145617008, 0.9958000779151917]],\n",
       " [881, [0.004415514413267374, 0.07054142653942108, 0.9250431060791016]],\n",
       " [882, [0.0696030929684639, 0.9130777716636658, 0.017319103702902794]],\n",
       " [883, [0.9935677647590637, 0.001581961871124804, 0.00485029025003314]],\n",
       " [884, [0.9955018162727356, 0.0017668624641373754, 0.002731385175138712]],\n",
       " [885, [0.012841749005019665, 0.40837374329566956, 0.5787844657897949]],\n",
       " [886, [0.0011782454093918204, 0.0020335272420197725, 0.9967882633209229]],\n",
       " [887, [0.0031065798830240965, 0.010021574795246124, 0.9868718385696411]],\n",
       " [888, [0.9937790036201477, 0.002372933551669121, 0.0038481212686747313]],\n",
       " [889, [0.0017269175732508302, 0.005566117819398642, 0.9927069544792175]],\n",
       " [890, [0.9763143658638, 0.005205472465604544, 0.018480224534869194]],\n",
       " [891, [0.030458679422736168, 0.8359172940254211, 0.13362406194210052]],\n",
       " [892, [0.9898334741592407, 0.0033781477250158787, 0.006788400933146477]],\n",
       " [893, [0.977712869644165, 0.0026944305282086134, 0.0195926520973444]],\n",
       " [894, [0.00048701680498197675, 0.9969596862792969, 0.002553292317315936]],\n",
       " [895, [0.0038312687538564205, 0.0013643949059769511, 0.994804322719574]],\n",
       " [896, [0.9070024490356445, 0.03270407021045685, 0.060293443500995636]],\n",
       " [897, [0.0011420869268476963, 0.00246065529063344, 0.9963973164558411]],\n",
       " [898, [0.9566605091094971, 0.004975771065801382, 0.03836368769407272]],\n",
       " [899, [0.0092538483440876, 0.0013869120739400387, 0.9893592596054077]],\n",
       " [900, [0.0030131840612739325, 0.05789420008659363, 0.9390925765037537]],\n",
       " [901, [0.001493956195190549, 0.9098881483078003, 0.08861791342496872]],\n",
       " [902, [0.9793862700462341, 0.0035744155757129192, 0.01703941635787487]],\n",
       " [903, [0.8908053636550903, 0.0018119515152648091, 0.10738274455070496]],\n",
       " [904, [0.9774332642555237, 0.0036309121642261744, 0.018935739994049072]],\n",
       " [905, [0.0022927874233573675, 0.0020957901142537594, 0.9956114888191223]],\n",
       " [906, [0.0007344505866058171, 0.9957897067070007, 0.0034758856054395437]],\n",
       " [907, [0.0004365206987131387, 0.9929226636886597, 0.006640779320150614]],\n",
       " [908, [0.5201945900917053, 0.24847842752933502, 0.23132699728012085]],\n",
       " [909, [0.39613795280456543, 0.0062877107411623, 0.5975743532180786]],\n",
       " [910, [0.003617836395278573, 0.0014740563929080963, 0.9949080348014832]],\n",
       " [911, [0.017028246074914932, 0.015811465680599213, 0.9671602249145508]],\n",
       " [912, [0.0019189409213140607, 0.0016798898577690125, 0.9964011907577515]],\n",
       " [913, [0.9889456629753113, 0.00158313091378659, 0.009471219964325428]],\n",
       " [914, [0.0010557582136243582, 0.0019557473715394735, 0.9969884753227234]],\n",
       " [915, [0.003807641798630357, 0.0009358846582472324, 0.9952564835548401]],\n",
       " [916, [0.001004438498057425, 0.9964666366577148, 0.0025289044715464115]],\n",
       " [917, [0.0011205719783902168, 0.9949532151222229, 0.003926174249500036]],\n",
       " [918, [0.0009380921837873757, 0.0021810184698551893, 0.9968808889389038]],\n",
       " [919, [0.9738643169403076, 0.020994437858462334, 0.005141343455761671]],\n",
       " [920, [0.007348829880356789, 0.009052824229001999, 0.9835984110832214]],\n",
       " [921, [0.985516369342804, 0.002402229467406869, 0.012081380002200603]],\n",
       " [922, [0.8103237152099609, 0.00481818150728941, 0.18485811352729797]],\n",
       " [923, [0.8459388613700867, 0.0065072267316281796, 0.1475539356470108]],\n",
       " [924, [0.0018677626503631473, 0.01458996906876564, 0.9835423231124878]],\n",
       " [925, [0.0014313225401565433, 0.9860019683837891, 0.012566668912768364]],\n",
       " [926, [0.8841210007667542, 0.014678538776934147, 0.10120044648647308]],\n",
       " [927, [0.9906291365623474, 0.0015886814799159765, 0.00778212072327733]],\n",
       " [928, [0.087006576359272, 0.0059018367901444435, 0.9070916175842285]],\n",
       " [929, [0.9411109089851379, 0.019226958975195885, 0.03966207802295685]],\n",
       " [930, [0.0007312704692594707, 0.008787221275269985, 0.9904815554618835]],\n",
       " [931, [0.9602830410003662, 0.018434925004839897, 0.02128208614885807]],\n",
       " [932, [0.009314688853919506, 0.0019178861984983087, 0.9887674450874329]],\n",
       " [933, [0.001442484324797988, 0.0013676444068551064, 0.9971898198127747]],\n",
       " [934, [0.0004318932769820094, 0.9975416660308838, 0.0020264391787350178]],\n",
       " [935, [0.00047003276995383203, 0.9957681894302368, 0.0037617648486047983]],\n",
       " [936, [0.9957471489906311, 0.0011594102252274752, 0.0030934580136090517]],\n",
       " [937, [0.0875096246600151, 0.009316552430391312, 0.9031738042831421]],\n",
       " [938, [0.9736437797546387, 0.013998831622302532, 0.012357301078736782]],\n",
       " [939, [0.003211443778127432, 0.00942462868988514, 0.9873639941215515]],\n",
       " [940, [0.9908304214477539, 0.001015440677292645, 0.008154110983014107]],\n",
       " [941, [0.9566915035247803, 0.0030373320914804935, 0.040271203964948654]],\n",
       " [942, [0.0005490319454111159, 0.9912285804748535, 0.008222386240959167]],\n",
       " [943, [0.016680527478456497, 0.5900929570198059, 0.3932264745235443]],\n",
       " [944, [0.0037166657857596874, 0.010225764475762844, 0.9860575795173645]],\n",
       " [945, [0.9882321953773499, 0.0014789866982027888, 0.010288863442838192]],\n",
       " [946, [0.9124581217765808, 0.004815906286239624, 0.08272595703601837]],\n",
       " [947, [0.0012470269575715065, 0.0016459689941257238, 0.9971070885658264]],\n",
       " [948, [0.0030775347258895636, 0.0015885931206867099, 0.9953338503837585]],\n",
       " [949, [0.9906736016273499, 0.0026986640878021717, 0.006627757102251053]],\n",
       " [950, [0.0005001652170903981, 0.9952415227890015, 0.004258361179381609]],\n",
       " [951, [0.031246282160282135, 0.19083504378795624, 0.7779186964035034]],\n",
       " [952, [0.0006678957142867148, 0.9954817295074463, 0.003850329667329788]],\n",
       " [953, [0.0002979471755679697, 0.9930764436721802, 0.006625669542700052]],\n",
       " [954, [0.0012517222203314304, 0.9957364797592163, 0.0030117437709122896]],\n",
       " [955, [0.7590901851654053, 0.004630110692232847, 0.2362796515226364]],\n",
       " [956, [0.0028923670761287212, 0.0016338367713615298, 0.9954738020896912]],\n",
       " [957, [0.9956915974617004, 0.0016373781254515052, 0.002670972840860486]],\n",
       " [958, [0.0005814931937493384, 0.9949695467948914, 0.004448923747986555]],\n",
       " [959, [0.004626754205673933, 0.9802194237709045, 0.015153791755437851]],\n",
       " [960, [0.0010966722620651126, 0.0035011747386306524, 0.9954020977020264]],\n",
       " [961, [0.00121595268137753, 0.9938363432884216, 0.004947651643306017]],\n",
       " [962, [0.0006749220774509013, 0.9716250896453857, 0.027699923142790794]],\n",
       " [963, [0.0017519014654681087, 0.9726070165634155, 0.02564113214612007]],\n",
       " [964, [0.298911452293396, 0.010197262279689312, 0.6908913254737854]],\n",
       " [965, [0.7649676203727722, 0.0011812172597274184, 0.23385114967823029]],\n",
       " [966, [0.001933434628881514, 0.003971660975366831, 0.9940949082374573]],\n",
       " [967, [0.9049782156944275, 0.016445958986878395, 0.07857581228017807]],\n",
       " [968, [0.9208407402038574, 0.000946303247474134, 0.07821286469697952]],\n",
       " [969, [0.0004683106380980462, 0.9452213644981384, 0.05431036651134491]],\n",
       " [970, [0.9942319989204407, 0.0014358005719259381, 0.004332208540290594]],\n",
       " [971, [0.9675289988517761, 0.0024867504835128784, 0.029984284192323685]],\n",
       " [972, [0.006553420331329107, 0.001660500536672771, 0.9917860627174377]],\n",
       " [973, [0.0004517776542343199, 0.9957540035247803, 0.0037942861672490835]],\n",
       " [974, [0.005720953457057476, 0.9049563407897949, 0.08932262659072876]],\n",
       " [975, [0.9586780071258545, 0.005917296279221773, 0.03540479391813278]],\n",
       " [976, [0.9706856608390808, 0.01893925853073597, 0.010375097393989563]],\n",
       " [977, [0.0009405697346664965, 0.002421167679131031, 0.9966381788253784]],\n",
       " [978, [0.9925220608711243, 0.001971124904230237, 0.005506785120815039]],\n",
       " [979, [0.9863373637199402, 0.0036555160768330097, 0.010007143951952457]],\n",
       " [980, [0.9912897348403931, 0.0025157914496958256, 0.0061945123597979546]],\n",
       " [981, [0.0022720082197338343, 0.009476998820900917, 0.9882509112358093]],\n",
       " [982, [0.376496821641922, 0.56653892993927, 0.05696422979235649]],\n",
       " [983, [0.0009958631126210093, 0.9179585576057434, 0.08104564249515533]],\n",
       " [984, [0.0014693246921524405, 0.0027510481886565685, 0.9957796335220337]],\n",
       " [985, [0.0018925588810816407, 0.0017680723685771227, 0.9963394403457642]],\n",
       " [986, [0.0005838568322360516, 0.9465493559837341, 0.0528668574988842]],\n",
       " [987, [0.000871720549184829, 0.9952680468559265, 0.0038601604755967855]],\n",
       " [988, [0.0015603613574057817, 0.0014893069164827466, 0.9969504475593567]],\n",
       " [989, [0.0015159896574914455, 0.9955248832702637, 0.0029591303318738937]],\n",
       " [990, [0.9944215416908264, 0.0013538097264245152, 0.004224695265293121]],\n",
       " [991, [0.9838175773620605, 0.008353986777365208, 0.007828484289348125]],\n",
       " [992, [0.0016448497772216797, 0.0013199594104662538, 0.9970351457595825]],\n",
       " [993, [0.0023584242444485426, 0.9901692867279053, 0.007472375873476267]],\n",
       " [994, [0.004481641110032797, 0.004750542342662811, 0.990767776966095]],\n",
       " [995, [0.9866053462028503, 0.002874883823096752, 0.010519775561988354]],\n",
       " [996, [0.0013386344071477652, 0.001828876556828618, 0.996832549571991]],\n",
       " [997, [0.0007792794494889677, 0.9785524010658264, 0.0206683911383152]],\n",
       " [998, [0.9852230548858643, 0.004809132311493158, 0.009967818856239319]],\n",
       " [999, [0.9887440204620361, 0.0022415833082050085, 0.009014338254928589]],\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_of_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 'contradiction'], [1, 'neutral'], [2, 'entailment'], [3, 'contradiction'], [4, 'entailment'], [5, 'neutral'], [6, 'neutral'], [7, 'neutral'], [8, 'entailment'], [9, 'neutral'], [10, 'contradiction'], [11, 'entailment'], [12, 'contradiction'], [13, 'entailment'], [14, 'neutral'], [15, 'entailment'], [16, 'contradiction'], [17, 'neutral'], [18, 'contradiction'], [19, 'neutral'], [20, 'neutral'], [21, 'neutral'], [22, 'neutral'], [23, 'entailment'], [24, 'contradiction'], [25, 'contradiction'], [26, 'entailment'], [27, 'entailment'], [28, 'entailment'], [29, 'entailment'], [30, 'contradiction'], [31, 'entailment'], [32, 'contradiction'], [33, 'neutral'], [34, 'neutral'], [35, 'contradiction'], [36, 'entailment'], [37, 'contradiction'], [38, 'entailment'], [39, 'contradiction'], [40, 'neutral'], [41, 'entailment'], [42, 'neutral'], [43, 'neutral'], [44, 'neutral'], [45, 'contradiction'], [46, 'neutral'], [47, 'entailment'], [48, 'contradiction'], [49, 'entailment'], [50, 'neutral'], [51, 'contradiction'], [52, 'contradiction'], [53, 'neutral'], [54, 'neutral'], [55, 'neutral'], [56, 'contradiction'], [57, 'neutral'], [58, 'neutral'], [59, 'entailment'], [60, 'entailment'], [61, 'neutral'], [62, 'neutral'], [63, 'neutral'], [64, 'neutral'], [65, 'contradiction'], [66, 'neutral'], [67, 'neutral'], [68, 'neutral'], [69, 'contradiction'], [70, 'entailment'], [71, 'neutral'], [72, 'entailment'], [73, 'contradiction'], [74, 'neutral'], [75, 'neutral'], [76, 'entailment'], [77, 'neutral'], [78, 'neutral'], [79, 'neutral'], [80, 'neutral'], [81, 'contradiction'], [82, 'contradiction'], [83, 'entailment'], [84, 'entailment'], [85, 'contradiction'], [86, 'neutral'], [87, 'contradiction'], [88, 'entailment'], [89, 'neutral'], [90, 'contradiction'], [91, 'neutral'], [92, 'entailment'], [93, 'contradiction'], [94, 'entailment'], [95, 'neutral'], [96, 'contradiction'], [97, 'contradiction'], [98, 'entailment'], [99, 'neutral'], [100, 'neutral'], [101, 'entailment'], [102, 'neutral'], [103, 'entailment'], [104, 'neutral'], [105, 'entailment'], [106, 'neutral'], [107, 'entailment'], [108, 'contradiction'], [109, 'contradiction'], [110, 'contradiction'], [111, 'neutral'], [112, 'contradiction'], [113, 'entailment'], [114, 'neutral'], [115, 'contradiction'], [116, 'entailment'], [117, 'contradiction'], [118, 'contradiction'], [119, 'contradiction'], [120, 'contradiction'], [121, 'contradiction'], [122, 'contradiction'], [123, 'neutral'], [124, 'neutral'], [125, 'entailment'], [126, 'entailment'], [127, 'contradiction'], [128, 'contradiction'], [129, 'neutral'], [130, 'contradiction'], [131, 'neutral'], [132, 'contradiction'], [133, 'entailment'], [134, 'neutral'], [135, 'entailment'], [136, 'contradiction'], [137, 'neutral'], [138, 'contradiction'], [139, 'contradiction'], [140, 'neutral'], [141, 'neutral'], [142, 'neutral'], [143, 'contradiction'], [144, 'neutral'], [145, 'neutral'], [146, 'entailment'], [147, 'contradiction'], [148, 'neutral'], [149, 'neutral'], [150, 'contradiction'], [151, 'entailment'], [152, 'contradiction'], [153, 'neutral'], [154, 'entailment'], [155, 'entailment'], [156, 'entailment'], [157, 'neutral'], [158, 'contradiction'], [159, 'contradiction'], [160, 'entailment'], [161, 'entailment'], [162, 'contradiction'], [163, 'entailment'], [164, 'neutral'], [165, 'neutral'], [166, 'neutral'], [167, 'entailment'], [168, 'neutral'], [169, 'entailment'], [170, 'neutral'], [171, 'neutral'], [172, 'entailment'], [173, 'contradiction'], [174, 'contradiction'], [175, 'neutral'], [176, 'neutral'], [177, 'neutral'], [178, 'contradiction'], [179, 'contradiction'], [180, 'contradiction'], [181, 'neutral'], [182, 'neutral'], [183, 'neutral'], [184, 'entailment'], [185, 'neutral'], [186, 'contradiction'], [187, 'neutral'], [188, 'neutral'], [189, 'contradiction'], [190, 'neutral'], [191, 'entailment'], [192, 'contradiction'], [193, 'neutral'], [194, 'neutral'], [195, 'entailment'], [196, 'contradiction'], [197, 'neutral'], [198, 'entailment'], [199, 'neutral'], [200, 'neutral'], [201, 'neutral'], [202, 'entailment'], [203, 'contradiction'], [204, 'contradiction'], [205, 'entailment'], [206, 'neutral'], [207, 'neutral'], [208, 'entailment'], [209, 'neutral'], [210, 'contradiction'], [211, 'contradiction'], [212, 'contradiction'], [213, 'neutral'], [214, 'neutral'], [215, 'contradiction'], [216, 'contradiction'], [217, 'entailment'], [218, 'contradiction'], [219, 'contradiction'], [220, 'neutral'], [221, 'neutral'], [222, 'entailment'], [223, 'contradiction'], [224, 'neutral'], [225, 'contradiction'], [226, 'contradiction'], [227, 'neutral'], [228, 'contradiction'], [229, 'contradiction'], [230, 'entailment'], [231, 'neutral'], [232, 'entailment'], [233, 'entailment'], [234, 'entailment'], [235, 'contradiction'], [236, 'neutral'], [237, 'neutral'], [238, 'entailment'], [239, 'entailment'], [240, 'entailment'], [241, 'contradiction'], [242, 'neutral'], [243, 'contradiction'], [244, 'neutral'], [245, 'entailment'], [246, 'neutral'], [247, 'neutral'], [248, 'contradiction'], [249, 'contradiction'], [250, 'entailment'], [251, 'contradiction'], [252, 'neutral'], [253, 'entailment'], [254, 'contradiction'], [255, 'neutral'], [256, 'contradiction'], [257, 'contradiction'], [258, 'entailment'], [259, 'entailment'], [260, 'contradiction'], [261, 'entailment'], [262, 'neutral'], [263, 'neutral'], [264, 'neutral'], [265, 'neutral'], [266, 'contradiction'], [267, 'neutral'], [268, 'contradiction'], [269, 'entailment'], [270, 'contradiction'], [271, 'neutral'], [272, 'entailment'], [273, 'entailment'], [274, 'neutral'], [275, 'contradiction'], [276, 'contradiction'], [277, 'entailment'], [278, 'neutral'], [279, 'contradiction'], [280, 'neutral'], [281, 'entailment'], [282, 'entailment'], [283, 'entailment'], [284, 'contradiction'], [285, 'entailment'], [286, 'contradiction'], [287, 'contradiction'], [288, 'entailment'], [289, 'neutral'], [290, 'entailment'], [291, 'contradiction'], [292, 'entailment'], [293, 'contradiction'], [294, 'entailment'], [295, 'contradiction'], [296, 'contradiction'], [297, 'contradiction'], [298, 'neutral'], [299, 'entailment'], [300, 'neutral'], [301, 'entailment'], [302, 'neutral'], [303, 'entailment'], [304, 'entailment'], [305, 'entailment'], [306, 'entailment'], [307, 'contradiction'], [308, 'entailment'], [309, 'entailment'], [310, 'neutral'], [311, 'contradiction'], [312, 'contradiction'], [313, 'neutral'], [314, 'neutral'], [315, 'neutral'], [316, 'neutral'], [317, 'neutral'], [318, 'entailment'], [319, 'entailment'], [320, 'entailment'], [321, 'neutral'], [322, 'neutral'], [323, 'entailment'], [324, 'contradiction'], [325, 'entailment'], [326, 'contradiction'], [327, 'neutral'], [328, 'contradiction'], [329, 'contradiction'], [330, 'neutral'], [331, 'entailment'], [332, 'entailment'], [333, 'contradiction'], [334, 'contradiction'], [335, 'contradiction'], [336, 'entailment'], [337, 'contradiction'], [338, 'contradiction'], [339, 'entailment'], [340, 'contradiction'], [341, 'neutral'], [342, 'contradiction'], [343, 'contradiction'], [344, 'neutral'], [345, 'entailment'], [346, 'contradiction'], [347, 'contradiction'], [348, 'contradiction'], [349, 'neutral'], [350, 'contradiction'], [351, 'neutral'], [352, 'entailment'], [353, 'contradiction'], [354, 'neutral'], [355, 'contradiction'], [356, 'neutral'], [357, 'entailment'], [358, 'contradiction'], [359, 'neutral'], [360, 'contradiction'], [361, 'contradiction'], [362, 'contradiction'], [363, 'neutral'], [364, 'contradiction'], [365, 'neutral'], [366, 'contradiction'], [367, 'entailment'], [368, 'contradiction'], [369, 'neutral'], [370, 'contradiction'], [371, 'contradiction'], [372, 'entailment'], [373, 'neutral'], [374, 'neutral'], [375, 'entailment'], [376, 'entailment'], [377, 'neutral'], [378, 'entailment'], [379, 'entailment'], [380, 'entailment'], [381, 'neutral'], [382, 'contradiction'], [383, 'neutral'], [384, 'entailment'], [385, 'neutral'], [386, 'entailment'], [387, 'entailment'], [388, 'neutral'], [389, 'entailment'], [390, 'neutral'], [391, 'entailment'], [392, 'contradiction'], [393, 'neutral'], [394, 'contradiction'], [395, 'neutral'], [396, 'neutral'], [397, 'neutral'], [398, 'contradiction'], [399, 'entailment'], [400, 'contradiction'], [401, 'entailment'], [402, 'contradiction'], [403, 'contradiction'], [404, 'entailment'], [405, 'neutral'], [406, 'contradiction'], [407, 'neutral'], [408, 'contradiction'], [409, 'neutral'], [410, 'contradiction'], [411, 'contradiction'], [412, 'contradiction'], [413, 'contradiction'], [414, 'neutral'], [415, 'neutral'], [416, 'contradiction'], [417, 'entailment'], [418, 'neutral'], [419, 'contradiction'], [420, 'neutral'], [421, 'neutral'], [422, 'entailment'], [423, 'neutral'], [424, 'entailment'], [425, 'neutral'], [426, 'neutral'], [427, 'contradiction'], [428, 'contradiction'], [429, 'contradiction'], [430, 'entailment'], [431, 'contradiction'], [432, 'neutral'], [433, 'neutral'], [434, 'entailment'], [435, 'neutral'], [436, 'entailment'], [437, 'entailment'], [438, 'entailment'], [439, 'entailment'], [440, 'entailment'], [441, 'entailment'], [442, 'entailment'], [443, 'neutral'], [444, 'contradiction'], [445, 'neutral'], [446, 'neutral'], [447, 'entailment'], [448, 'contradiction'], [449, 'contradiction'], [450, 'contradiction'], [451, 'entailment'], [452, 'entailment'], [453, 'contradiction'], [454, 'contradiction'], [455, 'entailment'], [456, 'neutral'], [457, 'contradiction'], [458, 'entailment'], [459, 'contradiction'], [460, 'entailment'], [461, 'entailment'], [462, 'contradiction'], [463, 'contradiction'], [464, 'entailment'], [465, 'contradiction'], [466, 'contradiction'], [467, 'neutral'], [468, 'entailment'], [469, 'neutral'], [470, 'contradiction'], [471, 'neutral'], [472, 'neutral'], [473, 'entailment'], [474, 'entailment'], [475, 'neutral'], [476, 'entailment'], [477, 'contradiction'], [478, 'contradiction'], [479, 'contradiction'], [480, 'neutral'], [481, 'entailment'], [482, 'entailment'], [483, 'contradiction'], [484, 'neutral'], [485, 'neutral'], [486, 'entailment'], [487, 'neutral'], [488, 'contradiction'], [489, 'entailment'], [490, 'entailment'], [491, 'neutral'], [492, 'neutral'], [493, 'entailment'], [494, 'entailment'], [495, 'entailment'], [496, 'entailment'], [497, 'neutral'], [498, 'entailment'], [499, 'entailment'], [500, 'entailment'], [501, 'entailment'], [502, 'neutral'], [503, 'contradiction'], [504, 'contradiction'], [505, 'entailment'], [506, 'neutral'], [507, 'contradiction'], [508, 'entailment'], [509, 'contradiction'], [510, 'contradiction'], [511, 'entailment'], [512, 'entailment'], [513, 'entailment'], [514, 'entailment'], [515, 'contradiction'], [516, 'entailment'], [517, 'entailment'], [518, 'contradiction'], [519, 'contradiction'], [520, 'entailment'], [521, 'neutral'], [522, 'contradiction'], [523, 'neutral'], [524, 'neutral'], [525, 'contradiction'], [526, 'entailment'], [527, 'entailment'], [528, 'contradiction'], [529, 'entailment'], [530, 'contradiction'], [531, 'neutral'], [532, 'contradiction'], [533, 'neutral'], [534, 'neutral'], [535, 'contradiction'], [536, 'contradiction'], [537, 'contradiction'], [538, 'neutral'], [539, 'neutral'], [540, 'neutral'], [541, 'contradiction'], [542, 'contradiction'], [543, 'entailment'], [544, 'contradiction'], [545, 'neutral'], [546, 'contradiction'], [547, 'entailment'], [548, 'neutral'], [549, 'entailment'], [550, 'neutral'], [551, 'contradiction'], [552, 'entailment'], [553, 'entailment'], [554, 'entailment'], [555, 'neutral'], [556, 'entailment'], [557, 'neutral'], [558, 'neutral'], [559, 'contradiction'], [560, 'entailment'], [561, 'contradiction'], [562, 'neutral'], [563, 'neutral'], [564, 'neutral'], [565, 'neutral'], [566, 'entailment'], [567, 'entailment'], [568, 'neutral'], [569, 'contradiction'], [570, 'neutral'], [571, 'entailment'], [572, 'entailment'], [573, 'contradiction'], [574, 'entailment'], [575, 'contradiction'], [576, 'entailment'], [577, 'neutral'], [578, 'contradiction'], [579, 'neutral'], [580, 'contradiction'], [581, 'entailment'], [582, 'entailment'], [583, 'neutral'], [584, 'entailment'], [585, 'entailment'], [586, 'neutral'], [587, 'entailment'], [588, 'contradiction'], [589, 'entailment'], [590, 'entailment'], [591, 'neutral'], [592, 'contradiction'], [593, 'entailment'], [594, 'contradiction'], [595, 'contradiction'], [596, 'contradiction'], [597, 'neutral'], [598, 'entailment'], [599, 'entailment'], [600, 'contradiction'], [601, 'neutral'], [602, 'contradiction'], [603, 'entailment'], [604, 'contradiction'], [605, 'neutral'], [606, 'contradiction'], [607, 'contradiction'], [608, 'contradiction'], [609, 'contradiction'], [610, 'entailment'], [611, 'entailment'], [612, 'neutral'], [613, 'entailment'], [614, 'contradiction'], [615, 'contradiction'], [616, 'neutral'], [617, 'neutral'], [618, 'contradiction'], [619, 'neutral'], [620, 'contradiction'], [621, 'neutral'], [622, 'entailment'], [623, 'contradiction'], [624, 'entailment'], [625, 'contradiction'], [626, 'contradiction'], [627, 'contradiction'], [628, 'neutral'], [629, 'entailment'], [630, 'entailment'], [631, 'entailment'], [632, 'neutral'], [633, 'entailment'], [634, 'neutral'], [635, 'contradiction'], [636, 'contradiction'], [637, 'entailment'], [638, 'entailment'], [639, 'contradiction'], [640, 'contradiction'], [641, 'entailment'], [642, 'neutral'], [643, 'entailment'], [644, 'neutral'], [645, 'entailment'], [646, 'neutral'], [647, 'contradiction'], [648, 'contradiction'], [649, 'neutral'], [650, 'entailment'], [651, 'contradiction'], [652, 'contradiction'], [653, 'entailment'], [654, 'neutral'], [655, 'neutral'], [656, 'entailment'], [657, 'contradiction'], [658, 'entailment'], [659, 'contradiction'], [660, 'neutral'], [661, 'contradiction'], [662, 'neutral'], [663, 'contradiction'], [664, 'neutral'], [665, 'contradiction'], [666, 'contradiction'], [667, 'neutral'], [668, 'neutral'], [669, 'neutral'], [670, 'neutral'], [671, 'contradiction'], [672, 'contradiction'], [673, 'contradiction'], [674, 'neutral'], [675, 'entailment'], [676, 'neutral'], [677, 'neutral'], [678, 'contradiction'], [679, 'contradiction'], [680, 'neutral'], [681, 'contradiction'], [682, 'entailment'], [683, 'contradiction'], [684, 'contradiction'], [685, 'contradiction'], [686, 'entailment'], [687, 'contradiction'], [688, 'entailment'], [689, 'entailment'], [690, 'neutral'], [691, 'entailment'], [692, 'entailment'], [693, 'neutral'], [694, 'entailment'], [695, 'contradiction'], [696, 'entailment'], [697, 'contradiction'], [698, 'entailment'], [699, 'entailment'], [700, 'neutral'], [701, 'entailment'], [702, 'entailment'], [703, 'neutral'], [704, 'entailment'], [705, 'entailment'], [706, 'contradiction'], [707, 'contradiction'], [708, 'neutral'], [709, 'contradiction'], [710, 'neutral'], [711, 'entailment'], [712, 'neutral'], [713, 'entailment'], [714, 'entailment'], [715, 'entailment'], [716, 'neutral'], [717, 'neutral'], [718, 'neutral'], [719, 'entailment'], [720, 'entailment'], [721, 'neutral'], [722, 'contradiction'], [723, 'contradiction'], [724, 'entailment'], [725, 'contradiction'], [726, 'neutral'], [727, 'contradiction'], [728, 'contradiction'], [729, 'entailment'], [730, 'entailment'], [731, 'neutral'], [732, 'entailment'], [733, 'neutral'], [734, 'entailment'], [735, 'neutral'], [736, 'contradiction'], [737, 'neutral'], [738, 'neutral'], [739, 'entailment'], [740, 'neutral'], [741, 'entailment'], [742, 'entailment'], [743, 'entailment'], [744, 'contradiction'], [745, 'neutral'], [746, 'neutral'], [747, 'contradiction'], [748, 'neutral'], [749, 'contradiction'], [750, 'neutral'], [751, 'neutral'], [752, 'neutral'], [753, 'neutral'], [754, 'entailment'], [755, 'neutral'], [756, 'neutral'], [757, 'contradiction'], [758, 'neutral'], [759, 'neutral'], [760, 'entailment'], [761, 'neutral'], [762, 'entailment'], [763, 'contradiction'], [764, 'neutral'], [765, 'entailment'], [766, 'neutral'], [767, 'entailment'], [768, 'contradiction'], [769, 'neutral'], [770, 'contradiction'], [771, 'neutral'], [772, 'entailment'], [773, 'entailment'], [774, 'neutral'], [775, 'neutral'], [776, 'contradiction'], [777, 'entailment'], [778, 'neutral'], [779, 'neutral'], [780, 'entailment'], [781, 'neutral'], [782, 'contradiction'], [783, 'neutral'], [784, 'entailment'], [785, 'entailment'], [786, 'contradiction'], [787, 'contradiction'], [788, 'entailment'], [789, 'contradiction'], [790, 'entailment'], [791, 'contradiction'], [792, 'entailment'], [793, 'neutral'], [794, 'neutral'], [795, 'entailment'], [796, 'contradiction'], [797, 'entailment'], [798, 'entailment'], [799, 'neutral'], [800, 'entailment'], [801, 'contradiction'], [802, 'entailment'], [803, 'neutral'], [804, 'contradiction'], [805, 'contradiction'], [806, 'entailment'], [807, 'entailment'], [808, 'contradiction'], [809, 'contradiction'], [810, 'entailment'], [811, 'contradiction'], [812, 'contradiction'], [813, 'neutral'], [814, 'neutral'], [815, 'neutral'], [816, 'neutral'], [817, 'neutral'], [818, 'neutral'], [819, 'entailment'], [820, 'neutral'], [821, 'neutral'], [822, 'neutral'], [823, 'neutral'], [824, 'neutral'], [825, 'neutral'], [826, 'contradiction'], [827, 'neutral'], [828, 'contradiction'], [829, 'neutral'], [830, 'entailment'], [831, 'contradiction'], [832, 'contradiction'], [833, 'entailment'], [834, 'neutral'], [835, 'contradiction'], [836, 'entailment'], [837, 'contradiction'], [838, 'neutral'], [839, 'contradiction'], [840, 'contradiction'], [841, 'entailment'], [842, 'contradiction'], [843, 'entailment'], [844, 'neutral'], [845, 'neutral'], [846, 'contradiction'], [847, 'contradiction'], [848, 'entailment'], [849, 'neutral'], [850, 'neutral'], [851, 'contradiction'], [852, 'entailment'], [853, 'neutral'], [854, 'contradiction'], [855, 'entailment'], [856, 'entailment'], [857, 'neutral'], [858, 'contradiction'], [859, 'entailment'], [860, 'contradiction'], [861, 'neutral'], [862, 'neutral'], [863, 'neutral'], [864, 'neutral'], [865, 'entailment'], [866, 'entailment'], [867, 'neutral'], [868, 'contradiction'], [869, 'neutral'], [870, 'neutral'], [871, 'contradiction'], [872, 'entailment'], [873, 'neutral'], [874, 'neutral'], [875, 'contradiction'], [876, 'entailment'], [877, 'contradiction'], [878, 'neutral'], [879, 'neutral'], [880, 'neutral'], [881, 'contradiction'], [882, 'contradiction'], [883, 'entailment'], [884, 'entailment'], [885, 'contradiction'], [886, 'neutral'], [887, 'neutral'], [888, 'entailment'], [889, 'neutral'], [890, 'entailment'], [891, 'entailment'], [892, 'entailment'], [893, 'entailment'], [894, 'contradiction'], [895, 'neutral'], [896, 'entailment'], [897, 'neutral'], [898, 'entailment'], [899, 'neutral'], [900, 'contradiction'], [901, 'contradiction'], [902, 'entailment'], [903, 'entailment'], [904, 'entailment'], [905, 'neutral'], [906, 'contradiction'], [907, 'contradiction'], [908, 'neutral'], [909, 'entailment'], [910, 'neutral'], [911, 'neutral'], [912, 'neutral'], [913, 'entailment'], [914, 'neutral'], [915, 'neutral'], [916, 'contradiction'], [917, 'contradiction'], [918, 'neutral'], [919, 'entailment'], [920, 'neutral'], [921, 'entailment'], [922, 'entailment'], [923, 'contradiction'], [924, 'neutral'], [925, 'contradiction'], [926, 'entailment'], [927, 'entailment'], [928, 'entailment'], [929, 'contradiction'], [930, 'neutral'], [931, 'entailment'], [932, 'neutral'], [933, 'neutral'], [934, 'contradiction'], [935, 'neutral'], [936, 'entailment'], [937, 'neutral'], [938, 'entailment'], [939, 'contradiction'], [940, 'entailment'], [941, 'entailment'], [942, 'contradiction'], [943, 'neutral'], [944, 'neutral'], [945, 'entailment'], [946, 'entailment'], [947, 'neutral'], [948, 'neutral'], [949, 'entailment'], [950, 'contradiction'], [951, 'contradiction'], [952, 'contradiction'], [953, 'contradiction'], [954, 'contradiction'], [955, 'neutral'], [956, 'contradiction'], [957, 'entailment'], [958, 'contradiction'], [959, 'contradiction'], [960, 'neutral'], [961, 'contradiction'], [962, 'contradiction'], [963, 'contradiction'], [964, 'entailment'], [965, 'neutral'], [966, 'contradiction'], [967, 'entailment'], [968, 'entailment'], [969, 'contradiction'], [970, 'entailment'], [971, 'entailment'], [972, 'neutral'], [973, 'contradiction'], [974, 'contradiction'], [975, 'entailment'], [976, 'entailment'], [977, 'neutral'], [978, 'entailment'], [979, 'entailment'], [980, 'entailment'], [981, 'neutral'], [982, 'entailment'], [983, 'contradiction'], [984, 'neutral'], [985, 'neutral'], [986, 'contradiction'], [987, 'contradiction'], [988, 'neutral'], [989, 'contradiction'], [990, 'entailment'], [991, 'entailment'], [992, 'neutral'], [993, 'contradiction'], [994, 'neutral'], [995, 'entailment'], [996, 'neutral'], [997, 'contradiction'], [998, 'entailment'], [999, 'entailment'], [1000, 'entailment'], [1001, 'entailment'], [1002, 'entailment'], [1003, 'neutral'], [1004, 'neutral'], [1005, 'entailment'], [1006, 'entailment'], [1007, 'entailment'], [1008, 'neutral'], [1009, 'neutral'], [1010, 'neutral'], [1011, 'entailment'], [1012, 'neutral'], [1013, 'contradiction'], [1014, 'neutral'], [1015, 'entailment'], [1016, 'entailment'], [1017, 'entailment'], [1018, 'contradiction'], [1019, 'contradiction'], [1020, 'entailment'], [1021, 'neutral'], [1022, 'neutral'], [1023, 'neutral'], [1024, 'entailment'], [1025, 'neutral'], [1026, 'neutral'], [1027, 'contradiction'], [1028, 'contradiction'], [1029, 'contradiction'], [1030, 'entailment'], [1031, 'neutral'], [1032, 'entailment'], [1033, 'entailment'], [1034, 'neutral'], [1035, 'contradiction'], [1036, 'neutral'], [1037, 'contradiction'], [1038, 'contradiction'], [1039, 'contradiction'], [1040, 'contradiction'], [1041, 'neutral'], [1042, 'contradiction'], [1043, 'entailment'], [1044, 'contradiction'], [1045, 'neutral'], [1046, 'contradiction'], [1047, 'entailment'], [1048, 'contradiction'], [1049, 'contradiction'], [1050, 'entailment'], [1051, 'contradiction'], [1052, 'entailment'], [1053, 'entailment'], [1054, 'neutral'], [1055, 'neutral'], [1056, 'neutral'], [1057, 'contradiction'], [1058, 'entailment'], [1059, 'neutral'], [1060, 'neutral'], [1061, 'contradiction'], [1062, 'contradiction'], [1063, 'entailment'], [1064, 'entailment'], [1065, 'neutral'], [1066, 'entailment'], [1067, 'entailment'], [1068, 'entailment'], [1069, 'neutral'], [1070, 'entailment'], [1071, 'entailment'], [1072, 'neutral'], [1073, 'neutral'], [1074, 'entailment'], [1075, 'neutral'], [1076, 'entailment'], [1077, 'neutral'], [1078, 'contradiction'], [1079, 'entailment'], [1080, 'neutral'], [1081, 'entailment'], [1082, 'contradiction'], [1083, 'contradiction'], [1084, 'contradiction'], [1085, 'contradiction'], [1086, 'contradiction'], [1087, 'entailment'], [1088, 'contradiction'], [1089, 'neutral'], [1090, 'entailment'], [1091, 'neutral'], [1092, 'entailment'], [1093, 'contradiction'], [1094, 'neutral'], [1095, 'neutral'], [1096, 'entailment'], [1097, 'neutral'], [1098, 'neutral'], [1099, 'contradiction'], [1100, 'entailment'], [1101, 'contradiction'], [1102, 'neutral'], [1103, 'entailment'], [1104, 'contradiction'], [1105, 'entailment'], [1106, 'contradiction'], [1107, 'contradiction'], [1108, 'entailment'], [1109, 'entailment'], [1110, 'entailment'], [1111, 'neutral'], [1112, 'contradiction'], [1113, 'contradiction'], [1114, 'neutral'], [1115, 'neutral'], [1116, 'neutral'], [1117, 'neutral'], [1118, 'entailment'], [1119, 'contradiction'], [1120, 'contradiction'], [1121, 'entailment'], [1122, 'neutral'], [1123, 'contradiction'], [1124, 'neutral'], [1125, 'contradiction'], [1126, 'entailment'], [1127, 'contradiction'], [1128, 'neutral'], [1129, 'contradiction'], [1130, 'entailment'], [1131, 'entailment'], [1132, 'contradiction'], [1133, 'contradiction'], [1134, 'contradiction'], [1135, 'contradiction'], [1136, 'contradiction'], [1137, 'contradiction'], [1138, 'contradiction'], [1139, 'entailment'], [1140, 'entailment'], [1141, 'neutral'], [1142, 'contradiction'], [1143, 'contradiction'], [1144, 'entailment'], [1145, 'neutral'], [1146, 'entailment'], [1147, 'entailment'], [1148, 'entailment'], [1149, 'contradiction'], [1150, 'entailment'], [1151, 'contradiction'], [1152, 'neutral'], [1153, 'contradiction'], [1154, 'entailment'], [1155, 'neutral'], [1156, 'neutral'], [1157, 'entailment'], [1158, 'neutral'], [1159, 'neutral'], [1160, 'entailment'], [1161, 'contradiction'], [1162, 'contradiction'], [1163, 'contradiction'], [1164, 'neutral'], [1165, 'neutral'], [1166, 'entailment'], [1167, 'entailment'], [1168, 'neutral'], [1169, 'neutral'], [1170, 'contradiction'], [1171, 'neutral'], [1172, 'contradiction'], [1173, 'neutral'], [1174, 'neutral'], [1175, 'entailment'], [1176, 'neutral'], [1177, 'entailment'], [1178, 'neutral'], [1179, 'contradiction'], [1180, 'neutral'], [1181, 'entailment'], [1182, 'contradiction'], [1183, 'neutral'], [1184, 'contradiction'], [1185, 'neutral'], [1186, 'neutral'], [1187, 'neutral'], [1188, 'entailment'], [1189, 'contradiction'], [1190, 'entailment'], [1191, 'entailment'], [1192, 'entailment'], [1193, 'neutral'], [1194, 'entailment'], [1195, 'entailment'], [1196, 'contradiction'], [1197, 'neutral'], [1198, 'contradiction'], [1199, 'contradiction'], [1200, 'entailment'], [1201, 'contradiction'], [1202, 'entailment'], [1203, 'neutral'], [1204, 'neutral'], [1205, 'neutral'], [1206, 'contradiction'], [1207, 'contradiction'], [1208, 'contradiction'], [1209, 'contradiction'], [1210, 'neutral'], [1211, 'entailment'], [1212, 'entailment'], [1213, 'contradiction'], [1214, 'neutral'], [1215, 'contradiction'], [1216, 'neutral'], [1217, 'entailment'], [1218, 'neutral'], [1219, 'neutral'], [1220, 'neutral'], [1221, 'contradiction'], [1222, 'neutral'], [1223, 'neutral'], [1224, 'entailment'], [1225, 'contradiction'], [1226, 'contradiction'], [1227, 'entailment'], [1228, 'entailment'], [1229, 'neutral'], [1230, 'contradiction'], [1231, 'contradiction'], [1232, 'entailment'], [1233, 'contradiction'], [1234, 'contradiction'], [1235, 'entailment'], [1236, 'entailment'], [1237, 'entailment'], [1238, 'entailment'], [1239, 'neutral'], [1240, 'neutral'], [1241, 'neutral'], [1242, 'neutral'], [1243, 'contradiction'], [1244, 'entailment'], [1245, 'contradiction'], [1246, 'contradiction'], [1247, 'contradiction'], [1248, 'neutral'], [1249, 'entailment'], [1250, 'neutral'], [1251, 'entailment'], [1252, 'neutral'], [1253, 'contradiction'], [1254, 'neutral'], [1255, 'contradiction'], [1256, 'entailment'], [1257, 'entailment'], [1258, 'entailment'], [1259, 'entailment'], [1260, 'neutral'], [1261, 'entailment'], [1262, 'entailment'], [1263, 'neutral'], [1264, 'neutral'], [1265, 'neutral'], [1266, 'entailment'], [1267, 'contradiction'], [1268, 'contradiction'], [1269, 'contradiction'], [1270, 'contradiction'], [1271, 'entailment'], [1272, 'contradiction'], [1273, 'entailment'], [1274, 'contradiction'], [1275, 'neutral'], [1276, 'neutral'], [1277, 'entailment'], [1278, 'contradiction'], [1279, 'entailment'], [1280, 'contradiction'], [1281, 'entailment'], [1282, 'contradiction'], [1283, 'entailment'], [1284, 'neutral'], [1285, 'entailment'], [1286, 'neutral'], [1287, 'neutral'], [1288, 'entailment'], [1289, 'neutral'], [1290, 'neutral'], [1291, 'neutral'], [1292, 'neutral'], [1293, 'neutral'], [1294, 'entailment'], [1295, 'contradiction'], [1296, 'contradiction'], [1297, 'neutral'], [1298, 'contradiction'], [1299, 'neutral'], [1300, 'contradiction'], [1301, 'entailment'], [1302, 'neutral'], [1303, 'neutral'], [1304, 'neutral'], [1305, 'neutral'], [1306, 'entailment'], [1307, 'neutral'], [1308, 'entailment'], [1309, 'contradiction'], [1310, 'neutral'], [1311, 'contradiction'], [1312, 'neutral'], [1313, 'neutral'], [1314, 'entailment'], [1315, 'neutral'], [1316, 'neutral'], [1317, 'entailment'], [1318, 'neutral'], [1319, 'contradiction'], [1320, 'contradiction'], [1321, 'entailment'], [1322, 'neutral'], [1323, 'contradiction'], [1324, 'entailment'], [1325, 'neutral'], [1326, 'neutral'], [1327, 'neutral'], [1328, 'neutral'], [1329, 'contradiction'], [1330, 'neutral'], [1331, 'neutral'], [1332, 'contradiction'], [1333, 'neutral'], [1334, 'contradiction'], [1335, 'neutral'], [1336, 'contradiction'], [1337, 'entailment'], [1338, 'entailment'], [1339, 'neutral'], [1340, 'entailment'], [1341, 'contradiction'], [1342, 'entailment'], [1343, 'neutral'], [1344, 'entailment'], [1345, 'contradiction'], [1346, 'contradiction'], [1347, 'neutral'], [1348, 'contradiction'], [1349, 'neutral'], [1350, 'neutral'], [1351, 'neutral'], [1352, 'contradiction'], [1353, 'entailment'], [1354, 'entailment'], [1355, 'contradiction'], [1356, 'contradiction'], [1357, 'contradiction'], [1358, 'entailment'], [1359, 'neutral'], [1360, 'neutral'], [1361, 'contradiction'], [1362, 'neutral'], [1363, 'neutral'], [1364, 'neutral'], [1365, 'neutral'], [1366, 'contradiction'], [1367, 'contradiction'], [1368, 'neutral'], [1369, 'entailment'], [1370, 'neutral'], [1371, 'contradiction'], [1372, 'neutral'], [1373, 'contradiction'], [1374, 'neutral'], [1375, 'neutral'], [1376, 'neutral'], [1377, 'contradiction'], [1378, 'entailment'], [1379, 'contradiction'], [1380, 'contradiction'], [1381, 'contradiction'], [1382, 'neutral'], [1383, 'entailment'], [1384, 'contradiction'], [1385, 'entailment'], [1386, 'entailment'], [1387, 'contradiction'], [1388, 'neutral'], [1389, 'contradiction'], [1390, 'contradiction'], [1391, 'neutral'], [1392, 'neutral'], [1393, 'entailment'], [1394, 'entailment'], [1395, 'neutral'], [1396, 'entailment'], [1397, 'entailment'], [1398, 'entailment'], [1399, 'contradiction'], [1400, 'neutral'], [1401, 'neutral'], [1402, 'neutral'], [1403, 'entailment'], [1404, 'entailment'], [1405, 'contradiction'], [1406, 'entailment'], [1407, 'entailment'], [1408, 'contradiction'], [1409, 'contradiction'], [1410, 'contradiction'], [1411, 'neutral'], [1412, 'neutral'], [1413, 'entailment'], [1414, 'contradiction'], [1415, 'neutral'], [1416, 'neutral'], [1417, 'contradiction'], [1418, 'entailment'], [1419, 'entailment'], [1420, 'entailment'], [1421, 'contradiction'], [1422, 'contradiction'], [1423, 'entailment'], [1424, 'contradiction'], [1425, 'entailment'], [1426, 'entailment'], [1427, 'contradiction'], [1428, 'neutral'], [1429, 'neutral'], [1430, 'entailment'], [1431, 'neutral'], [1432, 'neutral'], [1433, 'entailment'], [1434, 'neutral'], [1435, 'neutral'], [1436, 'contradiction'], [1437, 'entailment'], [1438, 'neutral'], [1439, 'contradiction'], [1440, 'contradiction'], [1441, 'entailment'], [1442, 'contradiction'], [1443, 'entailment'], [1444, 'neutral'], [1445, 'contradiction'], [1446, 'contradiction'], [1447, 'neutral'], [1448, 'contradiction'], [1449, 'entailment'], [1450, 'entailment'], [1451, 'neutral'], [1452, 'entailment'], [1453, 'neutral'], [1454, 'entailment'], [1455, 'contradiction'], [1456, 'contradiction'], [1457, 'neutral'], [1458, 'contradiction'], [1459, 'neutral'], [1460, 'contradiction'], [1461, 'neutral'], [1462, 'contradiction'], [1463, 'entailment'], [1464, 'entailment'], [1465, 'contradiction'], [1466, 'contradiction'], [1467, 'neutral'], [1468, 'neutral'], [1469, 'contradiction'], [1470, 'entailment'], [1471, 'contradiction'], [1472, 'neutral'], [1473, 'contradiction'], [1474, 'contradiction'], [1475, 'neutral'], [1476, 'neutral'], [1477, 'entailment'], [1478, 'entailment'], [1479, 'contradiction'], [1480, 'contradiction'], [1481, 'contradiction'], [1482, 'contradiction'], [1483, 'neutral'], [1484, 'entailment'], [1485, 'neutral'], [1486, 'neutral'], [1487, 'contradiction'], [1488, 'contradiction'], [1489, 'entailment'], [1490, 'contradiction'], [1491, 'entailment'], [1492, 'neutral'], [1493, 'entailment'], [1494, 'neutral'], [1495, 'entailment'], [1496, 'contradiction'], [1497, 'contradiction'], [1498, 'entailment'], [1499, 'contradiction'], [1500, 'contradiction'], [1501, 'entailment'], [1502, 'contradiction'], [1503, 'entailment'], [1504, 'neutral'], [1505, 'contradiction'], [1506, 'entailment'], [1507, 'entailment'], [1508, 'neutral'], [1509, 'entailment'], [1510, 'neutral'], [1511, 'contradiction'], [1512, 'contradiction'], [1513, 'neutral'], [1514, 'contradiction'], [1515, 'contradiction'], [1516, 'entailment'], [1517, 'neutral'], [1518, 'contradiction'], [1519, 'contradiction'], [1520, 'neutral'], [1521, 'contradiction'], [1522, 'neutral'], [1523, 'entailment'], [1524, 'contradiction'], [1525, 'neutral'], [1526, 'contradiction'], [1527, 'neutral'], [1528, 'neutral'], [1529, 'contradiction'], [1530, 'neutral'], [1531, 'entailment'], [1532, 'entailment'], [1533, 'contradiction'], [1534, 'neutral'], [1535, 'contradiction'], [1536, 'neutral'], [1537, 'entailment'], [1538, 'entailment'], [1539, 'neutral'], [1540, 'contradiction'], [1541, 'contradiction'], [1542, 'neutral'], [1543, 'contradiction'], [1544, 'neutral'], [1545, 'neutral'], [1546, 'neutral'], [1547, 'neutral'], [1548, 'neutral'], [1549, 'neutral'], [1550, 'contradiction'], [1551, 'entailment'], [1552, 'neutral'], [1553, 'entailment'], [1554, 'neutral'], [1555, 'neutral'], [1556, 'entailment'], [1557, 'neutral'], [1558, 'entailment'], [1559, 'entailment'], [1560, 'neutral'], [1561, 'entailment'], [1562, 'neutral'], [1563, 'contradiction'], [1564, 'contradiction'], [1565, 'entailment'], [1566, 'contradiction'], [1567, 'entailment'], [1568, 'entailment'], [1569, 'contradiction'], [1570, 'entailment'], [1571, 'neutral'], [1572, 'entailment'], [1573, 'contradiction'], [1574, 'contradiction'], [1575, 'contradiction'], [1576, 'neutral'], [1577, 'contradiction'], [1578, 'neutral'], [1579, 'neutral'], [1580, 'neutral'], [1581, 'contradiction'], [1582, 'entailment'], [1583, 'entailment'], [1584, 'neutral'], [1585, 'neutral'], [1586, 'neutral'], [1587, 'neutral'], [1588, 'entailment'], [1589, 'entailment'], [1590, 'contradiction'], [1591, 'contradiction'], [1592, 'contradiction'], [1593, 'entailment'], [1594, 'neutral'], [1595, 'neutral'], [1596, 'entailment'], [1597, 'entailment'], [1598, 'entailment'], [1599, 'neutral'], [1600, 'contradiction'], [1601, 'contradiction'], [1602, 'contradiction'], [1603, 'neutral'], [1604, 'entailment'], [1605, 'neutral'], [1606, 'entailment'], [1607, 'entailment'], [1608, 'contradiction'], [1609, 'entailment'], [1610, 'neutral'], [1611, 'neutral'], [1612, 'entailment'], [1613, 'neutral'], [1614, 'entailment'], [1615, 'contradiction'], [1616, 'neutral'], [1617, 'neutral'], [1618, 'neutral'], [1619, 'contradiction'], [1620, 'neutral'], [1621, 'entailment'], [1622, 'contradiction'], [1623, 'neutral'], [1624, 'contradiction'], [1625, 'neutral'], [1626, 'neutral'], [1627, 'contradiction'], [1628, 'contradiction'], [1629, 'neutral'], [1630, 'contradiction'], [1631, 'entailment'], [1632, 'entailment'], [1633, 'entailment'], [1634, 'neutral'], [1635, 'neutral'], [1636, 'entailment'], [1637, 'contradiction'], [1638, 'entailment'], [1639, 'entailment'], [1640, 'entailment'], [1641, 'entailment'], [1642, 'entailment'], [1643, 'entailment'], [1644, 'contradiction'], [1645, 'neutral'], [1646, 'contradiction'], [1647, 'neutral'], [1648, 'entailment'], [1649, 'contradiction'], [1650, 'contradiction'], [1651, 'entailment'], [1652, 'neutral'], [1653, 'contradiction'], [1654, 'contradiction'], [1655, 'entailment'], [1656, 'contradiction'], [1657, 'contradiction'], [1658, 'neutral'], [1659, 'neutral'], [1660, 'entailment'], [1661, 'neutral'], [1662, 'contradiction'], [1663, 'neutral'], [1664, 'neutral'], [1665, 'entailment']]\n"
     ]
    }
   ],
   "source": [
    "def num_to_label(label):\n",
    "    label_dict = {0: \"entailment\", 1: \"contradiction\", 2: \"neutral\"}\n",
    "    str_label = []\n",
    "\n",
    "    for i, v in enumerate(label):\n",
    "        str_label.append([i,label_dict[v]])\n",
    "    \n",
    "    return str_label\n",
    "\n",
    "answer = num_to_label(pred_answer)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      index          label\n",
      "0         0  contradiction\n",
      "1         1        neutral\n",
      "2         2     entailment\n",
      "3         3  contradiction\n",
      "4         4     entailment\n",
      "...     ...            ...\n",
      "1661   1661        neutral\n",
      "1662   1662  contradiction\n",
      "1663   1663        neutral\n",
      "1664   1664        neutral\n",
      "1665   1665     entailment\n",
      "\n",
      "[1666 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(answer, columns=['index', 'label'])\n",
    "\n",
    "df.to_csv('Large_data_roberta.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "094f33ed2071450d91c60f15b0331e8c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "095be4a6f89d4603b8a0b314727bd7a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1e6ccdaaa224d05be7928a5aafaab64",
      "max": 751504,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c5a393fdf1044e6298a258d4bb8fcd84",
      "value": 751504
     }
    },
    "0bbfd5f0ec2d4436a06bb502f8131f49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cde98b9a09144b6b25b121ab70b4ff4",
      "placeholder": "​",
      "style": "IPY_MODEL_21f45f4a718446cea0a2ae70c743ba61",
      "value": "Downloading: 100%"
     }
    },
    "0cde98b9a09144b6b25b121ab70b4ff4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14c3fbc303704c62957bd8c329b8024e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5fe9d0f2a6924eba82b3502d43030b61",
       "IPY_MODEL_8902ff1c00df45c78d5b02464ee50756",
       "IPY_MODEL_8fdfe2b3e6e64012ac265a462fbcf328"
      ],
      "layout": "IPY_MODEL_ddd8542cdb7c4844b19cb62fa5e5e146"
     }
    },
    "16cd5a8125f64e618585d262ed0dcc39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1809bf6050194337a8a1efd347bf1f88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1a91d489ae164c8ca4243d06894962fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1c5b91e6ae4d46d0b2f099cd1d4c15cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0bbfd5f0ec2d4436a06bb502f8131f49",
       "IPY_MODEL_2613deb06698404396c37a2cc26f5dc4",
       "IPY_MODEL_daa90c709c4b46439770068ba936d25f"
      ],
      "layout": "IPY_MODEL_d66d726b5a104734a7bcfa4d5bfd53b2"
     }
    },
    "1ced7dcc914c47788ca51353a2245f6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_25ef02ee9d634d79b632eb0a5ad0f6fa",
       "IPY_MODEL_095be4a6f89d4603b8a0b314727bd7a4",
       "IPY_MODEL_c3e047157c734a5fbf6a6c8dc27c5913"
      ],
      "layout": "IPY_MODEL_16cd5a8125f64e618585d262ed0dcc39"
     }
    },
    "1d38cc6507084d34a2d41ada6a37b83e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21f45f4a718446cea0a2ae70c743ba61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "25ef02ee9d634d79b632eb0a5ad0f6fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3560910251ab4451ba5df9bf0953e999",
      "placeholder": "​",
      "style": "IPY_MODEL_c0ef75dc157d4b2a9c881661aa39347c",
      "value": "Downloading: 100%"
     }
    },
    "2613deb06698404396c37a2cc26f5dc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3882a8be8ec44b9fb9595d4239c46930",
      "max": 375,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f6e6cbbc9d0e443ca58cdcfe2c44f49e",
      "value": 375
     }
    },
    "27848cf6c21243f99f6421c294e2844f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28c17f7168c24bcfbfb6cacf9d8a2b24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cd3c31a170a431ca04bbeeb641f6590": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "341d1f2f2ec34446bcc1049dafda1c82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3560910251ab4451ba5df9bf0953e999": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3882a8be8ec44b9fb9595d4239c46930": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3aa68625b6364fdcb47c203b43277941": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3bebe6d4ba11479c8d6b952c786d5393": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45f4bd07499745d09d398568221a49aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a046ee54d1d485e84835c858f31be0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c07c3365e984f3596c00edb9cd2c184": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cdf8336d32747629b01a6550240c6fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57d26fc2e49c4bfaa89415d44a0606ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b541face9d74d2eae19066c55320567": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_afd8616825be40a8a52b1df11d73c64d",
      "placeholder": "​",
      "style": "IPY_MODEL_341d1f2f2ec34446bcc1049dafda1c82",
      "value": "Downloading: 100%"
     }
    },
    "5fe9d0f2a6924eba82b3502d43030b61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3f1a17e1bd642e9b17fa94aff662eda",
      "placeholder": "​",
      "style": "IPY_MODEL_6c7eed7e50f34a1584a9639f26c277fe",
      "value": "Downloading: 100%"
     }
    },
    "683388a3551c4cfc922c56cba867d04c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6860bf24436f4e098c668a285fd06f59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5b541face9d74d2eae19066c55320567",
       "IPY_MODEL_fbabe1a9a00a419b8c5f54375e80d952",
       "IPY_MODEL_f207ba49d03d4d11abd8a50c624eb7c8"
      ],
      "layout": "IPY_MODEL_3aa68625b6364fdcb47c203b43277941"
     }
    },
    "6be5e8971a294d58b173245f74bf1975": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2cd3c31a170a431ca04bbeeb641f6590",
      "placeholder": "​",
      "style": "IPY_MODEL_77abc577d69e4ca69ff1afe8c3a37d43",
      "value": "Downloading: 100%"
     }
    },
    "6c7eed7e50f34a1584a9639f26c277fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "704b74d3a9a040e9a11e57234af566ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "73c789717a3745a9b431931ce845432b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77abc577d69e4ca69ff1afe8c3a37d43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7af67cc2ae4b4c3abb9e7a208ecc9fd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7cb35513245c4927858d1ad05062340d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8242b29066ed4717b957c897adba8533": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8437c6dcd988498fb32f3d0557bb2385": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_accc4b1e20d149aab8385df3109eb92f",
       "IPY_MODEL_9579ee57512f4bfbae9eeaac961ef7cb",
       "IPY_MODEL_a3591cd1d9f04511b42b45cde368fbc7"
      ],
      "layout": "IPY_MODEL_1d38cc6507084d34a2d41ada6a37b83e"
     }
    },
    "86daa3417a4147c4acc5b2cb0aff4935": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6be5e8971a294d58b173245f74bf1975",
       "IPY_MODEL_ebdfb09f0319470db6ef22d68916b111",
       "IPY_MODEL_c0c048b2bc734c2a9a05a3195b29b136"
      ],
      "layout": "IPY_MODEL_f2b21dab53e944a58572bbdd6c9c94b0"
     }
    },
    "8902ff1c00df45c78d5b02464ee50756": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28c17f7168c24bcfbfb6cacf9d8a2b24",
      "max": 173,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1809bf6050194337a8a1efd347bf1f88",
      "value": 173
     }
    },
    "8fdfe2b3e6e64012ac265a462fbcf328": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c40ba97a8a66488cb7b62cf85aead2b5",
      "placeholder": "​",
      "style": "IPY_MODEL_e8da32862f9049ceb6796662fcd4444f",
      "value": " 173/173 [00:00&lt;00:00, 3.16kB/s]"
     }
    },
    "9251c8da568641daacabf3e198c79290": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9579ee57512f4bfbae9eeaac961ef7cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_094f33ed2071450d91c60f15b0331e8c",
      "max": 248477,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7cb35513245c4927858d1ad05062340d",
      "value": 248477
     }
    },
    "a3591cd1d9f04511b42b45cde368fbc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45f4bd07499745d09d398568221a49aa",
      "placeholder": "​",
      "style": "IPY_MODEL_e03141fe0ca24aa888dc4ce1734ef897",
      "value": " 248k/248k [00:00&lt;00:00, 1.09MB/s]"
     }
    },
    "accc4b1e20d149aab8385df3109eb92f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c07c3365e984f3596c00edb9cd2c184",
      "placeholder": "​",
      "style": "IPY_MODEL_b18b9e54b8bb47a7ac0d5d4db9cc0946",
      "value": "Downloading: 100%"
     }
    },
    "afd8616825be40a8a52b1df11d73c64d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b18b9e54b8bb47a7ac0d5d4db9cc0946": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1e6ccdaaa224d05be7928a5aafaab64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0c048b2bc734c2a9a05a3195b29b136": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9251c8da568641daacabf3e198c79290",
      "placeholder": "​",
      "style": "IPY_MODEL_1a91d489ae164c8ca4243d06894962fa",
      "value": " 1.35G/1.35G [00:20&lt;00:00, 73.7MB/s]"
     }
    },
    "c0ef75dc157d4b2a9c881661aa39347c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3e047157c734a5fbf6a6c8dc27c5913": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a046ee54d1d485e84835c858f31be0c",
      "placeholder": "​",
      "style": "IPY_MODEL_57d26fc2e49c4bfaa89415d44a0606ec",
      "value": " 752k/752k [00:00&lt;00:00, 1.32MB/s]"
     }
    },
    "c40ba97a8a66488cb7b62cf85aead2b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5a393fdf1044e6298a258d4bb8fcd84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d66d726b5a104734a7bcfa4d5bfd53b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daa90c709c4b46439770068ba936d25f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cdf8336d32747629b01a6550240c6fe",
      "placeholder": "​",
      "style": "IPY_MODEL_8242b29066ed4717b957c897adba8533",
      "value": " 375/375 [00:00&lt;00:00, 13.1kB/s]"
     }
    },
    "ddd8542cdb7c4844b19cb62fa5e5e146": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e03141fe0ca24aa888dc4ce1734ef897": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8da32862f9049ceb6796662fcd4444f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebdfb09f0319470db6ef22d68916b111": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73c789717a3745a9b431931ce845432b",
      "max": 1346930258,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_704b74d3a9a040e9a11e57234af566ea",
      "value": 1346930258
     }
    },
    "f207ba49d03d4d11abd8a50c624eb7c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3bebe6d4ba11479c8d6b952c786d5393",
      "placeholder": "​",
      "style": "IPY_MODEL_683388a3551c4cfc922c56cba867d04c",
      "value": " 547/547 [00:00&lt;00:00, 9.70kB/s]"
     }
    },
    "f2b21dab53e944a58572bbdd6c9c94b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3f1a17e1bd642e9b17fa94aff662eda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6e6cbbc9d0e443ca58cdcfe2c44f49e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fbabe1a9a00a419b8c5f54375e80d952": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27848cf6c21243f99f6421c294e2844f",
      "max": 547,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7af67cc2ae4b4c3abb9e7a208ecc9fd5",
      "value": 547
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
