{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 10 13:03:59 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:05.0 Off |                  Off |\r\n",
      "| N/A   38C    P0    66W / 300W |   1180MiB / 32510MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:00:06.0 Off |                  Off |\r\n",
      "| N/A   44C    P0    71W / 300W |    844MiB / 32510MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     65182      C   .../envs/test_env/bin/python     1177MiB |\r\n",
      "|    1   N/A  N/A      7312      C   .../envs/test_env/bin/python      841MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_bVw3BzS66Gp"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "K7aUlj6L7eAH",
    "outputId": "575cdb6d-f613-45e8-82d4-146d2a0c2761"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...</td>\n",
       "      <td>씨름의 여자들의 놀이이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...</td>\n",
       "      <td>자작극을 벌인 이는 3명이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.</td>\n",
       "      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n",
       "      <td>원주민들은 종합대책에 만족했다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...</td>\n",
       "      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            premise  \\\n",
       "0      0  씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...   \n",
       "1      1  삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...   \n",
       "2      2                    이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.   \n",
       "3      3  광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...   \n",
       "4      4  진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...   \n",
       "\n",
       "                                hypothesis          label  \n",
       "0                           씨름의 여자들의 놀이이다.  contradiction  \n",
       "1                         자작극을 벌인 이는 3명이다.  contradiction  \n",
       "2  예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.     entailment  \n",
       "3                        원주민들은 종합대책에 만족했다.        neutral  \n",
       "4       이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.        neutral  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dpath = '/content/drive/My Drive/Seculayer/KSRC/'\n",
    "train = pd.read_csv('data/train_data.csv',encoding='utf-8')\n",
    "test = pd.read_csv('data/test_data.csv',encoding='utf-8')\n",
    "eval_klue = pd.read_json(\"data/klue-nli-v1.1_dev.json\")\n",
    "train_klue = pd.read_json(\"data/klue-nli-v1.1_train.json\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>genre</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>author</th>\n",
       "      <th>label2</th>\n",
       "      <th>label3</th>\n",
       "      <th>label4</th>\n",
       "      <th>label5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24993</th>\n",
       "      <td>klue-nli-v1_train_24993</td>\n",
       "      <td>wikitree</td>\n",
       "      <td>힐링푸드 전문가 양성 교육 참여, 개발 음식 활용 등의 혜택도 주어진다.</td>\n",
       "      <td>개발 음식 활용 혜택만 주어진다.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24994</th>\n",
       "      <td>klue-nli-v1_train_24994</td>\n",
       "      <td>wikitree</td>\n",
       "      <td>힐링푸드 전문가 양성 교육 참여, 개발 음식 활용 등의 혜택도 주어진다.</td>\n",
       "      <td>힐링푸드 전문가에게 혜택이 주어진다.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>klue-nli-v1_train_24995</td>\n",
       "      <td>wikitree</td>\n",
       "      <td>힐링푸드 전문가 양성 교육 참여, 개발 음식 활용 등의 혜택도 주어진다.</td>\n",
       "      <td>혜택에는 힐링푸드 전문가 양성 교육 참여가 있다.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>klue-nli-v1_train_24996</td>\n",
       "      <td>NSMC</td>\n",
       "      <td>힛걸 진심 최고다 그 어떤 히어로보다 멋지다</td>\n",
       "      <td>힛걸 그 어떤 히어로보다 별로다.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>klue-nli-v1_train_24997</td>\n",
       "      <td>NSMC</td>\n",
       "      <td>힛걸 진심 최고다 그 어떤 히어로보다 멋지다</td>\n",
       "      <td>힛걸 액션 장면 진심 그 어떤 히어로보다 멋지다.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          guid     genre  \\\n",
       "24993  klue-nli-v1_train_24993  wikitree   \n",
       "24994  klue-nli-v1_train_24994  wikitree   \n",
       "24995  klue-nli-v1_train_24995  wikitree   \n",
       "24996  klue-nli-v1_train_24996      NSMC   \n",
       "24997  klue-nli-v1_train_24997      NSMC   \n",
       "\n",
       "                                        premise                   hypothesis  \\\n",
       "24993  힐링푸드 전문가 양성 교육 참여, 개발 음식 활용 등의 혜택도 주어진다.           개발 음식 활용 혜택만 주어진다.   \n",
       "24994  힐링푸드 전문가 양성 교육 참여, 개발 음식 활용 등의 혜택도 주어진다.         힐링푸드 전문가에게 혜택이 주어진다.   \n",
       "24995  힐링푸드 전문가 양성 교육 참여, 개발 음식 활용 등의 혜택도 주어진다.  혜택에는 힐링푸드 전문가 양성 교육 참여가 있다.   \n",
       "24996                  힛걸 진심 최고다 그 어떤 히어로보다 멋지다           힛걸 그 어떤 히어로보다 별로다.   \n",
       "24997                  힛걸 진심 최고다 그 어떤 히어로보다 멋지다  힛걸 액션 장면 진심 그 어떤 히어로보다 멋지다.   \n",
       "\n",
       "          gold_label         author         label2         label3  \\\n",
       "24993  contradiction  contradiction  contradiction  contradiction   \n",
       "24994        neutral        neutral  contradiction        neutral   \n",
       "24995     entailment     entailment     entailment     entailment   \n",
       "24996  contradiction  contradiction  contradiction  contradiction   \n",
       "24997        neutral        neutral        neutral        neutral   \n",
       "\n",
       "              label4         label5  \n",
       "24993  contradiction  contradiction  \n",
       "24994        neutral  contradiction  \n",
       "24995     entailment     entailment  \n",
       "24996  contradiction  contradiction  \n",
       "24997        neutral        neutral  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_klue.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>source</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>author</th>\n",
       "      <th>label2</th>\n",
       "      <th>label3</th>\n",
       "      <th>label4</th>\n",
       "      <th>label5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>klue-nli-v1_dev_02995</td>\n",
       "      <td>wikinews</td>\n",
       "      <td>흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...</td>\n",
       "      <td>지방도 제1112호선을 넓히는 공사는 중단없이 마무리 되었다.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>klue-nli-v1_dev_02996</td>\n",
       "      <td>wikinews</td>\n",
       "      <td>흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...</td>\n",
       "      <td>지방도 제1112호선을 넓히는 공사가 중단된 건 세 번째이다.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>klue-nli-v1_dev_02997</td>\n",
       "      <td>wikinews</td>\n",
       "      <td>흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...</td>\n",
       "      <td>지방도 제1112호선은 흔히 비자림로라고 불린다.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>klue-nli-v1_dev_02998</td>\n",
       "      <td>airbnb</td>\n",
       "      <td>흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.</td>\n",
       "      <td>비흡연자는 발코니 있는 방이 필요없습니다.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>klue-nli-v1_dev_02999</td>\n",
       "      <td>airbnb</td>\n",
       "      <td>흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.</td>\n",
       "      <td>흡연하려면 발코니 있는 방을 선택하면 됩니다.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       guid    source  \\\n",
       "2995  klue-nli-v1_dev_02995  wikinews   \n",
       "2996  klue-nli-v1_dev_02996  wikinews   \n",
       "2997  klue-nli-v1_dev_02997  wikinews   \n",
       "2998  klue-nli-v1_dev_02998    airbnb   \n",
       "2999  klue-nli-v1_dev_02999    airbnb   \n",
       "\n",
       "                                                premise  \\\n",
       "2995  흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...   \n",
       "2996  흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...   \n",
       "2997  흔히 비자림로라고 불리는 지방도 제1112호선을 넓히는 공사가 1년만에 재개되었다가...   \n",
       "2998                흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.   \n",
       "2999                흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.   \n",
       "\n",
       "                              hypothesis     gold_label         author  \\\n",
       "2995  지방도 제1112호선을 넓히는 공사는 중단없이 마무리 되었다.  contradiction  contradiction   \n",
       "2996  지방도 제1112호선을 넓히는 공사가 중단된 건 세 번째이다.        neutral        neutral   \n",
       "2997         지방도 제1112호선은 흔히 비자림로라고 불린다.     entailment     entailment   \n",
       "2998             비흡연자는 발코니 있는 방이 필요없습니다.        neutral        neutral   \n",
       "2999           흡연하려면 발코니 있는 방을 선택하면 됩니다.     entailment     entailment   \n",
       "\n",
       "             label2         label3         label4         label5  \n",
       "2995  contradiction  contradiction  contradiction  contradiction  \n",
       "2996        neutral        neutral        neutral        neutral  \n",
       "2997     entailment     entailment     entailment     entailment  \n",
       "2998        neutral        neutral        neutral        neutral  \n",
       "2999     entailment     entailment     entailment     entailment  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_klue.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_concat_train = train_klue.rename(columns={\"gold_label\":\"label\"}).iloc[:,2:5]\n",
    "need_concat_eval = eval_klue.rename(columns={\"gold_label\":\"label\"}).iloc[:,2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train,need_concat_train,need_concat_eval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6FpW8SM7lwW",
    "outputId": "e779fb75-c660-4224-8296-fb17f2facfb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 52996 entries, 0 to 2999\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   index       24998 non-null  float64\n",
      " 1   premise     52996 non-null  object \n",
      " 2   hypothesis  52996 non-null  object \n",
      " 3   label       52996 non-null  object \n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 2.0+ MB\n",
      "None\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1666 entries, 0 to 1665\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   index       1666 non-null   int64 \n",
      " 1   premise     1666 non-null   object\n",
      " 2   hypothesis  1666 non-null   object\n",
      " 3   label       1666 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 52.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 결측치는 없음\n",
    "\n",
    "print(train.info(), end='\\n\\n')\n",
    "print(test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4L1jeCl72DF",
    "outputId": "b5c8ce74-4bce-4603-a0f6-e8cbcd527b6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Label: \n",
      "entailment       18122\n",
      "contradiction    17978\n",
      "neutral          16896\n",
      "Name: label, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train Label: ', train['label'].value_counts(), sep='\\n', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "9SAoqqNmAU3x",
    "outputId": "cbe32eff-2df0-4a4f-9d17-50537c24cd33"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data imbalance는 사실상 존재하지 않음 \n",
    "\n",
    "sns.countplot(data=train,x='label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "T6hQolMqAkji"
   },
   "outputs": [],
   "source": [
    "max_premise = np.max(train['premise'].str.len())\n",
    "\n",
    "max_hypothesis = np.max(train['hypothesis'].str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3AApuxsJBcYi",
    "outputId": "744f3bd4-e74b-40cf-defb-abfd64225d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max premise = 90 \n",
      "max hypothesis = 103\n"
     ]
    }
   ],
   "source": [
    "print('max premise =',max_premise,\"\\nmax hypothesis =\",max_hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "CxFj3gc2B2YB",
    "outputId": "b9605aef-724a-408b-ef18-9b3bd3be21e7",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reindex from a duplicate axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_45708/1238057895.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'premise'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/seaborn/distributions.py\u001b[0m in \u001b[0;36mhistplot\u001b[0;34m(data, x, y, hue, weights, stat, bins, binwidth, binrange, discrete, cumulative, common_bins, common_norm, multiple, element, fill, shrink, kde, kde_kws, line_kws, thresh, pthresh, pmax, cbar, cbar_ax, cbar_kws, palette, hue_order, hue_norm, color, log_scale, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m   1473\u001b[0m             \u001b[0mestimate_kws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimate_kws\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0mline_kws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_kws\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1475\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1476\u001b[0m         )\n\u001b[1;32m   1477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/seaborn/distributions.py\u001b[0m in \u001b[0;36mplot_univariate_histogram\u001b[0;34m(self, multiple, element, fill, common_norm, common_bins, shrink, kde, kde_kws, color, legend, line_kws, estimate_kws, **plot_kws)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;31m# First pass through the data to compute the histograms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msub_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_comp_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;31m# Prepare the relevant data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/seaborn/_core.py\u001b[0m in \u001b[0;36miter_data\u001b[0;34m(self, grouping_vars, reverse, from_comp_data)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfrom_comp_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomp_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/seaborn/_core.py\u001b[0m in \u001b[0;36mcomp_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1055\u001b[0m                     \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m                 \u001b[0mcomp_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m                 \u001b[0mcomp_col\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"log\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0miloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0miloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer_split_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_single_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_with_indexer_split_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_single_block\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1957\u001b[0m             \u001b[0;31m# setting for extensionarrays that store dicts. Need to decide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m             \u001b[0;31m# if it's worth supporting that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_align_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_align_series\u001b[0;34m(self, indexer, ser, multiindex_indexer)\u001b[0m\n\u001b[1;32m   2094\u001b[0m             \u001b[0;31m# series, so need to broadcast (see GH5206)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2095\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msum_aligners\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2096\u001b[0;31m                 \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2098\u001b[0m                 \u001b[0;31m# single indexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, index, **kwargs)\u001b[0m\n\u001b[1;32m   4578\u001b[0m     )\n\u001b[1;32m   4579\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4580\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4582\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecate_nonkeyword_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4817\u001b[0m         \u001b[0;31m# perform the reindex on the axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4818\u001b[0m         return self._reindex_axes(\n\u001b[0;32m-> 4819\u001b[0;31m             \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4820\u001b[0m         ).__finalize__(self, method=\"reindex\")\n\u001b[1;32m   4821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   4841\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4842\u001b[0m                 \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4843\u001b[0;31m                 \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4844\u001b[0m             )\n\u001b[1;32m   4845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[0;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[1;32m   4887\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4888\u001b[0m                 \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_dups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4889\u001b[0;31m                 \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4890\u001b[0m             )\n\u001b[1;32m   4891\u001b[0m             \u001b[0;31m# If we've made a copy once, no need to make another one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice)\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;31m# some axes don't allow reindexing with dups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_can_reindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test_env/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_validate_can_reindex\u001b[0;34m(self, indexer)\u001b[0m\n\u001b[1;32m   3783\u001b[0m         \u001b[0;31m# trying to reindex on an axis with duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_as_unique\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3785\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot reindex from a duplicate axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3787\u001b[0m     def reindex(\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reindex from a duplicate axis"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(x=train['premise'].str.len());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "5xFFliUaCiwR",
    "outputId": "b7132a6d-24cb-4773-edaa-13d9d7e75af2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(x=train['hypothesis'].str.len());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Itp65cYhD8UA"
   },
   "outputs": [],
   "source": [
    "english_list = []\n",
    "english_count = 0\n",
    "\n",
    "for i in range(0,len(train)):\n",
    "    if train['premise'].str[:][i].upper() != train['premise'].str[:][i].lower():\n",
    "        english_list.append(train['premise'].str[:][i])\n",
    "        english_count += 1\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZt_lU4PHRhE",
    "outputId": "098c52b6-4bb6-4fa4-b94e-3fc3404e3f28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한국어로만 이루어진 데이터임을 확인 \n",
    "english_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "yv3YDRoOHPoX",
    "outputId": "0883bafc-6b61-4dd4-c270-51516ed832f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서 소년이나 장정들이 ...</td>\n",
       "      <td>씨름의 여자들의 놀이이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나 ...</td>\n",
       "      <td>자작극을 벌인 이는 3명이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다</td>\n",
       "      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n",
       "      <td>원주민들은 종합대책에 만족했다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면 이런 상황에서는 ...</td>\n",
       "      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            premise  \\\n",
       "0    0.0  씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서 소년이나 장정들이 ...   \n",
       "1    1.0  삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나 ...   \n",
       "2    2.0                     이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다   \n",
       "3    3.0  광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...   \n",
       "4    4.0  진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면 이런 상황에서는 ...   \n",
       "\n",
       "                                hypothesis          label  \n",
       "0                           씨름의 여자들의 놀이이다.  contradiction  \n",
       "1                         자작극을 벌인 이는 3명이다.  contradiction  \n",
       "2  예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.     entailment  \n",
       "3                        원주민들은 종합대책에 만족했다.        neutral  \n",
       "4       이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.        neutral  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불 용어 처리 \n",
    "\n",
    "train['premise'] = train['premise'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
    "test['premise'] = test['premise'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "hCfxgISsHvZl",
    "outputId": "43f42d44-7632-48bd-d175-3c42d7029276"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서 소년이나 장정들이 ...</td>\n",
       "      <td>씨름의 여자들의 놀이이다</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나 ...</td>\n",
       "      <td>자작극을 벌인 이는 3명이다</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다</td>\n",
       "      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n",
       "      <td>원주민들은 종합대책에 만족했다</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면 이런 상황에서는 ...</td>\n",
       "      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            premise  \\\n",
       "0    0.0  씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서 소년이나 장정들이 ...   \n",
       "1    1.0  삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나 ...   \n",
       "2    2.0                     이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다   \n",
       "3    3.0  광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...   \n",
       "4    4.0  진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면 이런 상황에서는 ...   \n",
       "\n",
       "                               hypothesis          label  \n",
       "0                           씨름의 여자들의 놀이이다  contradiction  \n",
       "1                         자작극을 벌인 이는 3명이다  contradiction  \n",
       "2  예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다     entailment  \n",
       "3                        원주민들은 종합대책에 만족했다        neutral  \n",
       "4       이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다        neutral  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['hypothesis'] = train['hypothesis'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
    "test['hypothesis'] = test['hypothesis'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qg83MpRKHv-7",
    "outputId": "fb73b171-9390-40f7-f97c-2166594f99aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed:int = 2023):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1c5b91e6ae4d46d0b2f099cd1d4c15cf",
      "0bbfd5f0ec2d4436a06bb502f8131f49",
      "2613deb06698404396c37a2cc26f5dc4",
      "daa90c709c4b46439770068ba936d25f",
      "d66d726b5a104734a7bcfa4d5bfd53b2",
      "0cde98b9a09144b6b25b121ab70b4ff4",
      "21f45f4a718446cea0a2ae70c743ba61",
      "3882a8be8ec44b9fb9595d4239c46930",
      "f6e6cbbc9d0e443ca58cdcfe2c44f49e",
      "4cdf8336d32747629b01a6550240c6fe",
      "8242b29066ed4717b957c897adba8533",
      "8437c6dcd988498fb32f3d0557bb2385",
      "accc4b1e20d149aab8385df3109eb92f",
      "9579ee57512f4bfbae9eeaac961ef7cb",
      "a3591cd1d9f04511b42b45cde368fbc7",
      "1d38cc6507084d34a2d41ada6a37b83e",
      "4c07c3365e984f3596c00edb9cd2c184",
      "b18b9e54b8bb47a7ac0d5d4db9cc0946",
      "094f33ed2071450d91c60f15b0331e8c",
      "7cb35513245c4927858d1ad05062340d",
      "45f4bd07499745d09d398568221a49aa",
      "e03141fe0ca24aa888dc4ce1734ef897",
      "1ced7dcc914c47788ca51353a2245f6e",
      "25ef02ee9d634d79b632eb0a5ad0f6fa",
      "095be4a6f89d4603b8a0b314727bd7a4",
      "c3e047157c734a5fbf6a6c8dc27c5913",
      "16cd5a8125f64e618585d262ed0dcc39",
      "3560910251ab4451ba5df9bf0953e999",
      "c0ef75dc157d4b2a9c881661aa39347c",
      "b1e6ccdaaa224d05be7928a5aafaab64",
      "c5a393fdf1044e6298a258d4bb8fcd84",
      "4a046ee54d1d485e84835c858f31be0c",
      "57d26fc2e49c4bfaa89415d44a0606ec",
      "14c3fbc303704c62957bd8c329b8024e",
      "5fe9d0f2a6924eba82b3502d43030b61",
      "8902ff1c00df45c78d5b02464ee50756",
      "8fdfe2b3e6e64012ac265a462fbcf328",
      "ddd8542cdb7c4844b19cb62fa5e5e146",
      "f3f1a17e1bd642e9b17fa94aff662eda",
      "6c7eed7e50f34a1584a9639f26c277fe",
      "28c17f7168c24bcfbfb6cacf9d8a2b24",
      "1809bf6050194337a8a1efd347bf1f88",
      "c40ba97a8a66488cb7b62cf85aead2b5",
      "e8da32862f9049ceb6796662fcd4444f",
      "6860bf24436f4e098c668a285fd06f59",
      "5b541face9d74d2eae19066c55320567",
      "fbabe1a9a00a419b8c5f54375e80d952",
      "f207ba49d03d4d11abd8a50c624eb7c8",
      "3aa68625b6364fdcb47c203b43277941",
      "afd8616825be40a8a52b1df11d73c64d",
      "341d1f2f2ec34446bcc1049dafda1c82",
      "27848cf6c21243f99f6421c294e2844f",
      "7af67cc2ae4b4c3abb9e7a208ecc9fd5",
      "3bebe6d4ba11479c8d6b952c786d5393",
      "683388a3551c4cfc922c56cba867d04c",
      "86daa3417a4147c4acc5b2cb0aff4935",
      "6be5e8971a294d58b173245f74bf1975",
      "ebdfb09f0319470db6ef22d68916b111",
      "c0c048b2bc734c2a9a05a3195b29b136",
      "f2b21dab53e944a58572bbdd6c9c94b0",
      "2cd3c31a170a431ca04bbeeb641f6590",
      "77abc577d69e4ca69ff1afe8c3a37d43",
      "73c789717a3745a9b431931ce845432b",
      "704b74d3a9a040e9a11e57234af566ea",
      "9251c8da568641daacabf3e198c79290",
      "1a91d489ae164c8ca4243d06894962fa"
     ]
    },
    "id": "-rI1h2a6K9dL",
    "outputId": "30f09f75-ea4d-4986-cc46-67f306e7dba0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/centos/.cache/huggingface/hub/models--klue--roberta-large/snapshots/5193b95701189160c45d02a1033a4ea55bdbe259/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/centos/.cache/huggingface/hub/models--klue--roberta-large/snapshots/5193b95701189160c45d02a1033a4ea55bdbe259/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/centos/.cache/huggingface/hub/models--klue--roberta-large/snapshots/5193b95701189160c45d02a1033a4ea55bdbe259/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/centos/.cache/huggingface/hub/models--klue--roberta-large/snapshots/5193b95701189160c45d02a1033a4ea55bdbe259/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/centos/.cache/huggingface/hub/models--klue--roberta-large/snapshots/5193b95701189160c45d02a1033a4ea55bdbe259/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/centos/.cache/huggingface/hub/models--klue--roberta-large/snapshots/5193b95701189160c45d02a1033a4ea55bdbe259/pytorch_model.bin\n",
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (12): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (13): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (14): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (15): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (16): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (17): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (18): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (19): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (20): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (21): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (22): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (23): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"learning_rate\": 2.5e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000,\n",
      "  \"warmup_ratio\": 0.09,\n",
      "  \"weight_decay\": 0.007\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'klue/roberta-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.num_labels = 3\n",
    "config.learning_rate = 2.5e-05\n",
    "config.warmup_ratio = 0.09\n",
    "config.weight_decay = 0.007\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "print(model)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yca5HgSqLJfa",
    "outputId": "e90f071b-9d99-4443-a1dd-8076ee43532e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0, 15368,  2242,  2052,  1545, 19364,  2016,  8664,  1733,  3930,\n",
      "         2116, 25848,  2530,  2170,  3653,  8664,  3992,  5005,  2170, 10686,\n",
      "         2069,  7794,  2088,  1513,  2062,     2,  1545, 19364,  2016,  8664,\n",
      "         1733,  3930,  2259,  5562,  4780,  2200, 25848, 19521,  1513,  2062,\n",
      "            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1])\n",
      "[CLS] 해남군이 제25호 태풍 콩레이가 북상함에 따라 태풍 피해 예방에 총력을 기울이고 있다 [SEP] 제25호 태풍 콩레이는 빠른 속도로 북상하고 있다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# train test split 및 tokenizing \n",
    "# token에 들어가는 문장은 premise와 hypothesis를 concat 한 문장\n",
    "\n",
    "train_dataset, eval_dataset = train_test_split(train, test_size=0.2, shuffle=True, stratify=train['label'])\n",
    "\n",
    "tokenized_train = tokenizer(\n",
    "    list(train_dataset['premise']),\n",
    "    list(train_dataset['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=210, # Max_Length = 190\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "tokenized_eval = tokenizer(\n",
    "    list(eval_dataset['premise']),\n",
    "    list(eval_dataset['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=210,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "print(tokenized_train['input_ids'][0])\n",
    "print(tokenizer.decode(tokenized_train['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "D5bvDvPaMDkU"
   },
   "outputs": [],
   "source": [
    "class BERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pair_dataset, label):\n",
    "        self.pair_dataset = pair_dataset\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "        item['label'] = torch.tensor(self.label[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Fuk01B-iMUBI"
   },
   "outputs": [],
   "source": [
    "def label_to_num(label):\n",
    "    label_dict = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2, \"answer\": 3}\n",
    "    num_label = []\n",
    "\n",
    "    for v in label:\n",
    "        num_label.append(label_dict[v])\n",
    "    \n",
    "    return num_label\n",
    "\n",
    "\n",
    "train_label = label_to_num(train_dataset['label'].values)\n",
    "eval_label = label_to_num(eval_dataset['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6-3Soz2McqX",
    "outputId": "66cac98b-3367-4f27-e9a3-f3378302fc39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42396\n",
      "{'input_ids': tensor([    0, 11896,  2290,   544,    22,  2628,  2170,  2318,  7714,    21,\n",
      "         2353,  2154,  2252,  4639,  2290,   544,    22,  2628,  2170,  2318,\n",
      "         2259,  7714,  3956,  2154,  2252,  9889,  2290,   544,    24,  2628,\n",
      "         2170,  2318,  2259,  7714,  3740,  2154,  2252,  2052,  4589,  2897,\n",
      "         2062,     2, 11896,  2290,  2073,   864,  1823,  2069,  1220,  2259,\n",
      "         2062,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'label': tensor(0)}\n",
      "[CLS] 최우수상 각 2팀에게 상금 1백만원 우수상 각 2팀에게는 상금 50만원 장려상 각 4팀에게는 상금 30만원이 지급된다 [SEP] 최우수상은 두 팀을 뽑는다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BERTDataset(tokenized_train, train_label)\n",
    "eval_dataset = BERTDataset(tokenized_eval, eval_label)\n",
    "\n",
    "print(train_dataset.__len__())\n",
    "print(train_dataset.__getitem__(19997))\n",
    "print(tokenizer.decode(train_dataset.__getitem__(19997)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Kwczjk_6Mlox"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "  \"\"\" validation을 위한 metrics function \"\"\"\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  probs = pred.predictions\n",
    "\n",
    "  # calculate accuracy using sklearn's function\n",
    "  acc = accuracy_score(labels, preds) # 리더보드 평가에는 포함되지 않습니다.\n",
    "\n",
    "  return {\n",
    "      'accuracy': acc,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Q1BhVy-0O4oo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_ars = TrainingArguments(\n",
    "    output_dir='result/simple_roberta_more_epoch/',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_total_limit=5,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 500,\n",
    "    load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_ars,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 42396\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6630\n",
      "  Number of trainable parameters = 336659459\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msangmi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/centos/psw/KSRC/wandb/run-20230109_093703-2cb9puaf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sangmi/huggingface/runs/2cb9puaf\" target=\"_blank\">result/simple_roberta_more_epoch/</a></strong> to <a href=\"https://wandb.ai/sangmi/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6630' max='6630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6630/6630 1:43:09, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.458300</td>\n",
       "      <td>0.333561</td>\n",
       "      <td>0.896792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.228000</td>\n",
       "      <td>0.240110</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.153400</td>\n",
       "      <td>0.266176</td>\n",
       "      <td>0.930566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>0.227783</td>\n",
       "      <td>0.945660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.211995</td>\n",
       "      <td>0.957264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.183834</td>\n",
       "      <td>0.959906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.208669</td>\n",
       "      <td>0.963302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.214016</td>\n",
       "      <td>0.964151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.216754</td>\n",
       "      <td>0.963585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.185620</td>\n",
       "      <td>0.970849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.208698</td>\n",
       "      <td>0.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.215258</td>\n",
       "      <td>0.971226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.210855</td>\n",
       "      <td>0.971038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10600\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/simple_roberta_more_epoch/checkpoint-500\n",
      "Configuration saved in result/simple_roberta_more_epoch/checkpoint-500/config.json\n",
      "Model weights saved in result/simple_roberta_more_epoch/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in result/simple_roberta_more_epoch/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in result/simple_roberta_more_epoch/checkpoint-500/special_tokens_map.json\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10600\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/simple_roberta_more_epoch/checkpoint-1000\n",
      "Configuration saved in result/simple_roberta_more_epoch/checkpoint-1000/config.json\n",
      "Model weights saved in result/simple_roberta_more_epoch/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in result/simple_roberta_more_epoch/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in result/simple_roberta_more_epoch/checkpoint-1000/special_tokens_map.json\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10600\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/simple_roberta_more_epoch/checkpoint-1500\n",
      "Configuration saved in result/simple_roberta_more_epoch/checkpoint-1500/config.json\n",
      "Model weights saved in result/simple_roberta_more_epoch/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in result/simple_roberta_more_epoch/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in result/simple_roberta_more_epoch/checkpoint-1500/special_tokens_map.json\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10600\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/simple_roberta_more_epoch/checkpoint-2000\n",
      "Configuration saved in result/simple_roberta_more_epoch/checkpoint-2000/config.json\n",
      "Model weights saved in result/simple_roberta_more_epoch/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in result/simple_roberta_more_epoch/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in result/simple_roberta_more_epoch/checkpoint-2000/special_tokens_map.json\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10600\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/simple_roberta_more_epoch/checkpoint-2500\n",
      "Configuration saved in result/simple_roberta_more_epoch/checkpoint-2500/config.json\n",
      "Model weights saved in result/simple_roberta_more_epoch/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in result/simple_roberta_more_epoch/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in result/simple_roberta_more_epoch/checkpoint-2500/special_tokens_map.json\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10600\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/simple_roberta_more_epoch/checkpoint-3000\n",
      "Configuration saved in result/simple_roberta_more_epoch/checkpoint-3000/config.json\n",
      "Model weights saved in result/simple_roberta_more_epoch/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in result/simple_roberta_more_epoch/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in result/simple_roberta_more_epoch/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [result/simple_roberta_more_epoch/checkpoint-500] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10600\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/simple_roberta_more_epoch/checkpoint-3500\n",
      "Configuration saved in result/simple_roberta_more_epoch/checkpoint-3500/config.json\n",
      "Model weights saved in result/simple_roberta_more_epoch/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in result/simple_roberta_more_epoch/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in result/simple_roberta_more_epoch/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [result/simple_roberta_more_epoch/checkpoint-1000] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10600\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/simple_roberta_more_epoch/checkpoint-4000\n",
      "Configuration saved in result/simple_roberta_more_epoch/checkpoint-4000/config.json\n",
      "Model weights saved in result/simple_roberta_more_epoch/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in result/simple_roberta_more_epoch/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in result/simple_roberta_more_epoch/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [result/simple_roberta_more_epoch/checkpoint-1500] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10600\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/simple_roberta_more_epoch/checkpoint-4500\n",
      "Configuration saved in result/simple_roberta_more_epoch/checkpoint-4500/config.json\n",
      "Model weights saved in result/simple_roberta_more_epoch/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in result/simple_roberta_more_epoch/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in result/simple_roberta_more_epoch/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [result/simple_roberta_more_epoch/checkpoint-2000] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10600\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/simple_roberta_more_epoch/checkpoint-5000\n",
      "Configuration saved in result/simple_roberta_more_epoch/checkpoint-5000/config.json\n",
      "Model weights saved in result/simple_roberta_more_epoch/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in result/simple_roberta_more_epoch/checkpoint-5000/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in result/simple_roberta_more_epoch/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [result/simple_roberta_more_epoch/checkpoint-2500] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10600\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/simple_roberta_more_epoch/checkpoint-5500\n",
      "Configuration saved in result/simple_roberta_more_epoch/checkpoint-5500/config.json\n",
      "Model weights saved in result/simple_roberta_more_epoch/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in result/simple_roberta_more_epoch/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in result/simple_roberta_more_epoch/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [result/simple_roberta_more_epoch/checkpoint-3500] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10600\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/simple_roberta_more_epoch/checkpoint-6000\n",
      "Configuration saved in result/simple_roberta_more_epoch/checkpoint-6000/config.json\n",
      "Model weights saved in result/simple_roberta_more_epoch/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in result/simple_roberta_more_epoch/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in result/simple_roberta_more_epoch/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [result/simple_roberta_more_epoch/checkpoint-4000] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10600\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to result/simple_roberta_more_epoch/checkpoint-6500\n",
      "Configuration saved in result/simple_roberta_more_epoch/checkpoint-6500/config.json\n",
      "Model weights saved in result/simple_roberta_more_epoch/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in result/simple_roberta_more_epoch/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in result/simple_roberta_more_epoch/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [result/simple_roberta_more_epoch/checkpoint-4500] due to args.save_total_limit\n",
      "/home/centos/anaconda3/envs/test_env/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from result/simple_roberta_more_epoch/checkpoint-3000 (score: 0.18383391201496124).\n",
      "Configuration saved in result/simple_roberta_more_epoch/best_model/config.json\n",
      "Model weights saved in result/simple_roberta_more_epoch/best_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('result/simple_roberta_more_epoch/best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMOYxWImPRe3",
    "outputId": "c4fd6536-0c44-4915-a50c-0b441a027609",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/centos/.cache/huggingface/hub/models--klue--roberta-large/snapshots/5193b95701189160c45d02a1033a4ea55bdbe259/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/centos/.cache/huggingface/hub/models--klue--roberta-large/snapshots/5193b95701189160c45d02a1033a4ea55bdbe259/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/centos/.cache/huggingface/hub/models--klue--roberta-large/snapshots/5193b95701189160c45d02a1033a4ea55bdbe259/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/centos/.cache/huggingface/hub/models--klue--roberta-large/snapshots/5193b95701189160c45d02a1033a4ea55bdbe259/tokenizer_config.json\n",
      "loading configuration file result/simple_roberta_more_epoch/checkpoint-5000/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"result/simple_roberta_more_epoch/checkpoint-5000\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"learning_rate\": 2.5e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000,\n",
      "  \"warmup_ratio\": 0.09,\n",
      "  \"weight_decay\": 0.007\n",
      "}\n",
      "\n",
      "loading weights file result/simple_roberta_more_epoch/checkpoint-5000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at result/simple_roberta_more_epoch/checkpoint-5000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='klue/roberta-large', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "Tokenizer_NAME = \"klue/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(Tokenizer_NAME)\n",
    "\n",
    "MODEL_NAME = 'result/simple_roberta_more_epoch/checkpoint-5000'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "model.to(device)\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5TT-R3XwsIN",
    "outputId": "cf7a3a83-ebea-4aa5-c31d-386b9487620a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1666\n",
      "{'input_ids': tensor([    0,   720,  3994,  2052, 10428,  2775,   647,  3657,  2119,  1085,\n",
      "            3,     2,   720,  3994,  2052,   911,  2075,  3669,  2119,  3926,\n",
      "         2088,  1513,  2359, 13964,     2,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(3)}\n",
      "[CLS] 18일 귀국이라 발인도 지켜드리지 못해 더욱 죄송할 따름입니다 [SEP] 18일 배를 타고 여행을 떠났습니다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "test_label = label_to_num(test['label'].values)\n",
    "\n",
    "tokenized_test = tokenizer(\n",
    "    list(test['premise']),\n",
    "    list(test['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "test_dataset = BERTDataset(tokenized_test, test_label)\n",
    "\n",
    "print(test_dataset.__len__())\n",
    "print(test_dataset.__getitem__(1665))\n",
    "print(tokenizer.decode(test_dataset.__getitem__(6)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "On27w6Ukwohv",
    "outputId": "1b648050-b470-4f92-be09-1515b8d34e9c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:07<00:00, 14.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 2, 2, 1, 0, 2, 1, 0, 1, 0, 2, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 1, 2, 0, 0, 2, 1, 0, 1, 1, 2, 1, 0, 1, 0, 1, 2, 0, 2, 2, 2, 1, 2, 0, 1, 0, 0, 1, 0, 2, 0, 0, 2, 2, 2, 0, 0, 1, 2, 2, 2, 1, 2, 2, 2, 1, 0, 2, 0, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 1, 2, 1, 0, 2, 1, 2, 0, 1, 0, 2, 1, 1, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2, 1, 0, 1, 2, 1, 0, 2, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 0, 0, 0, 1, 2, 1, 2, 1, 0, 2, 0, 1, 2, 1, 1, 2, 2, 2, 0, 2, 1, 0, 1, 2, 2, 1, 0, 1, 2, 0, 0, 0, 1, 1, 2, 0, 0, 2, 0, 2, 1, 2, 0, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 0, 2, 1, 2, 0, 1, 2, 2, 1, 0, 2, 0, 2, 1, 1, 2, 2, 0, 0, 1, 1, 0, 0, 2, 0, 2, 1, 1, 1, 2, 1, 1, 1, 0, 1, 2, 2, 2, 0, 1, 2, 2, 1, 2, 2, 1, 0, 2, 1, 0, 0, 2, 2, 1, 0, 0, 0, 0, 2, 2, 2, 0, 2, 2, 1, 1, 0, 2, 2, 0, 1, 2, 1, 2, 0, 0, 1, 0, 1, 2, 2, 2, 1, 2, 2, 0, 2, 2, 0, 0, 2, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 0, 2, 1, 0, 2, 0, 1, 0, 1, 0, 1, 1, 1, 2, 0, 2, 0, 2, 0, 0, 0, 2, 1, 0, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 1, 0, 1, 2, 1, 1, 2, 0, 0, 1, 1, 1, 0, 1, 1, 2, 1, 2, 1, 1, 2, 0, 1, 1, 1, 2, 1, 2, 1, 0, 2, 1, 2, 0, 1, 2, 1, 0, 2, 2, 1, 2, 2, 0, 1, 2, 1, 2, 0, 2, 2, 0, 0, 2, 0, 0, 0, 2, 1, 2, 0, 2, 2, 0, 2, 1, 2, 0, 1, 2, 1, 2, 0, 2, 1, 0, 1, 2, 1, 1, 0, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 0, 2, 1, 2, 2, 0, 2, 0, 2, 2, 1, 1, 2, 0, 2, 2, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 2, 2, 1, 1, 1, 0, 0, 1, 1, 0, 2, 1, 2, 1, 0, 0, 1, 2, 2, 1, 1, 2, 0, 2, 1, 2, 0, 0, 0, 2, 0, 1, 1, 1, 0, 0, 1, 0, 2, 2, 0, 2, 1, 0, 0, 2, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 1, 2, 0, 2, 1, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 1, 1, 2, 2, 1, 1, 2, 1, 0, 0, 1, 0, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 0, 1, 2, 1, 0, 2, 0, 2, 1, 0, 0, 0, 2, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2, 0, 0, 2, 1, 0, 0, 0, 1, 0, 1, 0, 2, 1, 1, 1, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 1, 0, 1, 1, 1, 2, 0, 0, 0, 2, 2, 0, 2, 2, 1, 1, 1, 1, 0, 2, 2, 0, 1, 1, 2, 2, 1, 2, 2, 1, 0, 1, 1, 1, 1, 1, 2, 0, 2, 0, 2, 2, 2, 1, 1, 0, 0, 2, 2, 0, 2, 0, 2, 0, 2, 1, 1, 2, 0, 1, 1, 0, 2, 2, 1, 1, 0, 1, 2, 1, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 1, 1, 2, 1, 0, 1, 1, 1, 0, 1, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2, 1, 0, 1, 1, 2, 2, 2, 0, 2, 0, 0, 0, 2, 2, 2, 0, 0, 2, 2, 1, 2, 1, 2, 1, 1, 0, 0, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 0, 0, 0, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 2, 0, 2, 0, 1, 2, 0, 2, 0, 1, 2, 1, 2, 0, 0, 0, 2, 1, 0, 2, 2, 0, 0, 1, 2, 0, 0, 1, 2, 0, 1, 0, 1, 0, 2, 2, 0, 1, 0, 0, 2, 0, 1, 2, 2, 1, 1, 0, 0, 1, 1, 0, 1, 1, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 0, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 0, 1, 0, 2, 2, 1, 1, 0, 2, 2, 1, 0, 2, 1, 0, 0, 2, 1, 0, 1, 2, 2, 2, 2, 0, 0, 2, 1, 2, 2, 2, 0, 0, 2, 1, 0, 1, 2, 2, 2, 2, 1, 0, 0, 0, 2, 2, 0, 2, 0, 1, 0, 0, 1, 2, 0, 2, 0, 2, 2, 1, 0, 0, 0, 2, 1, 1, 0, 0, 2, 2, 2, 0, 2, 2, 1, 1, 2, 0, 2, 0, 0, 0, 2, 1, 0, 0, 0, 1, 2, 0, 2, 2, 1, 1, 0, 1, 0, 1, 0, 2, 1, 2, 2, 0, 0, 2, 2, 0, 1, 2, 1, 1, 1, 2, 2, 0, 1, 1, 2, 1, 1, 1, 0, 0, 2, 0, 0, 1, 0, 0, 2, 1, 1, 0, 0, 2, 0, 0, 0, 2, 0, 2, 2, 2, 1, 1, 2, 1, 0, 0, 2, 0, 2, 2, 2, 1, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 0, 2, 1, 2, 0, 0, 0, 1, 1, 0, 2, 2, 2, 0, 2, 2, 2, 1, 1, 0, 2, 0, 0, 2, 1, 2, 1, 1, 1, 0, 2, 1, 0, 1, 2, 1, 0, 0, 1, 0, 1, 0, 2, 2, 1, 2, 2, 0, 2, 1, 1, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 2, 1, 0, 2, 1, 1, 1, 2, 1, 2, 0, 1, 2, 0, 2, 1, 1, 2, 2, 0, 2, 2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 2, 2, 2, 1, 0, 1, 1, 2, 2, 1, 2, 2, 0, 1, 1, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 0, 2, 2, 1, 1, 0, 2, 0, 2, 0, 1, 0, 1, 2, 0, 0, 2, 2, 0, 2, 2, 0, 1, 1, 0, 2, 0, 2, 0, 2, 2, 1, 2, 1, 2, 2, 0, 2, 2, 2, 1, 2, 0, 1, 2, 1, 2, 2, 2, 0, 1, 0, 0, 1, 2, 0, 0, 1, 2, 1, 0, 2, 1, 0, 2, 2, 2, 1, 1, 1, 1, 2, 0, 0, 1, 2, 1, 2, 0, 2, 2, 2, 1, 1, 2, 0, 1, 2, 0, 2, 2, 2, 1, 0, 1, 1, 0, 0, 0, 0, 2, 2, 2, 1, 0, 0, 1, 1, 1, 2, 0, 2, 0, 1, 1, 2, 1, 0, 0, 1, 0, 2, 0, 0, 2, 2, 2, 0, 1, 1, 1, 1, 0, 1, 0, 1, 2, 2, 0, 1, 0, 1, 0, 1, 0, 2, 0, 1, 2, 0, 2, 2, 2, 2, 2, 0, 1, 1, 2, 1, 2, 1, 0, 0, 2, 2, 2, 1, 2, 0, 1, 2, 1, 1, 2, 1, 2, 2, 0, 2, 1, 1, 0, 2, 1, 1, 2, 0, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 0, 0, 2, 0, 1, 1, 2, 1, 2, 2, 2, 1, 0, 1, 1, 1, 1, 0, 2, 2, 1, 2, 2, 2, 2, 1, 2, 0, 1, 2, 1, 2, 1, 1, 2, 2, 1, 0, 2, 1, 2, 2, 0, 1, 0, 0, 1, 2, 1, 0, 2, 2, 0, 0, 2, 0, 2, 0, 2, 2, 2, 0, 0, 0, 1, 0, 1, 1, 1, 1, 2, 0, 0, 1, 2, 2, 1, 0, 0, 2, 2, 1, 0, 1, 0, 0, 2, 2, 2, 0, 2, 2, 0, 0, 2, 1, 0, 2, 1, 2, 0, 1, 0, 2, 1, 1, 2, 1, 0, 0, 2, 0, 1, 0, 1, 1, 2, 2, 2, 2, 2, 1, 0, 0, 1, 1, 2, 2, 2, 0, 1, 2, 1, 0, 2, 2, 0, 0, 1, 1, 1, 1, 2, 0, 2, 2, 1, 1, 0, 2, 0, 2, 2, 2, 0, 1, 1, 0, 1, 2, 2, 1, 0, 1, 0, 0, 0, 2, 1, 0, 1, 2, 2, 1, 1, 0, 2, 1, 1, 2, 1, 2, 0, 1, 2, 2, 2, 2, 0, 2, 0, 1, 1, 2, 1, 2, 0, 0, 2, 1, 1, 2, 2, 0, 1, 2, 2, 2, 2, 1, 0, 2, 0, 2, 2, 0, 2, 0, 0, 2, 0, 2, 2, 1, 0, 1, 0, 1, 1, 2, 2, 0, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 0, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 2, 2, 0, 0, 2, 2, 1, 1, 1, 2, 0, 2, 0, 0, 1, 0, 2, 2, 0, 2, 0, 1, 2, 1, 1, 1, 2, 0, 1, 2, 1, 2, 2, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 1, 1, 0, 2, 1, 1, 0, 2, 1, 2, 2, 0, 2, 1, 2, 2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "output_pred = []\n",
    "output_prob = []\n",
    "\n",
    "for i, data in enumerate(tqdm(dataloader)):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=data['input_ids'].to(device),\n",
    "            attention_mask=data['attention_mask'].to(device),\n",
    "            token_type_ids=data['token_type_ids'].to(device)\n",
    "        )\n",
    "    logits = outputs[0]\n",
    "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    result = np.argmax(logits, axis=-1)\n",
    "\n",
    "    output_pred.append(result)\n",
    "    output_prob.append(prob)\n",
    "  \n",
    "pred_answer, output_prob = np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()\n",
    "print(pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rR2B__eywvus",
    "outputId": "ba7b7017-fd7f-4120-9b90-5ed18409c339"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 'contradiction'], [1, 'entailment'], [2, 'neutral'], [3, 'contradiction'], [4, 'contradiction'], [5, 'neutral'], [6, 'neutral'], [7, 'contradiction'], [8, 'entailment'], [9, 'neutral'], [10, 'contradiction'], [11, 'entailment'], [12, 'contradiction'], [13, 'entailment'], [14, 'neutral'], [15, 'neutral'], [16, 'contradiction'], [17, 'neutral'], [18, 'contradiction'], [19, 'neutral'], [20, 'contradiction'], [21, 'neutral'], [22, 'contradiction'], [23, 'entailment'], [24, 'contradiction'], [25, 'contradiction'], [26, 'neutral'], [27, 'entailment'], [28, 'entailment'], [29, 'neutral'], [30, 'contradiction'], [31, 'entailment'], [32, 'contradiction'], [33, 'contradiction'], [34, 'neutral'], [35, 'contradiction'], [36, 'entailment'], [37, 'contradiction'], [38, 'entailment'], [39, 'contradiction'], [40, 'neutral'], [41, 'entailment'], [42, 'neutral'], [43, 'neutral'], [44, 'neutral'], [45, 'contradiction'], [46, 'neutral'], [47, 'entailment'], [48, 'contradiction'], [49, 'entailment'], [50, 'entailment'], [51, 'contradiction'], [52, 'entailment'], [53, 'neutral'], [54, 'entailment'], [55, 'entailment'], [56, 'neutral'], [57, 'neutral'], [58, 'neutral'], [59, 'entailment'], [60, 'entailment'], [61, 'contradiction'], [62, 'neutral'], [63, 'neutral'], [64, 'neutral'], [65, 'contradiction'], [66, 'neutral'], [67, 'neutral'], [68, 'neutral'], [69, 'contradiction'], [70, 'entailment'], [71, 'neutral'], [72, 'entailment'], [73, 'contradiction'], [74, 'neutral'], [75, 'neutral'], [76, 'neutral'], [77, 'neutral'], [78, 'neutral'], [79, 'neutral'], [80, 'contradiction'], [81, 'contradiction'], [82, 'contradiction'], [83, 'contradiction'], [84, 'entailment'], [85, 'contradiction'], [86, 'neutral'], [87, 'contradiction'], [88, 'entailment'], [89, 'neutral'], [90, 'contradiction'], [91, 'neutral'], [92, 'entailment'], [93, 'contradiction'], [94, 'entailment'], [95, 'neutral'], [96, 'contradiction'], [97, 'contradiction'], [98, 'entailment'], [99, 'neutral'], [100, 'neutral'], [101, 'entailment'], [102, 'neutral'], [103, 'entailment'], [104, 'neutral'], [105, 'entailment'], [106, 'neutral'], [107, 'neutral'], [108, 'contradiction'], [109, 'entailment'], [110, 'contradiction'], [111, 'neutral'], [112, 'contradiction'], [113, 'entailment'], [114, 'neutral'], [115, 'entailment'], [116, 'entailment'], [117, 'contradiction'], [118, 'contradiction'], [119, 'contradiction'], [120, 'contradiction'], [121, 'contradiction'], [122, 'contradiction'], [123, 'neutral'], [124, 'neutral'], [125, 'entailment'], [126, 'entailment'], [127, 'entailment'], [128, 'contradiction'], [129, 'neutral'], [130, 'contradiction'], [131, 'neutral'], [132, 'contradiction'], [133, 'entailment'], [134, 'neutral'], [135, 'entailment'], [136, 'contradiction'], [137, 'neutral'], [138, 'contradiction'], [139, 'contradiction'], [140, 'neutral'], [141, 'neutral'], [142, 'neutral'], [143, 'entailment'], [144, 'neutral'], [145, 'contradiction'], [146, 'entailment'], [147, 'contradiction'], [148, 'neutral'], [149, 'neutral'], [150, 'contradiction'], [151, 'entailment'], [152, 'contradiction'], [153, 'neutral'], [154, 'entailment'], [155, 'entailment'], [156, 'entailment'], [157, 'contradiction'], [158, 'contradiction'], [159, 'neutral'], [160, 'entailment'], [161, 'entailment'], [162, 'neutral'], [163, 'entailment'], [164, 'neutral'], [165, 'contradiction'], [166, 'neutral'], [167, 'entailment'], [168, 'neutral'], [169, 'entailment'], [170, 'neutral'], [171, 'neutral'], [172, 'entailment'], [173, 'neutral'], [174, 'neutral'], [175, 'neutral'], [176, 'neutral'], [177, 'neutral'], [178, 'contradiction'], [179, 'neutral'], [180, 'contradiction'], [181, 'neutral'], [182, 'neutral'], [183, 'neutral'], [184, 'entailment'], [185, 'neutral'], [186, 'contradiction'], [187, 'neutral'], [188, 'entailment'], [189, 'contradiction'], [190, 'neutral'], [191, 'neutral'], [192, 'contradiction'], [193, 'entailment'], [194, 'neutral'], [195, 'entailment'], [196, 'neutral'], [197, 'contradiction'], [198, 'contradiction'], [199, 'neutral'], [200, 'neutral'], [201, 'entailment'], [202, 'entailment'], [203, 'contradiction'], [204, 'contradiction'], [205, 'entailment'], [206, 'entailment'], [207, 'neutral'], [208, 'entailment'], [209, 'neutral'], [210, 'contradiction'], [211, 'contradiction'], [212, 'contradiction'], [213, 'neutral'], [214, 'contradiction'], [215, 'contradiction'], [216, 'contradiction'], [217, 'entailment'], [218, 'contradiction'], [219, 'neutral'], [220, 'neutral'], [221, 'neutral'], [222, 'entailment'], [223, 'contradiction'], [224, 'neutral'], [225, 'neutral'], [226, 'contradiction'], [227, 'neutral'], [228, 'neutral'], [229, 'contradiction'], [230, 'entailment'], [231, 'neutral'], [232, 'contradiction'], [233, 'entailment'], [234, 'entailment'], [235, 'neutral'], [236, 'neutral'], [237, 'contradiction'], [238, 'entailment'], [239, 'entailment'], [240, 'entailment'], [241, 'entailment'], [242, 'neutral'], [243, 'neutral'], [244, 'neutral'], [245, 'entailment'], [246, 'neutral'], [247, 'neutral'], [248, 'contradiction'], [249, 'contradiction'], [250, 'entailment'], [251, 'neutral'], [252, 'neutral'], [253, 'entailment'], [254, 'contradiction'], [255, 'neutral'], [256, 'contradiction'], [257, 'neutral'], [258, 'entailment'], [259, 'entailment'], [260, 'contradiction'], [261, 'entailment'], [262, 'contradiction'], [263, 'neutral'], [264, 'neutral'], [265, 'neutral'], [266, 'contradiction'], [267, 'neutral'], [268, 'neutral'], [269, 'entailment'], [270, 'neutral'], [271, 'neutral'], [272, 'entailment'], [273, 'entailment'], [274, 'neutral'], [275, 'contradiction'], [276, 'contradiction'], [277, 'contradiction'], [278, 'neutral'], [279, 'neutral'], [280, 'neutral'], [281, 'entailment'], [282, 'entailment'], [283, 'entailment'], [284, 'contradiction'], [285, 'entailment'], [286, 'neutral'], [287, 'contradiction'], [288, 'entailment'], [289, 'neutral'], [290, 'entailment'], [291, 'contradiction'], [292, 'entailment'], [293, 'contradiction'], [294, 'entailment'], [295, 'contradiction'], [296, 'contradiction'], [297, 'contradiction'], [298, 'neutral'], [299, 'entailment'], [300, 'neutral'], [301, 'entailment'], [302, 'neutral'], [303, 'entailment'], [304, 'entailment'], [305, 'entailment'], [306, 'neutral'], [307, 'contradiction'], [308, 'entailment'], [309, 'entailment'], [310, 'neutral'], [311, 'contradiction'], [312, 'neutral'], [313, 'neutral'], [314, 'neutral'], [315, 'neutral'], [316, 'neutral'], [317, 'neutral'], [318, 'neutral'], [319, 'entailment'], [320, 'entailment'], [321, 'neutral'], [322, 'neutral'], [323, 'neutral'], [324, 'contradiction'], [325, 'entailment'], [326, 'contradiction'], [327, 'neutral'], [328, 'contradiction'], [329, 'contradiction'], [330, 'neutral'], [331, 'entailment'], [332, 'entailment'], [333, 'contradiction'], [334, 'contradiction'], [335, 'contradiction'], [336, 'entailment'], [337, 'contradiction'], [338, 'contradiction'], [339, 'neutral'], [340, 'contradiction'], [341, 'neutral'], [342, 'contradiction'], [343, 'contradiction'], [344, 'neutral'], [345, 'entailment'], [346, 'contradiction'], [347, 'contradiction'], [348, 'contradiction'], [349, 'neutral'], [350, 'contradiction'], [351, 'neutral'], [352, 'contradiction'], [353, 'entailment'], [354, 'neutral'], [355, 'contradiction'], [356, 'neutral'], [357, 'entailment'], [358, 'contradiction'], [359, 'neutral'], [360, 'contradiction'], [361, 'entailment'], [362, 'neutral'], [363, 'neutral'], [364, 'contradiction'], [365, 'neutral'], [366, 'neutral'], [367, 'entailment'], [368, 'contradiction'], [369, 'neutral'], [370, 'contradiction'], [371, 'neutral'], [372, 'entailment'], [373, 'neutral'], [374, 'neutral'], [375, 'entailment'], [376, 'entailment'], [377, 'neutral'], [378, 'entailment'], [379, 'entailment'], [380, 'entailment'], [381, 'neutral'], [382, 'contradiction'], [383, 'neutral'], [384, 'entailment'], [385, 'neutral'], [386, 'neutral'], [387, 'entailment'], [388, 'neutral'], [389, 'contradiction'], [390, 'neutral'], [391, 'entailment'], [392, 'contradiction'], [393, 'neutral'], [394, 'contradiction'], [395, 'neutral'], [396, 'entailment'], [397, 'neutral'], [398, 'contradiction'], [399, 'entailment'], [400, 'contradiction'], [401, 'neutral'], [402, 'contradiction'], [403, 'contradiction'], [404, 'entailment'], [405, 'neutral'], [406, 'contradiction'], [407, 'neutral'], [408, 'contradiction'], [409, 'neutral'], [410, 'contradiction'], [411, 'contradiction'], [412, 'contradiction'], [413, 'contradiction'], [414, 'neutral'], [415, 'contradiction'], [416, 'contradiction'], [417, 'entailment'], [418, 'neutral'], [419, 'contradiction'], [420, 'neutral'], [421, 'neutral'], [422, 'entailment'], [423, 'neutral'], [424, 'entailment'], [425, 'neutral'], [426, 'neutral'], [427, 'contradiction'], [428, 'contradiction'], [429, 'neutral'], [430, 'entailment'], [431, 'neutral'], [432, 'neutral'], [433, 'contradiction'], [434, 'entailment'], [435, 'neutral'], [436, 'entailment'], [437, 'entailment'], [438, 'entailment'], [439, 'entailment'], [440, 'entailment'], [441, 'contradiction'], [442, 'entailment'], [443, 'neutral'], [444, 'contradiction'], [445, 'neutral'], [446, 'neutral'], [447, 'neutral'], [448, 'contradiction'], [449, 'contradiction'], [450, 'contradiction'], [451, 'entailment'], [452, 'entailment'], [453, 'contradiction'], [454, 'contradiction'], [455, 'entailment'], [456, 'neutral'], [457, 'contradiction'], [458, 'neutral'], [459, 'contradiction'], [460, 'entailment'], [461, 'entailment'], [462, 'contradiction'], [463, 'neutral'], [464, 'neutral'], [465, 'contradiction'], [466, 'contradiction'], [467, 'neutral'], [468, 'entailment'], [469, 'neutral'], [470, 'contradiction'], [471, 'neutral'], [472, 'entailment'], [473, 'entailment'], [474, 'entailment'], [475, 'neutral'], [476, 'entailment'], [477, 'contradiction'], [478, 'contradiction'], [479, 'contradiction'], [480, 'entailment'], [481, 'entailment'], [482, 'contradiction'], [483, 'entailment'], [484, 'neutral'], [485, 'neutral'], [486, 'entailment'], [487, 'neutral'], [488, 'contradiction'], [489, 'entailment'], [490, 'entailment'], [491, 'neutral'], [492, 'neutral'], [493, 'entailment'], [494, 'entailment'], [495, 'entailment'], [496, 'entailment'], [497, 'neutral'], [498, 'neutral'], [499, 'entailment'], [500, 'entailment'], [501, 'entailment'], [502, 'neutral'], [503, 'contradiction'], [504, 'neutral'], [505, 'entailment'], [506, 'neutral'], [507, 'contradiction'], [508, 'entailment'], [509, 'contradiction'], [510, 'neutral'], [511, 'entailment'], [512, 'entailment'], [513, 'entailment'], [514, 'entailment'], [515, 'contradiction'], [516, 'entailment'], [517, 'entailment'], [518, 'contradiction'], [519, 'contradiction'], [520, 'neutral'], [521, 'neutral'], [522, 'contradiction'], [523, 'contradiction'], [524, 'neutral'], [525, 'contradiction'], [526, 'entailment'], [527, 'entailment'], [528, 'contradiction'], [529, 'entailment'], [530, 'neutral'], [531, 'neutral'], [532, 'contradiction'], [533, 'neutral'], [534, 'neutral'], [535, 'contradiction'], [536, 'contradiction'], [537, 'contradiction'], [538, 'neutral'], [539, 'neutral'], [540, 'neutral'], [541, 'contradiction'], [542, 'contradiction'], [543, 'entailment'], [544, 'contradiction'], [545, 'neutral'], [546, 'contradiction'], [547, 'entailment'], [548, 'neutral'], [549, 'entailment'], [550, 'neutral'], [551, 'contradiction'], [552, 'entailment'], [553, 'entailment'], [554, 'entailment'], [555, 'neutral'], [556, 'entailment'], [557, 'neutral'], [558, 'neutral'], [559, 'contradiction'], [560, 'contradiction'], [561, 'contradiction'], [562, 'neutral'], [563, 'neutral'], [564, 'entailment'], [565, 'neutral'], [566, 'entailment'], [567, 'entailment'], [568, 'neutral'], [569, 'contradiction'], [570, 'entailment'], [571, 'entailment'], [572, 'entailment'], [573, 'contradiction'], [574, 'entailment'], [575, 'contradiction'], [576, 'entailment'], [577, 'neutral'], [578, 'contradiction'], [579, 'contradiction'], [580, 'contradiction'], [581, 'entailment'], [582, 'entailment'], [583, 'neutral'], [584, 'entailment'], [585, 'entailment'], [586, 'neutral'], [587, 'entailment'], [588, 'neutral'], [589, 'entailment'], [590, 'entailment'], [591, 'neutral'], [592, 'contradiction'], [593, 'entailment'], [594, 'contradiction'], [595, 'contradiction'], [596, 'contradiction'], [597, 'neutral'], [598, 'entailment'], [599, 'entailment'], [600, 'entailment'], [601, 'neutral'], [602, 'neutral'], [603, 'entailment'], [604, 'neutral'], [605, 'neutral'], [606, 'contradiction'], [607, 'contradiction'], [608, 'contradiction'], [609, 'contradiction'], [610, 'entailment'], [611, 'neutral'], [612, 'neutral'], [613, 'entailment'], [614, 'contradiction'], [615, 'contradiction'], [616, 'neutral'], [617, 'neutral'], [618, 'contradiction'], [619, 'neutral'], [620, 'neutral'], [621, 'contradiction'], [622, 'entailment'], [623, 'contradiction'], [624, 'contradiction'], [625, 'contradiction'], [626, 'contradiction'], [627, 'contradiction'], [628, 'neutral'], [629, 'entailment'], [630, 'neutral'], [631, 'entailment'], [632, 'neutral'], [633, 'neutral'], [634, 'neutral'], [635, 'contradiction'], [636, 'contradiction'], [637, 'entailment'], [638, 'entailment'], [639, 'neutral'], [640, 'neutral'], [641, 'entailment'], [642, 'neutral'], [643, 'entailment'], [644, 'neutral'], [645, 'entailment'], [646, 'neutral'], [647, 'contradiction'], [648, 'contradiction'], [649, 'neutral'], [650, 'entailment'], [651, 'contradiction'], [652, 'contradiction'], [653, 'entailment'], [654, 'neutral'], [655, 'neutral'], [656, 'contradiction'], [657, 'contradiction'], [658, 'entailment'], [659, 'contradiction'], [660, 'neutral'], [661, 'contradiction'], [662, 'neutral'], [663, 'neutral'], [664, 'neutral'], [665, 'contradiction'], [666, 'neutral'], [667, 'entailment'], [668, 'neutral'], [669, 'neutral'], [670, 'neutral'], [671, 'neutral'], [672, 'neutral'], [673, 'contradiction'], [674, 'neutral'], [675, 'entailment'], [676, 'neutral'], [677, 'neutral'], [678, 'contradiction'], [679, 'contradiction'], [680, 'neutral'], [681, 'contradiction'], [682, 'entailment'], [683, 'contradiction'], [684, 'contradiction'], [685, 'contradiction'], [686, 'entailment'], [687, 'contradiction'], [688, 'entailment'], [689, 'entailment'], [690, 'neutral'], [691, 'entailment'], [692, 'entailment'], [693, 'neutral'], [694, 'entailment'], [695, 'entailment'], [696, 'entailment'], [697, 'contradiction'], [698, 'entailment'], [699, 'entailment'], [700, 'neutral'], [701, 'entailment'], [702, 'entailment'], [703, 'neutral'], [704, 'contradiction'], [705, 'entailment'], [706, 'contradiction'], [707, 'contradiction'], [708, 'neutral'], [709, 'neutral'], [710, 'neutral'], [711, 'entailment'], [712, 'neutral'], [713, 'entailment'], [714, 'entailment'], [715, 'entailment'], [716, 'neutral'], [717, 'neutral'], [718, 'neutral'], [719, 'entailment'], [720, 'entailment'], [721, 'neutral'], [722, 'neutral'], [723, 'contradiction'], [724, 'neutral'], [725, 'contradiction'], [726, 'neutral'], [727, 'contradiction'], [728, 'contradiction'], [729, 'entailment'], [730, 'entailment'], [731, 'neutral'], [732, 'neutral'], [733, 'neutral'], [734, 'entailment'], [735, 'neutral'], [736, 'entailment'], [737, 'neutral'], [738, 'neutral'], [739, 'entailment'], [740, 'neutral'], [741, 'entailment'], [742, 'entailment'], [743, 'entailment'], [744, 'contradiction'], [745, 'neutral'], [746, 'neutral'], [747, 'contradiction'], [748, 'contradiction'], [749, 'neutral'], [750, 'neutral'], [751, 'neutral'], [752, 'neutral'], [753, 'neutral'], [754, 'entailment'], [755, 'neutral'], [756, 'neutral'], [757, 'contradiction'], [758, 'neutral'], [759, 'neutral'], [760, 'entailment'], [761, 'neutral'], [762, 'entailment'], [763, 'contradiction'], [764, 'neutral'], [765, 'entailment'], [766, 'neutral'], [767, 'entailment'], [768, 'contradiction'], [769, 'neutral'], [770, 'contradiction'], [771, 'neutral'], [772, 'entailment'], [773, 'entailment'], [774, 'entailment'], [775, 'neutral'], [776, 'contradiction'], [777, 'entailment'], [778, 'neutral'], [779, 'neutral'], [780, 'entailment'], [781, 'entailment'], [782, 'contradiction'], [783, 'neutral'], [784, 'entailment'], [785, 'entailment'], [786, 'contradiction'], [787, 'neutral'], [788, 'entailment'], [789, 'contradiction'], [790, 'entailment'], [791, 'contradiction'], [792, 'entailment'], [793, 'neutral'], [794, 'neutral'], [795, 'entailment'], [796, 'contradiction'], [797, 'entailment'], [798, 'entailment'], [799, 'neutral'], [800, 'entailment'], [801, 'contradiction'], [802, 'neutral'], [803, 'neutral'], [804, 'contradiction'], [805, 'contradiction'], [806, 'entailment'], [807, 'entailment'], [808, 'contradiction'], [809, 'contradiction'], [810, 'entailment'], [811, 'contradiction'], [812, 'contradiction'], [813, 'neutral'], [814, 'neutral'], [815, 'neutral'], [816, 'neutral'], [817, 'neutral'], [818, 'neutral'], [819, 'entailment'], [820, 'neutral'], [821, 'neutral'], [822, 'neutral'], [823, 'neutral'], [824, 'neutral'], [825, 'neutral'], [826, 'contradiction'], [827, 'neutral'], [828, 'contradiction'], [829, 'neutral'], [830, 'entailment'], [831, 'contradiction'], [832, 'contradiction'], [833, 'neutral'], [834, 'neutral'], [835, 'contradiction'], [836, 'neutral'], [837, 'contradiction'], [838, 'neutral'], [839, 'contradiction'], [840, 'contradiction'], [841, 'entailment'], [842, 'contradiction'], [843, 'entailment'], [844, 'neutral'], [845, 'neutral'], [846, 'contradiction'], [847, 'contradiction'], [848, 'entailment'], [849, 'neutral'], [850, 'neutral'], [851, 'contradiction'], [852, 'entailment'], [853, 'neutral'], [854, 'contradiction'], [855, 'entailment'], [856, 'entailment'], [857, 'neutral'], [858, 'contradiction'], [859, 'entailment'], [860, 'contradiction'], [861, 'neutral'], [862, 'neutral'], [863, 'neutral'], [864, 'neutral'], [865, 'entailment'], [866, 'entailment'], [867, 'neutral'], [868, 'contradiction'], [869, 'neutral'], [870, 'neutral'], [871, 'neutral'], [872, 'entailment'], [873, 'entailment'], [874, 'neutral'], [875, 'contradiction'], [876, 'entailment'], [877, 'contradiction'], [878, 'neutral'], [879, 'neutral'], [880, 'neutral'], [881, 'neutral'], [882, 'contradiction'], [883, 'entailment'], [884, 'entailment'], [885, 'entailment'], [886, 'neutral'], [887, 'neutral'], [888, 'entailment'], [889, 'neutral'], [890, 'entailment'], [891, 'contradiction'], [892, 'entailment'], [893, 'entailment'], [894, 'contradiction'], [895, 'neutral'], [896, 'entailment'], [897, 'neutral'], [898, 'entailment'], [899, 'neutral'], [900, 'neutral'], [901, 'contradiction'], [902, 'entailment'], [903, 'entailment'], [904, 'entailment'], [905, 'neutral'], [906, 'contradiction'], [907, 'contradiction'], [908, 'entailment'], [909, 'entailment'], [910, 'neutral'], [911, 'neutral'], [912, 'neutral'], [913, 'entailment'], [914, 'neutral'], [915, 'neutral'], [916, 'contradiction'], [917, 'contradiction'], [918, 'neutral'], [919, 'entailment'], [920, 'neutral'], [921, 'entailment'], [922, 'entailment'], [923, 'entailment'], [924, 'neutral'], [925, 'contradiction'], [926, 'entailment'], [927, 'entailment'], [928, 'entailment'], [929, 'contradiction'], [930, 'neutral'], [931, 'entailment'], [932, 'neutral'], [933, 'neutral'], [934, 'contradiction'], [935, 'contradiction'], [936, 'entailment'], [937, 'contradiction'], [938, 'entailment'], [939, 'contradiction'], [940, 'entailment'], [941, 'neutral'], [942, 'contradiction'], [943, 'neutral'], [944, 'neutral'], [945, 'entailment'], [946, 'entailment'], [947, 'neutral'], [948, 'neutral'], [949, 'entailment'], [950, 'contradiction'], [951, 'neutral'], [952, 'contradiction'], [953, 'contradiction'], [954, 'contradiction'], [955, 'neutral'], [956, 'neutral'], [957, 'entailment'], [958, 'contradiction'], [959, 'contradiction'], [960, 'neutral'], [961, 'contradiction'], [962, 'contradiction'], [963, 'contradiction'], [964, 'entailment'], [965, 'entailment'], [966, 'neutral'], [967, 'entailment'], [968, 'entailment'], [969, 'contradiction'], [970, 'entailment'], [971, 'entailment'], [972, 'neutral'], [973, 'contradiction'], [974, 'contradiction'], [975, 'entailment'], [976, 'entailment'], [977, 'neutral'], [978, 'entailment'], [979, 'entailment'], [980, 'entailment'], [981, 'neutral'], [982, 'entailment'], [983, 'neutral'], [984, 'neutral'], [985, 'neutral'], [986, 'contradiction'], [987, 'contradiction'], [988, 'neutral'], [989, 'contradiction'], [990, 'entailment'], [991, 'entailment'], [992, 'neutral'], [993, 'entailment'], [994, 'neutral'], [995, 'neutral'], [996, 'neutral'], [997, 'contradiction'], [998, 'contradiction'], [999, 'entailment'], [1000, 'contradiction'], [1001, 'entailment'], [1002, 'entailment'], [1003, 'entailment'], [1004, 'neutral'], [1005, 'entailment'], [1006, 'entailment'], [1007, 'entailment'], [1008, 'neutral'], [1009, 'neutral'], [1010, 'neutral'], [1011, 'entailment'], [1012, 'neutral'], [1013, 'contradiction'], [1014, 'neutral'], [1015, 'entailment'], [1016, 'entailment'], [1017, 'entailment'], [1018, 'contradiction'], [1019, 'contradiction'], [1020, 'entailment'], [1021, 'neutral'], [1022, 'neutral'], [1023, 'neutral'], [1024, 'entailment'], [1025, 'neutral'], [1026, 'neutral'], [1027, 'neutral'], [1028, 'contradiction'], [1029, 'contradiction'], [1030, 'entailment'], [1031, 'neutral'], [1032, 'entailment'], [1033, 'entailment'], [1034, 'neutral'], [1035, 'contradiction'], [1036, 'neutral'], [1037, 'contradiction'], [1038, 'contradiction'], [1039, 'contradiction'], [1040, 'entailment'], [1041, 'neutral'], [1042, 'contradiction'], [1043, 'entailment'], [1044, 'contradiction'], [1045, 'neutral'], [1046, 'contradiction'], [1047, 'entailment'], [1048, 'entailment'], [1049, 'contradiction'], [1050, 'entailment'], [1051, 'contradiction'], [1052, 'entailment'], [1053, 'neutral'], [1054, 'neutral'], [1055, 'contradiction'], [1056, 'neutral'], [1057, 'neutral'], [1058, 'entailment'], [1059, 'neutral'], [1060, 'contradiction'], [1061, 'contradiction'], [1062, 'contradiction'], [1063, 'entailment'], [1064, 'entailment'], [1065, 'neutral'], [1066, 'entailment'], [1067, 'entailment'], [1068, 'entailment'], [1069, 'neutral'], [1070, 'entailment'], [1071, 'entailment'], [1072, 'neutral'], [1073, 'neutral'], [1074, 'entailment'], [1075, 'neutral'], [1076, 'entailment'], [1077, 'neutral'], [1078, 'contradiction'], [1079, 'entailment'], [1080, 'neutral'], [1081, 'contradiction'], [1082, 'contradiction'], [1083, 'contradiction'], [1084, 'neutral'], [1085, 'contradiction'], [1086, 'neutral'], [1087, 'entailment'], [1088, 'contradiction'], [1089, 'neutral'], [1090, 'entailment'], [1091, 'neutral'], [1092, 'contradiction'], [1093, 'contradiction'], [1094, 'neutral'], [1095, 'neutral'], [1096, 'entailment'], [1097, 'neutral'], [1098, 'neutral'], [1099, 'contradiction'], [1100, 'entailment'], [1101, 'contradiction'], [1102, 'contradiction'], [1103, 'contradiction'], [1104, 'contradiction'], [1105, 'entailment'], [1106, 'contradiction'], [1107, 'contradiction'], [1108, 'entailment'], [1109, 'entailment'], [1110, 'entailment'], [1111, 'contradiction'], [1112, 'contradiction'], [1113, 'contradiction'], [1114, 'neutral'], [1115, 'neutral'], [1116, 'neutral'], [1117, 'contradiction'], [1118, 'entailment'], [1119, 'contradiction'], [1120, 'contradiction'], [1121, 'neutral'], [1122, 'neutral'], [1123, 'contradiction'], [1124, 'neutral'], [1125, 'neutral'], [1126, 'entailment'], [1127, 'contradiction'], [1128, 'contradiction'], [1129, 'contradiction'], [1130, 'neutral'], [1131, 'entailment'], [1132, 'entailment'], [1133, 'contradiction'], [1134, 'contradiction'], [1135, 'contradiction'], [1136, 'contradiction'], [1137, 'contradiction'], [1138, 'contradiction'], [1139, 'entailment'], [1140, 'neutral'], [1141, 'neutral'], [1142, 'contradiction'], [1143, 'contradiction'], [1144, 'entailment'], [1145, 'neutral'], [1146, 'entailment'], [1147, 'neutral'], [1148, 'entailment'], [1149, 'contradiction'], [1150, 'entailment'], [1151, 'contradiction'], [1152, 'neutral'], [1153, 'entailment'], [1154, 'entailment'], [1155, 'neutral'], [1156, 'neutral'], [1157, 'entailment'], [1158, 'neutral'], [1159, 'neutral'], [1160, 'entailment'], [1161, 'contradiction'], [1162, 'contradiction'], [1163, 'entailment'], [1164, 'neutral'], [1165, 'entailment'], [1166, 'neutral'], [1167, 'entailment'], [1168, 'neutral'], [1169, 'neutral'], [1170, 'contradiction'], [1171, 'neutral'], [1172, 'contradiction'], [1173, 'neutral'], [1174, 'neutral'], [1175, 'entailment'], [1176, 'neutral'], [1177, 'neutral'], [1178, 'neutral'], [1179, 'contradiction'], [1180, 'neutral'], [1181, 'entailment'], [1182, 'contradiction'], [1183, 'neutral'], [1184, 'contradiction'], [1185, 'neutral'], [1186, 'neutral'], [1187, 'neutral'], [1188, 'entailment'], [1189, 'contradiction'], [1190, 'entailment'], [1191, 'entailment'], [1192, 'contradiction'], [1193, 'neutral'], [1194, 'entailment'], [1195, 'entailment'], [1196, 'contradiction'], [1197, 'neutral'], [1198, 'contradiction'], [1199, 'entailment'], [1200, 'neutral'], [1201, 'contradiction'], [1202, 'entailment'], [1203, 'neutral'], [1204, 'neutral'], [1205, 'neutral'], [1206, 'contradiction'], [1207, 'contradiction'], [1208, 'contradiction'], [1209, 'contradiction'], [1210, 'neutral'], [1211, 'entailment'], [1212, 'entailment'], [1213, 'contradiction'], [1214, 'neutral'], [1215, 'contradiction'], [1216, 'neutral'], [1217, 'entailment'], [1218, 'neutral'], [1219, 'neutral'], [1220, 'neutral'], [1221, 'contradiction'], [1222, 'contradiction'], [1223, 'neutral'], [1224, 'entailment'], [1225, 'contradiction'], [1226, 'neutral'], [1227, 'entailment'], [1228, 'neutral'], [1229, 'neutral'], [1230, 'neutral'], [1231, 'contradiction'], [1232, 'entailment'], [1233, 'contradiction'], [1234, 'contradiction'], [1235, 'entailment'], [1236, 'entailment'], [1237, 'entailment'], [1238, 'entailment'], [1239, 'neutral'], [1240, 'neutral'], [1241, 'neutral'], [1242, 'contradiction'], [1243, 'entailment'], [1244, 'entailment'], [1245, 'contradiction'], [1246, 'contradiction'], [1247, 'contradiction'], [1248, 'neutral'], [1249, 'entailment'], [1250, 'neutral'], [1251, 'entailment'], [1252, 'contradiction'], [1253, 'contradiction'], [1254, 'neutral'], [1255, 'contradiction'], [1256, 'entailment'], [1257, 'entailment'], [1258, 'contradiction'], [1259, 'entailment'], [1260, 'neutral'], [1261, 'entailment'], [1262, 'entailment'], [1263, 'neutral'], [1264, 'neutral'], [1265, 'neutral'], [1266, 'entailment'], [1267, 'contradiction'], [1268, 'contradiction'], [1269, 'contradiction'], [1270, 'contradiction'], [1271, 'entailment'], [1272, 'contradiction'], [1273, 'entailment'], [1274, 'contradiction'], [1275, 'neutral'], [1276, 'neutral'], [1277, 'entailment'], [1278, 'contradiction'], [1279, 'entailment'], [1280, 'contradiction'], [1281, 'entailment'], [1282, 'contradiction'], [1283, 'entailment'], [1284, 'neutral'], [1285, 'entailment'], [1286, 'contradiction'], [1287, 'neutral'], [1288, 'entailment'], [1289, 'neutral'], [1290, 'neutral'], [1291, 'neutral'], [1292, 'neutral'], [1293, 'neutral'], [1294, 'entailment'], [1295, 'contradiction'], [1296, 'contradiction'], [1297, 'neutral'], [1298, 'contradiction'], [1299, 'neutral'], [1300, 'contradiction'], [1301, 'entailment'], [1302, 'entailment'], [1303, 'neutral'], [1304, 'neutral'], [1305, 'neutral'], [1306, 'contradiction'], [1307, 'neutral'], [1308, 'entailment'], [1309, 'contradiction'], [1310, 'neutral'], [1311, 'contradiction'], [1312, 'contradiction'], [1313, 'neutral'], [1314, 'contradiction'], [1315, 'neutral'], [1316, 'neutral'], [1317, 'entailment'], [1318, 'neutral'], [1319, 'contradiction'], [1320, 'contradiction'], [1321, 'entailment'], [1322, 'neutral'], [1323, 'contradiction'], [1324, 'contradiction'], [1325, 'neutral'], [1326, 'entailment'], [1327, 'neutral'], [1328, 'neutral'], [1329, 'contradiction'], [1330, 'neutral'], [1331, 'neutral'], [1332, 'contradiction'], [1333, 'neutral'], [1334, 'contradiction'], [1335, 'neutral'], [1336, 'contradiction'], [1337, 'neutral'], [1338, 'entailment'], [1339, 'neutral'], [1340, 'entailment'], [1341, 'entailment'], [1342, 'entailment'], [1343, 'neutral'], [1344, 'entailment'], [1345, 'contradiction'], [1346, 'contradiction'], [1347, 'neutral'], [1348, 'contradiction'], [1349, 'neutral'], [1350, 'neutral'], [1351, 'neutral'], [1352, 'contradiction'], [1353, 'entailment'], [1354, 'contradiction'], [1355, 'contradiction'], [1356, 'contradiction'], [1357, 'contradiction'], [1358, 'entailment'], [1359, 'neutral'], [1360, 'neutral'], [1361, 'contradiction'], [1362, 'neutral'], [1363, 'neutral'], [1364, 'neutral'], [1365, 'neutral'], [1366, 'contradiction'], [1367, 'neutral'], [1368, 'entailment'], [1369, 'contradiction'], [1370, 'neutral'], [1371, 'contradiction'], [1372, 'neutral'], [1373, 'contradiction'], [1374, 'contradiction'], [1375, 'neutral'], [1376, 'neutral'], [1377, 'contradiction'], [1378, 'entailment'], [1379, 'neutral'], [1380, 'contradiction'], [1381, 'neutral'], [1382, 'neutral'], [1383, 'entailment'], [1384, 'contradiction'], [1385, 'entailment'], [1386, 'entailment'], [1387, 'contradiction'], [1388, 'neutral'], [1389, 'contradiction'], [1390, 'entailment'], [1391, 'neutral'], [1392, 'neutral'], [1393, 'entailment'], [1394, 'entailment'], [1395, 'neutral'], [1396, 'entailment'], [1397, 'neutral'], [1398, 'entailment'], [1399, 'neutral'], [1400, 'neutral'], [1401, 'neutral'], [1402, 'entailment'], [1403, 'entailment'], [1404, 'entailment'], [1405, 'contradiction'], [1406, 'entailment'], [1407, 'contradiction'], [1408, 'contradiction'], [1409, 'contradiction'], [1410, 'contradiction'], [1411, 'neutral'], [1412, 'entailment'], [1413, 'entailment'], [1414, 'contradiction'], [1415, 'neutral'], [1416, 'neutral'], [1417, 'contradiction'], [1418, 'entailment'], [1419, 'entailment'], [1420, 'neutral'], [1421, 'neutral'], [1422, 'contradiction'], [1423, 'entailment'], [1424, 'contradiction'], [1425, 'entailment'], [1426, 'entailment'], [1427, 'neutral'], [1428, 'neutral'], [1429, 'neutral'], [1430, 'entailment'], [1431, 'neutral'], [1432, 'neutral'], [1433, 'entailment'], [1434, 'entailment'], [1435, 'neutral'], [1436, 'contradiction'], [1437, 'entailment'], [1438, 'neutral'], [1439, 'contradiction'], [1440, 'neutral'], [1441, 'entailment'], [1442, 'contradiction'], [1443, 'entailment'], [1444, 'neutral'], [1445, 'contradiction'], [1446, 'contradiction'], [1447, 'neutral'], [1448, 'contradiction'], [1449, 'entailment'], [1450, 'entailment'], [1451, 'neutral'], [1452, 'entailment'], [1453, 'contradiction'], [1454, 'entailment'], [1455, 'contradiction'], [1456, 'contradiction'], [1457, 'neutral'], [1458, 'neutral'], [1459, 'neutral'], [1460, 'neutral'], [1461, 'neutral'], [1462, 'contradiction'], [1463, 'entailment'], [1464, 'entailment'], [1465, 'contradiction'], [1466, 'contradiction'], [1467, 'neutral'], [1468, 'neutral'], [1469, 'neutral'], [1470, 'entailment'], [1471, 'contradiction'], [1472, 'neutral'], [1473, 'contradiction'], [1474, 'entailment'], [1475, 'neutral'], [1476, 'neutral'], [1477, 'entailment'], [1478, 'entailment'], [1479, 'contradiction'], [1480, 'contradiction'], [1481, 'contradiction'], [1482, 'contradiction'], [1483, 'neutral'], [1484, 'entailment'], [1485, 'neutral'], [1486, 'neutral'], [1487, 'contradiction'], [1488, 'contradiction'], [1489, 'entailment'], [1490, 'neutral'], [1491, 'entailment'], [1492, 'neutral'], [1493, 'neutral'], [1494, 'neutral'], [1495, 'entailment'], [1496, 'contradiction'], [1497, 'contradiction'], [1498, 'entailment'], [1499, 'contradiction'], [1500, 'neutral'], [1501, 'neutral'], [1502, 'contradiction'], [1503, 'entailment'], [1504, 'contradiction'], [1505, 'entailment'], [1506, 'entailment'], [1507, 'entailment'], [1508, 'neutral'], [1509, 'contradiction'], [1510, 'entailment'], [1511, 'contradiction'], [1512, 'neutral'], [1513, 'neutral'], [1514, 'contradiction'], [1515, 'contradiction'], [1516, 'entailment'], [1517, 'neutral'], [1518, 'contradiction'], [1519, 'contradiction'], [1520, 'neutral'], [1521, 'contradiction'], [1522, 'neutral'], [1523, 'entailment'], [1524, 'contradiction'], [1525, 'neutral'], [1526, 'neutral'], [1527, 'neutral'], [1528, 'neutral'], [1529, 'entailment'], [1530, 'neutral'], [1531, 'entailment'], [1532, 'contradiction'], [1533, 'contradiction'], [1534, 'neutral'], [1535, 'contradiction'], [1536, 'neutral'], [1537, 'entailment'], [1538, 'entailment'], [1539, 'neutral'], [1540, 'contradiction'], [1541, 'contradiction'], [1542, 'neutral'], [1543, 'neutral'], [1544, 'entailment'], [1545, 'contradiction'], [1546, 'neutral'], [1547, 'neutral'], [1548, 'neutral'], [1549, 'neutral'], [1550, 'contradiction'], [1551, 'entailment'], [1552, 'neutral'], [1553, 'entailment'], [1554, 'neutral'], [1555, 'neutral'], [1556, 'entailment'], [1557, 'neutral'], [1558, 'entailment'], [1559, 'entailment'], [1560, 'neutral'], [1561, 'entailment'], [1562, 'neutral'], [1563, 'neutral'], [1564, 'contradiction'], [1565, 'entailment'], [1566, 'contradiction'], [1567, 'entailment'], [1568, 'contradiction'], [1569, 'contradiction'], [1570, 'neutral'], [1571, 'neutral'], [1572, 'entailment'], [1573, 'contradiction'], [1574, 'contradiction'], [1575, 'contradiction'], [1576, 'neutral'], [1577, 'contradiction'], [1578, 'contradiction'], [1579, 'neutral'], [1580, 'neutral'], [1581, 'contradiction'], [1582, 'contradiction'], [1583, 'entailment'], [1584, 'neutral'], [1585, 'neutral'], [1586, 'neutral'], [1587, 'neutral'], [1588, 'entailment'], [1589, 'entailment'], [1590, 'entailment'], [1591, 'entailment'], [1592, 'contradiction'], [1593, 'entailment'], [1594, 'neutral'], [1595, 'neutral'], [1596, 'entailment'], [1597, 'entailment'], [1598, 'neutral'], [1599, 'neutral'], [1600, 'contradiction'], [1601, 'contradiction'], [1602, 'contradiction'], [1603, 'neutral'], [1604, 'entailment'], [1605, 'neutral'], [1606, 'entailment'], [1607, 'entailment'], [1608, 'contradiction'], [1609, 'entailment'], [1610, 'neutral'], [1611, 'neutral'], [1612, 'entailment'], [1613, 'neutral'], [1614, 'entailment'], [1615, 'contradiction'], [1616, 'neutral'], [1617, 'contradiction'], [1618, 'contradiction'], [1619, 'contradiction'], [1620, 'neutral'], [1621, 'entailment'], [1622, 'contradiction'], [1623, 'neutral'], [1624, 'contradiction'], [1625, 'neutral'], [1626, 'neutral'], [1627, 'entailment'], [1628, 'contradiction'], [1629, 'entailment'], [1630, 'contradiction'], [1631, 'entailment'], [1632, 'entailment'], [1633, 'entailment'], [1634, 'entailment'], [1635, 'neutral'], [1636, 'entailment'], [1637, 'contradiction'], [1638, 'neutral'], [1639, 'entailment'], [1640, 'entailment'], [1641, 'entailment'], [1642, 'entailment'], [1643, 'entailment'], [1644, 'neutral'], [1645, 'neutral'], [1646, 'neutral'], [1647, 'entailment'], [1648, 'entailment'], [1649, 'contradiction'], [1650, 'contradiction'], [1651, 'entailment'], [1652, 'neutral'], [1653, 'contradiction'], [1654, 'contradiction'], [1655, 'entailment'], [1656, 'neutral'], [1657, 'contradiction'], [1658, 'neutral'], [1659, 'neutral'], [1660, 'entailment'], [1661, 'neutral'], [1662, 'contradiction'], [1663, 'neutral'], [1664, 'neutral'], [1665, 'contradiction']]\n"
     ]
    }
   ],
   "source": [
    "def num_to_label(label):\n",
    "    label_dict = {0: \"entailment\", 1: \"contradiction\", 2: \"neutral\"}\n",
    "    str_label = []\n",
    "\n",
    "    for i, v in enumerate(label):\n",
    "        str_label.append([i,label_dict[v]])\n",
    "    \n",
    "    return str_label\n",
    "\n",
    "answer = num_to_label(pred_answer)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQ2t9exhwx9r",
    "outputId": "c7da8224-ffdb-4911-f111-3270c87ba521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      index          label\n",
      "0         0  contradiction\n",
      "1         1     entailment\n",
      "2         2        neutral\n",
      "3         3  contradiction\n",
      "4         4  contradiction\n",
      "...     ...            ...\n",
      "1661   1661        neutral\n",
      "1662   1662  contradiction\n",
      "1663   1663        neutral\n",
      "1664   1664        neutral\n",
      "1665   1665  contradiction\n",
      "\n",
      "[1666 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(answer, columns=['index', 'label'])\n",
    "\n",
    "df.to_csv('simple_roberta_tune.csv', index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7Zi4_T3w--b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "094f33ed2071450d91c60f15b0331e8c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "095be4a6f89d4603b8a0b314727bd7a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1e6ccdaaa224d05be7928a5aafaab64",
      "max": 751504,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c5a393fdf1044e6298a258d4bb8fcd84",
      "value": 751504
     }
    },
    "0bbfd5f0ec2d4436a06bb502f8131f49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cde98b9a09144b6b25b121ab70b4ff4",
      "placeholder": "​",
      "style": "IPY_MODEL_21f45f4a718446cea0a2ae70c743ba61",
      "value": "Downloading: 100%"
     }
    },
    "0cde98b9a09144b6b25b121ab70b4ff4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14c3fbc303704c62957bd8c329b8024e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5fe9d0f2a6924eba82b3502d43030b61",
       "IPY_MODEL_8902ff1c00df45c78d5b02464ee50756",
       "IPY_MODEL_8fdfe2b3e6e64012ac265a462fbcf328"
      ],
      "layout": "IPY_MODEL_ddd8542cdb7c4844b19cb62fa5e5e146"
     }
    },
    "16cd5a8125f64e618585d262ed0dcc39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1809bf6050194337a8a1efd347bf1f88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1a91d489ae164c8ca4243d06894962fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1c5b91e6ae4d46d0b2f099cd1d4c15cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0bbfd5f0ec2d4436a06bb502f8131f49",
       "IPY_MODEL_2613deb06698404396c37a2cc26f5dc4",
       "IPY_MODEL_daa90c709c4b46439770068ba936d25f"
      ],
      "layout": "IPY_MODEL_d66d726b5a104734a7bcfa4d5bfd53b2"
     }
    },
    "1ced7dcc914c47788ca51353a2245f6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_25ef02ee9d634d79b632eb0a5ad0f6fa",
       "IPY_MODEL_095be4a6f89d4603b8a0b314727bd7a4",
       "IPY_MODEL_c3e047157c734a5fbf6a6c8dc27c5913"
      ],
      "layout": "IPY_MODEL_16cd5a8125f64e618585d262ed0dcc39"
     }
    },
    "1d38cc6507084d34a2d41ada6a37b83e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21f45f4a718446cea0a2ae70c743ba61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "25ef02ee9d634d79b632eb0a5ad0f6fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3560910251ab4451ba5df9bf0953e999",
      "placeholder": "​",
      "style": "IPY_MODEL_c0ef75dc157d4b2a9c881661aa39347c",
      "value": "Downloading: 100%"
     }
    },
    "2613deb06698404396c37a2cc26f5dc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3882a8be8ec44b9fb9595d4239c46930",
      "max": 375,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f6e6cbbc9d0e443ca58cdcfe2c44f49e",
      "value": 375
     }
    },
    "27848cf6c21243f99f6421c294e2844f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28c17f7168c24bcfbfb6cacf9d8a2b24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cd3c31a170a431ca04bbeeb641f6590": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "341d1f2f2ec34446bcc1049dafda1c82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3560910251ab4451ba5df9bf0953e999": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3882a8be8ec44b9fb9595d4239c46930": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3aa68625b6364fdcb47c203b43277941": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3bebe6d4ba11479c8d6b952c786d5393": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45f4bd07499745d09d398568221a49aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a046ee54d1d485e84835c858f31be0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c07c3365e984f3596c00edb9cd2c184": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cdf8336d32747629b01a6550240c6fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57d26fc2e49c4bfaa89415d44a0606ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b541face9d74d2eae19066c55320567": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_afd8616825be40a8a52b1df11d73c64d",
      "placeholder": "​",
      "style": "IPY_MODEL_341d1f2f2ec34446bcc1049dafda1c82",
      "value": "Downloading: 100%"
     }
    },
    "5fe9d0f2a6924eba82b3502d43030b61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3f1a17e1bd642e9b17fa94aff662eda",
      "placeholder": "​",
      "style": "IPY_MODEL_6c7eed7e50f34a1584a9639f26c277fe",
      "value": "Downloading: 100%"
     }
    },
    "683388a3551c4cfc922c56cba867d04c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6860bf24436f4e098c668a285fd06f59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5b541face9d74d2eae19066c55320567",
       "IPY_MODEL_fbabe1a9a00a419b8c5f54375e80d952",
       "IPY_MODEL_f207ba49d03d4d11abd8a50c624eb7c8"
      ],
      "layout": "IPY_MODEL_3aa68625b6364fdcb47c203b43277941"
     }
    },
    "6be5e8971a294d58b173245f74bf1975": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2cd3c31a170a431ca04bbeeb641f6590",
      "placeholder": "​",
      "style": "IPY_MODEL_77abc577d69e4ca69ff1afe8c3a37d43",
      "value": "Downloading: 100%"
     }
    },
    "6c7eed7e50f34a1584a9639f26c277fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "704b74d3a9a040e9a11e57234af566ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "73c789717a3745a9b431931ce845432b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77abc577d69e4ca69ff1afe8c3a37d43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7af67cc2ae4b4c3abb9e7a208ecc9fd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7cb35513245c4927858d1ad05062340d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8242b29066ed4717b957c897adba8533": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8437c6dcd988498fb32f3d0557bb2385": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_accc4b1e20d149aab8385df3109eb92f",
       "IPY_MODEL_9579ee57512f4bfbae9eeaac961ef7cb",
       "IPY_MODEL_a3591cd1d9f04511b42b45cde368fbc7"
      ],
      "layout": "IPY_MODEL_1d38cc6507084d34a2d41ada6a37b83e"
     }
    },
    "86daa3417a4147c4acc5b2cb0aff4935": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6be5e8971a294d58b173245f74bf1975",
       "IPY_MODEL_ebdfb09f0319470db6ef22d68916b111",
       "IPY_MODEL_c0c048b2bc734c2a9a05a3195b29b136"
      ],
      "layout": "IPY_MODEL_f2b21dab53e944a58572bbdd6c9c94b0"
     }
    },
    "8902ff1c00df45c78d5b02464ee50756": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28c17f7168c24bcfbfb6cacf9d8a2b24",
      "max": 173,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1809bf6050194337a8a1efd347bf1f88",
      "value": 173
     }
    },
    "8fdfe2b3e6e64012ac265a462fbcf328": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c40ba97a8a66488cb7b62cf85aead2b5",
      "placeholder": "​",
      "style": "IPY_MODEL_e8da32862f9049ceb6796662fcd4444f",
      "value": " 173/173 [00:00&lt;00:00, 3.16kB/s]"
     }
    },
    "9251c8da568641daacabf3e198c79290": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9579ee57512f4bfbae9eeaac961ef7cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_094f33ed2071450d91c60f15b0331e8c",
      "max": 248477,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7cb35513245c4927858d1ad05062340d",
      "value": 248477
     }
    },
    "a3591cd1d9f04511b42b45cde368fbc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45f4bd07499745d09d398568221a49aa",
      "placeholder": "​",
      "style": "IPY_MODEL_e03141fe0ca24aa888dc4ce1734ef897",
      "value": " 248k/248k [00:00&lt;00:00, 1.09MB/s]"
     }
    },
    "accc4b1e20d149aab8385df3109eb92f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c07c3365e984f3596c00edb9cd2c184",
      "placeholder": "​",
      "style": "IPY_MODEL_b18b9e54b8bb47a7ac0d5d4db9cc0946",
      "value": "Downloading: 100%"
     }
    },
    "afd8616825be40a8a52b1df11d73c64d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b18b9e54b8bb47a7ac0d5d4db9cc0946": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1e6ccdaaa224d05be7928a5aafaab64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0c048b2bc734c2a9a05a3195b29b136": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9251c8da568641daacabf3e198c79290",
      "placeholder": "​",
      "style": "IPY_MODEL_1a91d489ae164c8ca4243d06894962fa",
      "value": " 1.35G/1.35G [00:20&lt;00:00, 73.7MB/s]"
     }
    },
    "c0ef75dc157d4b2a9c881661aa39347c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3e047157c734a5fbf6a6c8dc27c5913": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a046ee54d1d485e84835c858f31be0c",
      "placeholder": "​",
      "style": "IPY_MODEL_57d26fc2e49c4bfaa89415d44a0606ec",
      "value": " 752k/752k [00:00&lt;00:00, 1.32MB/s]"
     }
    },
    "c40ba97a8a66488cb7b62cf85aead2b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5a393fdf1044e6298a258d4bb8fcd84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d66d726b5a104734a7bcfa4d5bfd53b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daa90c709c4b46439770068ba936d25f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cdf8336d32747629b01a6550240c6fe",
      "placeholder": "​",
      "style": "IPY_MODEL_8242b29066ed4717b957c897adba8533",
      "value": " 375/375 [00:00&lt;00:00, 13.1kB/s]"
     }
    },
    "ddd8542cdb7c4844b19cb62fa5e5e146": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e03141fe0ca24aa888dc4ce1734ef897": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8da32862f9049ceb6796662fcd4444f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebdfb09f0319470db6ef22d68916b111": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73c789717a3745a9b431931ce845432b",
      "max": 1346930258,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_704b74d3a9a040e9a11e57234af566ea",
      "value": 1346930258
     }
    },
    "f207ba49d03d4d11abd8a50c624eb7c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3bebe6d4ba11479c8d6b952c786d5393",
      "placeholder": "​",
      "style": "IPY_MODEL_683388a3551c4cfc922c56cba867d04c",
      "value": " 547/547 [00:00&lt;00:00, 9.70kB/s]"
     }
    },
    "f2b21dab53e944a58572bbdd6c9c94b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3f1a17e1bd642e9b17fa94aff662eda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6e6cbbc9d0e443ca58cdcfe2c44f49e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fbabe1a9a00a419b8c5f54375e80d952": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27848cf6c21243f99f6421c294e2844f",
      "max": 547,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7af67cc2ae4b4c3abb9e7a208ecc9fd5",
      "value": 547
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
